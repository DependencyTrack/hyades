{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hyades","text":""},{"location":"#what-is-this","title":"What is this? \ud83e\udd14","text":"<p>Hyades, named after the star cluster closest to earth,  decouples responsibilities from Dependency-Track's monolithic API server into separate,  scalable\u2122 services. We're using Kafka (or Kafka-compatible brokers like Redpanda) for communicating between API  server and Hyades services.</p> <p>If you're interested in the technical background of this project, please refer to \ud83d\udc49 <code>WTF.md</code> \ud83d\udc48.</p> <p>As of now, Hyades is capable of:</p> <ul> <li>Performing vulnerability analysis using scanners that leverage:</li> <li>Dependency-Track's internal vulnerability database</li> <li>OSS Index</li> <li>Snyk</li> <li>Gathering component metadata (e.g. latest available version) from remote repositories</li> <li>Sending notifications via all channels supported by the original API server (E-Mail, Webhook, etc.)</li> </ul> <p>Here's a rough overview of the architecture:</p> <p></p> <p>To read more about the individual services, refer to their respective <code>REAMDE.md</code>:</p> <ul> <li>Repository Metadata Analyzer</li> <li>Vulnerability Analyzer</li> </ul>"},{"location":"#great-can-i-try-it","title":"Great, can I try it? \ud83d\ude4c","text":"<p>Yes! We prepared demo setup that you can use to play around with Hyades. Check out \ud83d\udc49 <code>DEMO.md</code> \ud83d\udc48 for details!</p>"},{"location":"#technical-documentation","title":"Technical Documentation \ud83d\udcbb","text":""},{"location":"#configuration","title":"Configuration \ud83d\udcdd","text":"<p>See <code>CONFIGURATION.md</code>.</p>"},{"location":"#development","title":"Development","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>JDK 17+</li> <li>Docker</li> </ul>"},{"location":"#building","title":"Building","text":"<pre><code>mvn clean install -DskipTests\n</code></pre>"},{"location":"#running-locally","title":"Running locally","text":"<p>Running the Hyades services locally requires both a Kafka broker and a database server to be present. Containers for Redpanda and PostgreSQL can be launched using Docker Compose:</p> <pre><code>docker compose up -d\n</code></pre> <p>To launch individual services execute the <code>quarkus:dev</code> Maven goal for the respective module:</p> <pre><code>mvn -pl vulnerability-analyzer quarkus:dev\n</code></pre> <p>Make sure you've built the project at least once, otherwise the above command will fail.</p> <p>Note If you're unfamiliar with Quarkus' Dev Mode, you can read more about it  here</p>"},{"location":"#testing","title":"Testing \ud83e\udd1e","text":""},{"location":"#unit-testing","title":"Unit Testing \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"<p>To execute the unit tests for all Hyades modules:</p> <pre><code>mvn clean verify\n</code></pre>"},{"location":"#end-to-end-testing","title":"End-To-End Testing \ud83e\udddf","text":"<p>Note End-to-end tests are based on container images. The tags of those images are currently hardcoded. For the Hyades services, the tags are set to <code>latest</code>. If you want to test local changes, you'll have to first: * Build container images locally * Update the tags in <code>AbstractE2ET</code></p> <p>To execute end-to-end tests as part of the build:</p> <pre><code>mvn clean verify -Pe2e-all\n</code></pre> <p>To execute only the end-to-end tests:</p> <pre><code>mvn -pl e2e clean verify -Pe2e-all\n</code></pre>"},{"location":"#load-testing","title":"Load Testing \ud83d\ude80","text":"<p>See <code>load-tests</code>.</p>"},{"location":"#deployment","title":"Deployment \ud83d\udea2","text":"<p>The recommended way to deploy Hyades is via Helm. Our chart is not officially published to any repository yet, so for now you'll have to clone this repository to access it.</p> <p>The chart does not include:</p> <ul> <li>a database</li> <li>a Kafka-compatible broker</li> <li>the API server</li> <li>the frontend</li> </ul> <p>While API server and frontend will eventually be included, database and Kafka broker will not.</p> <p>Helm charts to deploy Kafka brokers to Kubernetes are provided by both Strimzi  and Redpanda. </p>"},{"location":"#minikube","title":"Minikube","text":"<p>Deploying to a local Minikube cluster is a great way to get started.</p> <p>Note For now, services not included in the Helm chart are deployed using Docker Compose.</p> <ol> <li>Start PostgreSQL and Redpanda via Docker Compose <pre><code>docker compose up -d\n</code></pre></li> <li>Start the API server and frontend <pre><code>docker compose up -d apiserver frontend\n</code></pre></li> <li>Start a local Minikube cluster <pre><code>minikube start\n</code></pre></li> <li>Deploy Hyades <pre><code>helm install hyades ./helm-charts/ \\\n  -n hyades --create-namespace \\\n  -f ./helm-charts/hyades/values.yaml \\\n  -f ./helm-charts/hyades/values-minikube.yaml\n</code></pre></li> </ol>"},{"location":"#monitoring","title":"Monitoring \ud83d\udcca","text":""},{"location":"#metrics","title":"Metrics","text":"<p>A basic metrics monitoring stack is provided, consisting of Prometheus and Grafana. To start both services, run:</p> <pre><code>docker compose --profile monitoring up -d\n</code></pre> <p>The services will be available locally at the following locations:</p> <ul> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000</li> </ul> <p>Prometheus is configured to scrape metrics from the following services in a 5s intervals:</p> <ul> <li>Redpanda Broker</li> <li>API Server</li> <li>Notification Publisher</li> <li>Repository Meta Analyzer</li> <li>Vulnerability Analyzer</li> </ul> <p>The Grafana instance will be automatically provisioned to use Prometheus as data source. Additionally, dashboards for the following services are automatically set up:</p> <ul> <li>Redpanda Broker</li> <li>API Server</li> <li>Vulnerability Analyzer</li> </ul>"},{"location":"#redpanda-console","title":"Redpanda Console \ud83d\udc3c","text":"<p>The provided <code>docker-compose.yml</code> includes an instance of Redpanda Console to aid with gaining insight into what's happening in the message broker. Among many other things, it can be used to inspect messages inside any given topic.</p> <p></p> <p>The console is exposed at <code>http://127.0.0.1:28080</code> and does not require authentication. It's intended for local use only.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#why","title":"Why?","text":"<p>tl;dr: Dependency-Track's architecture prevents it from scaling past a certain workload.</p> <p>Dependency-Track, for the most part, is an event-based system. As a platform for ingesting data (in the form of BOMs), listening for and itself emitting signals on it, an event-based architecture makes sense conceptually. The majority of operations Dependency-Track performs happen asynchronously, without client interaction.</p> <p>On a technical level, Dependency-Track uses an in-memory publish-subscribe architecture, implemented using Java <code>ExecutorService</code>s. An <code>ExecutorService</code> can be thought of as a pool of worker threads, consuming from an internal task queue. Tasks can be submitted to an <code>ExecutorService</code>, which will then execute them one-by-one. As multiple threads work on the queue in  parallel, the order in which tasks are being processed is not guaranteed. Thread pool sizes can vary from one,  up to unbounded numbers of threads.</p> <p>In Dependency-Track, when an event is published, subscribers to the event are looked up.  Per API contract,  event subscribers must implement an <code>inform</code> method, which takes the published event as argument. For any given event, 0-N tasks will be enqueued to the <code>ExecutorService</code>'s task queue - one for each subscriber.</p> <p></p> <p>There are three <code>ExecutorService</code> instances in Dependency-Track:</p> <ul> <li><code>EventService</code></li> <li><code>SingleThreadedEventService</code></li> <li><code>NotificationService</code></li> </ul> <p><code>EventService</code> forms the primary worker pool. Its thread pool size defaults to <code>&lt;NUM_CPU&gt; * 4</code>.  A machine with a 4-core CPU will thus have a thread pool size of <code>16</code>. The size is configurable. Common tasks handled by this worker pool include:</p> <ul> <li>Processing of uploaded BOMs and VEXs</li> <li>Performing vulnerability analysis of components, projects, or the entire portfolio</li> <li>Performing repository meta analysis of components, projects, or the entire portfolio</li> <li>Calculation of metrics for components, projects, or the entire portfolio</li> </ul> <p><code>SingleThreadedEventService</code> is a worker pool with only a single thread. The purpose of this worker pool is execute tasks that must not be run in parallel. As such, it serves as a means to serialize task execution.  Common tasks handled by this worker pool include:</p> <ul> <li>Mirroring of the NVD vulnerability database</li> <li>Updating Lucene indexes on disk</li> </ul> <p><code>NotificationService</code> is a dedicated worker pool for dispatching notifications. Its thread pool size defaults to <code>4</code> and is not configurable.</p>"},{"location":"about/#limitations","title":"Limitations","text":"<p>While this architecture works great for small to medium workloads, it presents various challenges for larger ones:</p> <ol> <li>Not horizontally scalable. As pub-sub is happening entirely in-memory, it is not possible to distribute the work to multiple application instances. The only way to handle more load using this architecture is to scale vertically, e.g.</li> <li>Increasing <code>ExecutorService</code> thread pool sizes (<code>alpine.worker.threads</code>, <code>alpine.worker.thread.multiplier</code>)</li> <li>Increasing database connection pool sizes</li> <li>Increasing resource allocations for CPU, RAM, and potentially disk / network</li> <li>No ordering guarantees of events. As multiple threads work on a shared queue of tasks in parallel, there is no way of enforcing a reliable ordering of events. </li> <li>Limited fault-tolerance. If an instance of Dependency-Track goes down, planned or unplanned, all queued tasks are  gone. Not only does this impact business-as-usual operation, but also limits the times when upgrades can be applied.</li> <li>Shared, multipurpose task queue. A single task queue is used to process all kinds of events. This means that lots of events of a certain type can \"clog\" the task queue, preventing other types of events from being processed. This is further amplified if processing of events \"clogging\" the queue relies on external services, introducing further latency. Ideally, there should be a dedicated queue per event type, so that one busy queue doesn't block others.</li> <li>Prone to race conditions. As a consequence of (2), it is possible that multiple events addressing the same thing are processed in parallel, leading to race conditions in cache lookups or database operations. Race conditions would be an even bigger problem if the work was shared across multiple application instances, and would require distributed locking as a countermeasure, which is inherently hard to get right.</li> </ol> <p>In order to scale Dependency-Track beyond its current capabilities, a distributed messaging service is required.</p>"},{"location":"about/#related-issues","title":"Related Issues","text":"<p>On multiple occasions in the past, the Dependency-Track community raised questions about high availability (HA) deployments, and / or how to better scale the platform:</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/issues/218</li> <li>https://github.com/DependencyTrack/dependency-track/issues/903</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1210</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1856</li> </ul> <p>Note The work we've done so far does not make the API server highly available. However, it does address a substantial chunk of work that is required to make that happen.</p>"},{"location":"about/#why-kafka","title":"Why Kafka?","text":"<p>Kafka was chosen because it employs various concepts  that are advantageous for Dependency-Track:</p> <ul> <li>It supports publish-subscribe use cases based on topics and partitions</li> <li>Events with the same key are guaranteed to be sent to the same partition</li> <li>Order of events is guaranteed on the partition level</li> <li>Consumers can share the load of consuming from a topic by forming consumer groups<ul> <li>Minor drawback: maximum concurrency is bound to the number of partitions</li> </ul> </li> <li>It is distributed and fault-tolerant by design, replication is built-in</li> <li>Events are stored durably on the brokers, with various options to control retention</li> <li>Log compaction allows for fault-tolerant, stateful processing,   by streaming changes of a local key-value database to Kafka</li> <li>In certain cases, this can aid in reducing load on the database server</li> <li>Mature ecosystem around it, including a vast landscape of client libraries for various languages</li> <li>Kafka Streams with its support for     stateful transformations     in particular turned out to be a unique selling point for the Kafka ecosystem</li> <li>Mature cloud offerings for fully managed instances (see Options for running Kafka)</li> </ul> <p>The concept of partitioned topics turned out to be especially useful: We can rely on the fact that events with the same key always end up in the same partition, and are processed by only one consumer (within a consumer group) at a time. In case of vulnerability scanning, by choosing the component's PURL as event key, it can be guaranteed that only the first event triggers an HTTP request to OSS Index, while later events can be handled immediately from cache. There is no race condition anymore between lookup and population of the cache.</p> <p>We also found the first-class support for stateful processing incredibly useful in some cases, e.g.:</p> <ul> <li>Scatter-gather.    As used for scanning one component with multiple analyzers. Waiting for all analyzers to complete is a stateful   operation, that otherwise would require database access.</li> <li>Batching. Some external services allow for submitting multiple component identifiers per request.   With OSS Index, up to 128 package URLs can be sent in a single request. Submitting only one package URL at a   time would drastically increase end-to-end latency. It'd also present a greater risk of getting rate limited.</li> </ul> <p>That being said, Kafka does add a considerable amount of operational complexity. Historically, Kafka has depended on Apache ZooKeeper. Operating both Kafka and ZooKeeper is not something we wanted  to force DT users to do. Luckily, the Apache Kafka project has been working on removing the ZooKeeper dependency,  and replacing it with Kafka's own raft consensus protocol (KRaft).</p> <p>There are other, more light-weight, yet Kafka API-compatible broker implementations available, too.  Redpanda being the most popular. Redpanda is distributed in a single, self-contained binary and is optimal for deployments with limited resources. Having options like Redpanda available makes building a system on Kafka much more viable.</p> <p>For this reason in fact, we primarily develop with, and test against, Redpanda.</p>"},{"location":"about/#considered-alternatives","title":"Considered alternatives","text":"<p>Before choosing for Kafka, we looked at various other messaging systems.</p>"},{"location":"about/#apache-pulsar","title":"Apache Pulsar","text":"<p>Among all options, Pulsar was the most promising besides Kafka. Pulsar prides itself in  being truly cloud native, supporting tiered storage, and multiple messaging paradigms (pub-sub and queueing).  It has native support for negative acknowledgment  of messages, and message retries.  However, the Pulsar architecture consists not  only of Pulsar brokers, but also requires Apache ZooKeeper and Apache BookKeeper  clusters. We had to dismiss Pulsar for its operational complexity.</p>"},{"location":"about/#rabbitmq","title":"RabbitMQ","text":"<p>RabbitMQ is a popular message broker. It exceeds in cases where multiple worker processes need to work on a shared queue of tasks (similar to how Dependency-Track does it today). It can achieve a high level of concurrency, as there's no limit to how many consumers can consume from a queue. This high grade of concurrency comes with the cost of lost ordering, and high potential for race conditions. </p> <p>RabbitMQ supports Kafka-like partitioned streams via its Streams plugin. In the end, we decided against RabbitMQ, because brokers do not support key-based compaction, and its consumer libraries  in turn lack adequate support for fault-tolerant stateful operations.</p>"},{"location":"about/#liftbridge","title":"Liftbridge","text":"<p>Liftbridge is built on top of NATS and provides Kafka-like features. It is however not compatible with the Kafka API, as it uses a custom envelope protocol, and is heavily focused on Go. There are no managed service offerings for Liftbridge, leaving self-hosting as only option to run it.</p>"},{"location":"about/#options-for-running-kafka","title":"Options for running Kafka","text":"<p>When it comes to running Kafka in production, users will have the choice between various self-hosted and fully managed  Infrastructure as a Service (IaaS) solutions. The following table lists a few, but there will be more we don't know of:</p> Solution Type URL Apache Kafka Self-Hosted https://kafka.apache.org/quickstart Redpanda Self-Hosted / IaaS https://redpanda.com/ Strimzi Self-Hosted https://strimzi.io/ Aiven IaaS https://aiven.io/kafka AWS MSK IaaS https://aws.amazon.com/msk/ Azure Event Hubs IaaS https://azure.microsoft.com/en-us/products/event-hubs/ Confluent Cloud IaaS https://www.confluent.io/ Red Hat OpenShift Streams IaaS https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview <p>The wide range of mature IaaS offerings is a very important benefit of Kafka over other messaging systems we evaluated.</p>"},{"location":"about/#why-java","title":"Why Java?","text":"<p>We went with Java for now because it was the path of the least resistance for us. There is no intention to exclusively  use Java though. We are considering to use Go, and generally  are open to any technology that makes sense.</p>"},{"location":"about/#why-not-microservices","title":"Why not microservices?","text":"<p>The proposed architecture is based on the rough idea of domain services for now. This keeps the number of independent services manageable, while still allowing us to distribute the overall system load. If absolutely necessary, it is  possible to break this up even further. For example, instead of having one vulnerability-analyzer service,  the scanners for each vulnerability source (e.g. OSS Index, Snyk) could be separated out into dedicated microservices.</p>"},{"location":"architecture/design/workflow-state-tracking/","title":"Tracking of Workflow State for BOM Processing and Analysis","text":"<p>Note This document was extracted from #664.</p> <p>For CI/CD use-cases, Dependency-Track offers a mechanism that allows clients to poll whether the BOM they just uploaded is still being processed. \"Processing\" in this context refers to:</p> <ul> <li>Consumption (Parsing)</li> <li>Ingestion (Sync parsed data with database)</li> <li>Vulnerability analysis</li> <li>Policy Evaluation</li> </ul> <p>This is an important capability, allowing for implementation of quality gates and reporting in CI/CD pipelines.</p> <p>The mechanism works by assigning identifiers (UUIDs) to events in Alpine's in-memory event system. As long as an event associated to a given identifier can be found in its internal queue, the identifier is considered to be \"processing\":</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/blob/6153d286d1ac806462bc76cfe17d84a57c224671/src/main/java/org/dependencytrack/resources/v1/BomResource.java#L323-L342</li> <li>https://github.com/stevespringett/Alpine/blob/cd6aa7ed05376935ab32bc43819eba0e3a525b7f/alpine-infra/src/main/java/alpine/event/framework/BaseEventService.java#L158-L161</li> </ul> <p>Events can be chained, such that a <code>BomUploadEvent</code> will trigger a <code>VulnerabilityAnalysisEvent</code>, which will trigger a <code>PolicyEvaluationEvent</code>, and so on. The event identifier is inherited by chained events.</p> <p>As everything happens in-memory, this mechanism does not work when multiple instances of the API server are used in an active-active deployment. For the functionality to continue to work, the state of processing steps must be persistent to external storage.</p> <p>Decoupled from #633.</p>"},{"location":"architecture/design/workflow-state-tracking/#design","title":"Design","text":"<p>Note The goal for now is not to build a multi-purpose workflow engine, but to track state of one specific workflow. In a future iteration, we may invest more time into coming up with a generic workflow engine concept.</p>"},{"location":"architecture/design/workflow-state-tracking/#what-to-track","title":"What to track","text":"<ol> <li>BOM consumption (parsing, de-duplication, etc.)</li> <li>BOM processing (ingestion into database)</li> <li>Vulnerability analysis</li> <li>Repository metadata analysis</li> <li>Policy evaluation</li> <li>Metrics update</li> </ol> <p>The order of execution as of today is as follows:</p> <pre><code>flowchart LR\n    A(BOM_CONSUMPTION) --&gt; B(BOM_PROCESSING)\n    B --&gt; C(VULN_ANALYSIS)\n    B --&gt; D(REPO_META_ANALYSIS)\n    C --&gt; E(POLICY_EVALUATION)\n    E --&gt; F(METRICS_UPDATE)\n    B -. BOM contains no &lt;br/&gt;components to analyze .-&gt; F\n</code></pre> <p>Note Completion of repository metadata analysis can currently not be tracked. We'll need something similar to what we introduced in https://github.com/DependencyTrack/hyades-apiserver/pull/40 for vulnerability analysis completion tracking. For the initial implementation, it may be OK to not track it.</p>"},{"location":"architecture/design/workflow-state-tracking/#states","title":"States","text":"<p>There are multiple states a processing step can be in:</p> <ul> <li><code>PENDING</code>: Initial state</li> <li><code>COMPLETED</code>: Completion detected; No failures</li> <li><code>FAILED</code>: Completion detected; Failures</li> <li><code>CANCELLED</code>: Never started because a precondition failed<ul> <li>e.g. vulnerability analysis will be cancelled when BOM ingestion failed</li> </ul> </li> <li><code>NOT_APPLICABLE</code>: The step is not applicable to the subject of the analysis<ul> <li>e.g. the analysis was triggered for an existing project, without BOM upload</li> </ul> </li> <li><code>TIMED_OUT</code>: The deadline for the step completion was exceeded; Step is unlikely to complete</li> </ul> <pre><code>stateDiagram-v2\n    [*] --&gt; PENDING\n    PENDING --&gt; COMPLETED\n    COMPLETED --&gt; [*]\n    PENDING --&gt; FAILED\n    FAILED --&gt; [*]\n    PENDING --&gt; CANCELLED\n    CANCELLED --&gt; [*]\n    [*] --&gt; NOT_APPLICABLE\n    NOT_APPLICABLE --&gt; [*]\n    PENDING --&gt; TIMED_OUT\n    TIMED_OUT --&gt; COMPLETED\n    TIMED_OUT --&gt; FAILED\n</code></pre> <p>Each step is responsible for updating its own state.</p> <p>The overall state can be considered to be complete, if there's no step in <code>PENDING</code> state.</p> <p>When a step failure is detected, a \"failure reason\" message must be persisted. If multiple steps fail, (rough) failure details for each step must be available.</p> <p>There should be a deadline mechanism, which automatically transitions steps from <code>PENDING</code> into <code>TIMED_OUT</code> state. Steps in <code>TIMED_OUT</code> state communicate that it is unlikely that a terminal state can be reached (<code>COMPLETED</code>, <code>FAILED</code>). However, it is still possible (e.g. due to significant consumer lag, events may arrive late).</p>"},{"location":"architecture/design/workflow-state-tracking/#workflow","title":"Workflow","text":"<p>The API server will act as workflow orchestrator, kicking off new steps as needed. This implies that completion of a step must be registered by the API server.</p> <p>To illustrate how the initial stage of the workflow should look like:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ BomResource: Upload BOM\n    BomResource -&gt;&gt; BomResource: Validate BOM\n    BomResource -&gt;&gt; BomResource: Generate Correlation ID (UUID)\n    BomResource -&gt;&gt; Database: Create workflow steps\n    Note over BomResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=BOM_CONSUMPTION&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=BOM_PROCESSING&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Status=PENDING\n    BomResource -&gt;&gt; BomUploadProcessingTask: Dispatch BomUploadEvent\n    BomResource -&gt;&gt;- Client: Correlation ID\n    par\n        loop Continuously\n            Client -&gt;&gt; BomResource: Poll Status\n            Note over Client, BomResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n        end\n    and\n        BomUploadProcessingTask -&gt;&gt; Database: Update step start time\n        activate BomUploadProcessingTask\n        Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;StartedAt=&lt;NOW&gt;\n        BomUploadProcessingTask -&gt;&gt; BomUploadProcessingTask: Consume BOM&lt;br/&gt;(Parse, De-dupe)\n        alt Consumption succeeded\n            BomUploadProcessingTask -&gt;&gt; Database: Update step status\n            Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Process BOM&lt;br/&gt;(Insert, Update, Delete)\n            alt Processing succeeded\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.repo-meta-analysis.component\" topic\n            else Processing failed\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=VULN_ANALYSIS&lt;br/&gt;Status=CANCELLED\n            end\n        else Consumption failed\n            BomUploadProcessingTask -&gt;&gt; Database: Update step Status\n            Note over BomUploadProcessingTask, Database: Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=CANCELLED\n        end\n        deactivate BomUploadProcessingTask\n    end\n</code></pre> <p>When triggering a re-analysis of an already existing project, the workflow may be kicked off as follows:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ FindingResource: Re-Analyze project\n    FindingResource -&gt;&gt; FindingResource: Generate Correlation ID (UUID)\n    FindingResource -&gt;&gt; Database: Create workflow steps\n    Note over FindingResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=POLICY_EVALUATION&lt;br/&gt;Status=PENDING\n    FindingResource -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n    FindingResource -&gt;&gt;- Client: Correlation ID\n    loop Continuously\n        Client -&gt;&gt; FindingResource: Poll Status\n        Note over Client, FindingResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n    end\n</code></pre>"},{"location":"architecture/design/workflow-state-tracking/#proposed-table-schema","title":"Proposed Table Schema","text":"<p>Each step of the workflow will be represented in a dedicated row. This allows us to add or remove steps without altering the database schema (see original version of the schema further down below), or even add steps while the workflow is running. It also plays better with concurrent writes, as no two threads / instances will need to modify the same row.</p> Name Type Nullable Example ID <code>SERIAL</code> \u274c 1 PARENT_STEP_ID <code>SERIAL FK</code> \u2705 0 TOKEN <code>VARCHAR(36)</code> \u274c <code>484d9eaa-7ea4-4476-97d6-f36327b5a626</code> STARTED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> UPDATED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> STEP <code>VARCHAR(64)</code> \u274c <code>METRICS_UPDATE</code> STATUS <code>VARCHAR(64)</code> \u274c <code>PENDING</code> FAILURE_REASON <code>TEXT</code> \u2705 <code>Failed to acquire database connection</code> <p>Potential Future Improvements: * Do we need/want to capture the order in which steps are supposed to be executed? * Do we need/want to capture metadata of the overall workflow (who triggered it, when was it triggered, correlation id, ...)?</p> Original Version  | Name | Type | Nullable | Example | | :--- | :--- | :---: | :--- | | TOKEN | `VARCHAR(36)` | \u274c | `484d9eaa-7ea4-4476-97d6-f36327b5a626` | | STARTED_AT | `TIMESTAMP` | \u274c | `1999-01-08 04:05:06` | | UPDATED_AT | `TIMESTAMP` | \u274c | `1999-01-08 04:05:06` | | BOM_CONSUMPTION | `VARCHAR(64)` | \u274c | `PENDING` | | BOM_PROCESSING | `VARCHAR(64)` | \u274c | `PENDING` | | VULN_ANALYSIS | `VARCHAR(64)` | \u274c | `PENDING` | | REPO_META_ANALYSIS | `VARCHAR(64)` | \u274c | `PENDING` | | POLICY_EVALUATION | `VARCHAR(64)` | \u274c | `PENDING` | | METRICS_UPDATE | `VARCHAR(64)` | \u274c | `PENDING` | | FAILURE_REASON | `TEXT` | \u2705 | - |  `FAILURE_REASON` is a field of unlimited length. It either holds no value (`NULL`), or a JSON object listing failure details per step, e.g.:  <pre><code>{\n  \"BOM_PROCESSING\": \"Failed to acquire database connection\"\n}\n</code></pre> <p>Where applicable, the \"detailed\" status of a step is tracked in a dedicated table.</p>"},{"location":"architecture/design/workflow-state-tracking/#retention","title":"Retention","text":"<p>Rows in the table should be cleaned up on a recurring basis.</p> <p>This could be as simple as scheduling a job that executes this SQL query:</p> <pre><code>DELETE FROM org.dependencytrack.repometaanalyzer.model.WorkflowStep WHERE this.updatedAt &lt; :threshold\n</code></pre> <p>A retention time of 1-3 days since the last update should be reasonable.</p>"},{"location":"architecture/design/workflow-state-tracking/#rest-api-endpoints","title":"REST API endpoints","text":"<p>The existing endpoint to check whether a BOM is still being processed should continue to work as expected:</p> <p><pre><code>GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;\n</code></pre> <pre><code>{\n  \"processing\": true\n}\n</code></pre></p> <p>An additional endpoint may be added, which allows for retrieval of the individual step states:</p> <p><pre><code>GET /api/v1/workflow/token/&lt;CORRELATION_ID&gt;/status\n</code></pre> <pre><code>[\n  {\n    \"step\": \"BOM_CONSUMPTION\",\n    \"status\": \"COMPLETED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\"\n  },\n  {\n    \"step\": \"BOM_PROCESSING\",\n    \"status\": \"FAILED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\",\n    \"failureReason\": \"Failed to acquire database connection\"\n  },\n  {\n    \"step\": \"VULN_ANALYSIS\",\n    \"status\": \"CANCELLED\"\n  }\n]\n</code></pre></p> <p>If all a client cares about are vulnerability analysis results, they could stop polling immediately after <code>vulnerabilityAnalysis</code> transitions into the <code>COMPLETED</code> state.</p>"},{"location":"operations/database/","title":"Database","text":"<p>Dependency-Track requires a PostgreSQL, or PostgreSQL-compatible database to operate.</p> <p>The lowest supported version is 11. You are encouraged to use the newest available version.</p> <p>Depending on available resources, individual preferences, or organizational policies, you will have to choose between a managed, or self-hosted solution.</p>"},{"location":"operations/database/#managed-solutions","title":"Managed Solutions","text":"<p>The official PostgreSQL website hosts a list of well-known commercial hosting providers.</p> <p>Popular choices include:</p> <ul> <li>Amazon RDS for PostgreSQL</li> <li>Aiven for PostgreSQL</li> <li>Azure Database for PostgreSQL</li> <li>Google Cloud SQL for PostgreSQL</li> </ul> <p>We are not actively testing against cloud offerings. But as a rule of thumb, solutions offering \"vanilla\" PostgreSQL,  or extensions of it (for example Neon or Timescale), will most definitely work with Dependency-Track.</p> <p>The same is not necessarily true for platforms based on heavily modified PostgreSQL, or even entire re-implementations such as CockroachDB or YugabyteDB. Such solutions make certain trade-offs to achieve higher levels of scalability, which might impact functionality that Dependency-Track relies on. If you'd like to see support for those, please let us know!</p>"},{"location":"operations/database/#self-hosting","title":"Self-Hosting","text":""},{"location":"operations/database/#bare-metal-docker","title":"Bare Metal / Docker","text":"<p>For Docker deployments, use the official <code>postgres</code> image.</p> <p>Warning</p> <p>Do not use the <code>latest</code> tag! You may end up doing a major version upgrade without knowing it, ultimately breaking your database! Pin the tag to at least the major version (e.g. <code>16</code>), or better yet the minor version (e.g. <code>16.2</code>). Refer to Upgrades to upgrade instructions.</p> <p>For bare metal deloyments, it's usually best to install PostgreSQL from your distribution's package repository. See for example:</p> <ul> <li>PostgreSQL instructions for Debian</li> <li>Install and configure PostgreSQL on Ubuntu</li> <li>Using PostgreSQL with Red Hat Enterprise Linux</li> </ul> <p>To get the most out of your Dependency-Track installation, we recommend to run PostgreSQL on a separate machine than the application containers. You want PostgreSQL to be able to leverage the entire machine's resources, without being impacted by other applications.</p> <p>For smaller and non-critical deployments, it is totally fine to run everything on a single machine.</p>"},{"location":"operations/database/#configuration","title":"Configuration","text":"<p>You should be aware that the default PostgreSQL configuration is extremely conservative. It is intended to make PostgreSQL usable on minimal hardware, which is great for testing, but can seriously cripple performance in production environments. Not adjusting it to your specific setup will most certainly leave performance on the table.</p> <p>If you're lucky enough to have access to professional database administrators, ask them for help. They will know your organisation's best practices and can guide you in adjusting it for Dependency-Track.</p> <p>If you're not as lucky, we can wholeheartedly recommend PGTune. Given a bit of basic info about your system, it will provide a sensible baseline configuration. For the DB Type option, select <code>Online transaction processing system</code>.</p> <p></p> <p>The <code>postgresql.conf</code> is usually located at <code>/var/lib/postgresql/data/postgresql.conf</code>. Most of these settings require a restart of the application.</p> <p>In a Docker Compose setup, you can alternatively apply the desired configuration via command line flags. For example:</p> <pre><code>services:\n  postgres:\n    image: postgres:16\n    command: &gt;-\n      postgres\n        -c 'shared_buffers=2GB'\n        -c 'effective_cache_size=6GB'\n</code></pre> <p>Note</p> <p>Got more tips to configure or tune PostgreSQL, that may be helpful to others? We'd love to include it in the docs, please do raise a PR!</p>"},{"location":"operations/database/#upgrades","title":"Upgrades","text":"<p>Follow the official upgrading guide. Be sure to select the version of the documentation that corresponds to the PostgreSQL version you are running.</p> <p>Warning</p> <p>Pay attention to the fact that major version upgrades usually require a backup-and-restore cycle, due to potentially breaking changes in the underlying data storage format. Minor version upgrades are usually safe to perform in a rolling manor.</p>"},{"location":"operations/database/#kubernetes","title":"Kubernetes","text":"<p>We generally advise against running PostgreSQL on Kubernetes, unless you really know what you're doing. Wielding heavy machinery such as Postgres Operator is not something you should do lightheartedly.</p> <p>If you know what you're doing, you definitely don't need advice from us. Smooth sailing! \u2693\ufe0f</p>"},{"location":"operations/database/#schema-migrations","title":"Schema Migrations","text":"<p>Schema migrations are performed automatically by the API server upon startup. It leverages Liquibase for doing so. There is usually no manual action required when upgrading from an older Dependency-Track version, unless explicitly stated otherwise in the release notes.</p> <p>This behavior can be turned off by setting <code>database.run.migrations</code>  on the API server container to <code>false</code>.</p> <p>It is possible to use different credentials for migrations than for the application itself. This can be achieved with the following options:</p> <ul> <li><code>database.migration.url</code></li> <li><code>database.migration.username</code></li> <li><code>database.migration.password</code></li> </ul> <p>The above with default to the main database credentials if not provided explicitly.</p>"},{"location":"reference/topics/","title":"Topics","text":"Name Partitions Config <code>dtrack.notification.analyzer</code> 3 <code>dtrack.notification.bom</code> 3 <code>dtrack.notification.configuration</code> 3 <code>dtrack.notification.datasource-mirroring</code> 3 <code>dtrack.notification.file-system</code> 3 <code>dtrack.notification.integration</code> 3 <code>dtrack.notification.new-vulnerability</code> 3 <code>dtrack.notification.new-vulnerable-dependency</code> 3 <code>dtrack.notification.policy-violation</code> 3 <code>dtrack.notification.project-audit-change</code> 3 <code>dtrack.notification.project-created</code> 3 <code>dtrack.notification.repository</code> 3 <code>dtrack.notification.vex</code> 3 <code>dtrack.notification.project-vuln-analysis-complete</code> <sup>3</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>dtrack.repo-meta-analysis.component</code><sup>1A</sup> 3 <code>dtrack.repo-meta-analysis.result</code> 3 <code>dtrack.vuln-analysis.component</code><sup>1B</sup> 3 <code>dtrack.vuln-analysis.result</code> 3 <code>dtrack.vuln-analysis.result.processed</code> 3 <code>dtrack.vuln-analysis.scanner.result</code><sup>1B</sup> 3 <code>dtrack.vulnerability</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.epss</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.digest</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.mirror.command</code><sup>2</sup> 1 <code>dtrack.vulnerability.mirror.state</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>hyades-repository-meta-analyzer-command-by-purl-coordinates-repartition</code><sup>1A</sup> 3 <code>hyades-vulnerability-analyzer-completed-scans-table-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-last-update-store-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-table-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-ossindex-batch-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-ossindex-retry-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-scan-task-internal-repartition</code> 3 <code>hyades-vulnerability-analyzer-scan-task-ossindex-repartition</code><sup>1C</sup> 3 <code>hyades-vulnerability-analyzer-scan-task-snyk-repartition</code><sup>1D</sup> 3 <code>hyades-vulnerability-analyzer-snyk-batch-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-snyk-retry-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <p><sup>1X</sup> The topic is subject to co-partitioning requirements <sup>2</sup> The partition number of this topic should not be changed <sup>3</sup> To use this notification, the PROJECT_VULN_ANALYSIS_COMPLETE group needs to be manually added through api call to the notify on condition The put request to be executed is here</p>"},{"location":"reference/topics/#co-partitioning-requirements","title":"Co-Partitioning Requirements","text":"<p>Some topics must be co-partitioned, meaning they must share the exact same number of partitions. Applications using those topics will not work correctly when this is not the case.</p>"},{"location":"reference/configuration/api-server/","title":"API Server","text":""},{"location":"reference/configuration/api-server/#cors","title":"CORS","text":""},{"location":"reference/configuration/api-server/#alpinecorsallowcredentials","title":"alpine.cors.allow.credentials","text":"<p>Controls the content of the <code>Access-Control-Allow-Credentials</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_CORS_ALLOW_CREDENTIALS</code>"},{"location":"reference/configuration/api-server/#alpinecorsallowheaders","title":"alpine.cors.allow.headers","text":"<p>Controls the content of the <code>Access-Control-Allow-Headers</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, *</code> ENV <code>ALPINE_CORS_ALLOW_HEADERS</code>"},{"location":"reference/configuration/api-server/#alpinecorsallowmethods","title":"alpine.cors.allow.methods","text":"<p>Controls the content of the <code>Access-Control-Allow-Methods</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>GET POST PUT DELETE OPTIONS</code> ENV <code>ALPINE_CORS_ALLOW_METHODS</code>"},{"location":"reference/configuration/api-server/#alpinecorsalloworigin","title":"alpine.cors.allow.origin","text":"<p>Controls the content of the <code>Access-Control-Allow-Origin</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>*</code> ENV <code>ALPINE_CORS_ALLOW_ORIGIN</code>"},{"location":"reference/configuration/api-server/#alpinecorsenabled","title":"alpine.cors.enabled","text":"<p>Defines whether Cross Origin Resource Sharing  (CORS) headers shall be included in REST API responses.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_CORS_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpinecorsexposeheaders","title":"alpine.cors.expose.headers","text":"<p>Controls the content of the <code>Access-Control-Expose-Headers</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count</code> ENV <code>ALPINE_CORS_EXPOSE_HEADERS</code>"},{"location":"reference/configuration/api-server/#alpinecorsmaxage","title":"alpine.cors.max.age","text":"<p>Controls the content of the <code>Access-Control-Max-Age</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>integer</code> Default <code>3600</code> ENV <code>ALPINE_CORS_MAX_AGE</code>"},{"location":"reference/configuration/api-server/#database","title":"Database","text":""},{"location":"reference/configuration/api-server/#alpinedatabasepassword","title":"alpine.database.password","text":"<p>Specifies the password to use when authenticating to the database.  </p> Required false Type <code>string</code> Default <code>dtrack</code> ENV <code>ALPINE_DATABASE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolenabled","title":"alpine.database.pool.enabled","text":"<p>Specifies if the database connection pool is enabled.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_DATABASE_POOL_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolidletimeout","title":"alpine.database.pool.idle.timeout","text":"<p>This property controls the maximum amount of time that a connection is  allowed to sit idle in the pool.  The property can be set globally for both transactional and non-transactional  connection pools, or for each pool type separately. When both global and pool-specific  properties are set, the pool-specific properties take precedence.  </p> Required false Type <code>integer</code> Default <code>300000</code> ENV <code>ALPINE_DATABASE_POOL_IDLE_TIMEOUT</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolmaxlifetime","title":"alpine.database.pool.max.lifetime","text":"<p>This property controls the maximum lifetime of a connection in the pool.  An in-use connection will never be retired, only when it is closed will  it then be removed.  The property can be set globally for both transactional and non-transactional  connection pools, or for each pool type separately. When both global and pool-specific  properties are set, the pool-specific properties take precedence.  </p> Required false Type <code>integer</code> Default <code>600000</code> ENV <code>ALPINE_DATABASE_POOL_MAX_LIFETIME</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolmaxsize","title":"alpine.database.pool.max.size","text":"<p>This property controls the maximum size that the pool is allowed to reach,  including both idle and in-use connections.  The property can be set globally for both transactional and non-transactional  connection pools, or for each pool type separately. When both global and pool-specific  properties are set, the pool-specific properties take precedence.  </p> Required false Type <code>integer</code> Default <code>20</code> ENV <code>ALPINE_DATABASE_POOL_MAX_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolminidle","title":"alpine.database.pool.min.idle","text":"<p>This property controls the minimum number of idle connections in the pool.  This value should be equal to or less than <code>alpine.database.pool.max.size</code>.  Warning: If the value is less than <code>alpine.database.pool.max.size</code>,  <code>alpine.database.pool.idle.timeout</code> will have no effect.  The property can be set globally for both transactional and non-transactional  connection pools, or for each pool type separately. When both global and pool-specific  properties are set, the pool-specific properties take precedence.  </p> Required false Type <code>integer</code> Default <code>10</code> ENV <code>ALPINE_DATABASE_POOL_MIN_IDLE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolnontxidletimeout","title":"alpine.database.pool.nontx.idle.timeout","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.idle.timeout}</code> ENV <code>ALPINE_DATABASE_POOL_NONTX_IDLE_TIMEOUT</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolnontxmaxlifetime","title":"alpine.database.pool.nontx.max.lifetime","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.max.lifetime}</code> ENV <code>ALPINE_DATABASE_POOL_NONTX_MAX_LIFETIME</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolnontxmaxsize","title":"alpine.database.pool.nontx.max.size","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.max.size}</code> ENV <code>ALPINE_DATABASE_POOL_NONTX_MAX_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolnontxminidle","title":"alpine.database.pool.nontx.min.idle","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.min.idle}</code> ENV <code>ALPINE_DATABASE_POOL_NONTX_MIN_IDLE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepooltxidletimeout","title":"alpine.database.pool.tx.idle.timeout","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.idle.timeout}</code> ENV <code>ALPINE_DATABASE_POOL_TX_IDLE_TIMEOUT</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepooltxmaxlifetime","title":"alpine.database.pool.tx.max.lifetime","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.max.lifetime}</code> ENV <code>ALPINE_DATABASE_POOL_TX_MAX_LIFETIME</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepooltxmaxsize","title":"alpine.database.pool.tx.max.size","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.max.size}</code> ENV <code>ALPINE_DATABASE_POOL_TX_MAX_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepooltxminidle","title":"alpine.database.pool.tx.min.idle","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.min.idle}</code> ENV <code>ALPINE_DATABASE_POOL_TX_MIN_IDLE</code>"},{"location":"reference/configuration/api-server/#alpinedatabaseurl","title":"alpine.database.url","text":"<p>Specifies the JDBC URL to use when connecting to the database.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>jdbc:postgresql://localhost:5432/dtrack</code> ENV <code>ALPINE_DATABASE_URL</code>"},{"location":"reference/configuration/api-server/#alpinedatabaseusername","title":"alpine.database.username","text":"<p>Specifies the username to use when authenticating to the database.  </p> Required false Type <code>string</code> Default <code>dtrack</code> ENV <code>ALPINE_DATABASE_USERNAME</code>"},{"location":"reference/configuration/api-server/#databasemigrationpassword","title":"database.migration.password","text":"<p>Defines the database password for executing migrations.  If not set, the value of <code>alpine.database.password</code> will be used.  </p> Required false Type <code>string</code> Default <code>${alpine.database.password}</code> ENV <code>DATABASE_MIGRATION_PASSWORD</code>"},{"location":"reference/configuration/api-server/#databasemigrationurl","title":"database.migration.url","text":"<p>Defines the database JDBC URL to use when executing migrations.  If not set, the value of <code>alpine.database.url</code> will be used.  Should generally not be set, unless TLS authentication is used,  and custom connection variables are required.  </p> Required false Type <code>string</code> Default <code>${alpine.database.url}</code> ENV <code>DATABASE_MIGRATION_URL</code>"},{"location":"reference/configuration/api-server/#databasemigrationusername","title":"database.migration.username","text":"<p>Defines the database user for executing migrations.  If not set, the value of <code>alpine.database.username</code> will be used.  </p> Required false Type <code>string</code> Default <code>${alpine.database.username}</code> ENV <code>DATABASE_MIGRATION_USERNAME</code>"},{"location":"reference/configuration/api-server/#runmigrations","title":"run.migrations","text":"<p>Defines whether database migrations should be executed on startup.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>RUN_MIGRATIONS</code>"},{"location":"reference/configuration/api-server/#general","title":"General","text":""},{"location":"reference/configuration/api-server/#alpineapikeyprefix","title":"alpine.api.key.prefix","text":"<p>Defines the prefix to be used for API keys. A maximum prefix length of 251  characters is supported. The prefix may also be left empty.  </p> Required false Type <code>string</code> Default <code>odt_</code> ENV <code>ALPINE_API_KEY_PREFIX</code>"},{"location":"reference/configuration/api-server/#alpinebcryptrounds","title":"alpine.bcrypt.rounds","text":"<p>Specifies the number of bcrypt rounds to use when hashing a user's password.  The higher the number the more secure the password, at the expense of  hardware resources and additional time to generate the hash.  </p> Required true Type <code>integer</code> Default <code>14</code> ENV <code>ALPINE_BCRYPT_ROUNDS</code>"},{"location":"reference/configuration/api-server/#alpinedatadirectory","title":"alpine.data.directory","text":"<p>Defines the path to the data directory. This directory will hold logs,  keys, and any database or index files along with application-specific  files or directories.  </p> Required true Type <code>string</code> Default <code>~/.dependency-track</code> ENV <code>ALPINE_DATA_DIRECTORY</code>"},{"location":"reference/configuration/api-server/#alpineprivatekeypath","title":"alpine.private.key.path","text":"<p>Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.  The keys will be generated upon first startup if they do not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/private.key</code> Example <code>/var/run/secrets/private.key</code> ENV <code>ALPINE_PRIVATE_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#alpinepublickeypath","title":"alpine.public.key.path","text":"<p>Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.  The keys will be generated upon first startup if they do not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/public.key</code> Example <code>/var/run/secrets/public.key</code> ENV <code>ALPINE_PUBLIC_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#alpinesecretkeypath","title":"alpine.secret.key.path","text":"<p>Defines the path to the secret key to be used for data encryption and decryption.  The key will be generated upon first startup if it does not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/secret.key</code> ENV <code>ALPINE_SECRET_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#bomuploadprocessingtrxflushthreshold","title":"bom.upload.processing.trx.flush.threshold","text":"<p>Defines the number of write operations to perform during BOM processing before changes are flushed to the database.  Smaller values may lower memory usage of the API server, whereas higher values will improve performance as fewer  network round-trips to the database are necessary.  </p> Required false Type <code>integer</code> Default <code>10000</code> ENV <code>BOM_UPLOAD_PROCESSING_TRX_FLUSH_THRESHOLD</code>"},{"location":"reference/configuration/api-server/#integritycheckenabled","title":"integrity.check.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INTEGRITY_CHECK_ENABLED</code>"},{"location":"reference/configuration/api-server/#integrityinitializerenabled","title":"integrity.initializer.enabled","text":"<p>Specifies whether the Integrity Initializer shall be enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INTEGRITY_INITIALIZER_ENABLED</code>"},{"location":"reference/configuration/api-server/#tmpdelaybomprocessednotification","title":"tmp.delay.bom.processed.notification","text":"<p>Delays the BOM_PROCESSED notification until the vulnerability analysis associated with a given BOM upload  is completed. The intention being that it is then \"safe\" to query the API for any identified vulnerabilities.  This is specifically for cases where polling the /api/v1/bom/token/ endpoint is not feasible.  THIS IS A TEMPORARY FUNCTIONALITY AND MAY BE REMOVED IN FUTURE RELEASES WITHOUT FURTHER NOTICE.   Required false Type <code>boolean</code> Default <code>false</code> ENV <code>TMP_DELAY_BOM_PROCESSED_NOTIFICATION</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicyanalysisenabled","title":"vulnerability.policy.analysis.enabled","text":"<p>Defines whether vulnerability policy analysis is enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>VULNERABILITY_POLICY_ANALYSIS_ENABLED</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleauthpassword","title":"vulnerability.policy.bundle.auth.password","text":"<p>For nginx server, if username and bearer token both are provided, basic auth will be used,  else the auth header will be added based on the not null values  Defines the password to be used for basic authentication against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_AUTH_PASSWORD</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleauthusername","title":"vulnerability.policy.bundle.auth.username","text":"<p>Defines the username to be used for basic authentication against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_AUTH_USERNAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundlebearertoken","title":"vulnerability.policy.bundle.bearer.token","text":"<p>Defines the token to be used as bearerAuth against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_BEARER_TOKEN</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundlesourcetype","title":"vulnerability.policy.bundle.source.type","text":"<p>Defines the type of source from which policy bundles are being fetched from.  Required when <code>vulnerability.policy.bundle.url</code> is set.  </p> Required false Type <code>enum</code> Valid Values <code>[nginx, s3]</code> Default <code>NGINX</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_SOURCE_TYPE</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleurl","title":"vulnerability.policy.bundle.url","text":"<p>Defines where to fetch the policy bundle from.For S3, just the base url needs to be provided with port  For nginx, the whole url with bundle name needs to be given  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>http://example.com:80/bundles/bundle.zip</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_URL</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3accesskey","title":"vulnerability.policy.s3.access.key","text":"<p>S3 related details. Access key, secret key, bucket name and bundle names are mandatory if S3 is chosen. Region is optional  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_ACCESS_KEY</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3bucketname","title":"vulnerability.policy.s3.bucket.name","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_BUCKET_NAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3bundlename","title":"vulnerability.policy.s3.bundle.name","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_BUNDLE_NAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3region","title":"vulnerability.policy.s3.region","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_REGION</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3secretkey","title":"vulnerability.policy.s3.secret.key","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_SECRET_KEY</code>"},{"location":"reference/configuration/api-server/#workflowretentionduration","title":"workflow.retention.duration","text":"<p>Defines the duration for how long workflow data is being retained, after all steps transitioned into a non-terminal  state (CANCELLED, COMPLETED, FAILED, NOT_APPLICABLE).  The duration must be specified in ISO8601 notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).  </p> Required false Type <code>duration</code> Default <code>P3D</code> ENV <code>WORKFLOW_RETENTION_DURATION</code>"},{"location":"reference/configuration/api-server/#workflowsteptimeoutduration","title":"workflow.step.timeout.duration","text":"<p>Defines the duration for how long a workflow step is allowed to remain in PENDING state  after being started. If this duration is exceeded, workflow steps will transition into the TIMED_OUT state.  If they remain in TIMED_OUT for the same duration, they will transition to the FAILED state.  The duration must be specified in ISO8601 notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).  </p> Required false Type <code>duration</code> Default <code>PT1H</code> ENV <code>WORKFLOW_STEP_TIMEOUT_DURATION</code>"},{"location":"reference/configuration/api-server/#http","title":"HTTP","text":""},{"location":"reference/configuration/api-server/#alpinehttpproxyaddress","title":"alpine.http.proxy.address","text":"<p>HTTP proxy address. If set, then <code>alpine.http.proxy.port</code> must be set too.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>proxy.example.com</code> ENV <code>ALPINE_HTTP_PROXY_ADDRESS</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxypassword","title":"alpine.http.proxy.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_HTTP_PROXY_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxyport","title":"alpine.http.proxy.port","text":"Required false Type <code>integer</code> Default <code>null</code> Example <code>8888</code> ENV <code>ALPINE_HTTP_PROXY_PORT</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxyusername","title":"alpine.http.proxy.username","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_HTTP_PROXY_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutconnection","title":"alpine.http.timeout.connection","text":"<p>Defines the connection timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>30</code> ENV <code>ALPINE_HTTP_TIMEOUT_CONNECTION</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutpool","title":"alpine.http.timeout.pool","text":"<p>Defines the request timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>60</code> ENV <code>ALPINE_HTTP_TIMEOUT_POOL</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutsocket","title":"alpine.http.timeout.socket","text":"<p>Defines the socket / read timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>30</code> ENV <code>ALPINE_HTTP_TIMEOUT_SOCKET</code>"},{"location":"reference/configuration/api-server/#alpinenoproxy","title":"alpine.no.proxy","text":"Required false Type <code>string</code> Default <code>null</code> Example <code>localhost,127.0.0.1</code> ENV <code>ALPINE_NO_PROXY</code>"},{"location":"reference/configuration/api-server/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorconsumerautooffsetreset","title":"alpine.kafka.processor.epss.mirror.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorconsumergroupid","title":"alpine.kafka.processor.epss.mirror.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrormaxbatchsize","title":"alpine.kafka.processor.epss.mirror.max.batch.size","text":"Required true Type <code>integer</code> Default <code>500</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_MAX_BATCH_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrormaxconcurrency","title":"alpine.kafka.processor.epss.mirror.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorprocessingorder","title":"alpine.kafka.processor.epss.mirror.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorretryinitialdelayms","title":"alpine.kafka.processor.epss.mirror.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorretrymaxdelayms","title":"alpine.kafka.processor.epss.mirror.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorretrymultiplier","title":"alpine.kafka.processor.epss.mirror.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorretryrandomizationfactor","title":"alpine.kafka.processor.epss.mirror.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultconsumerautooffsetreset","title":"alpine.kafka.processor.repo.meta.analysis.result.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultconsumergroupid","title":"alpine.kafka.processor.repo.meta.analysis.result.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultmaxconcurrency","title":"alpine.kafka.processor.repo.meta.analysis.result.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultprocessingorder","title":"alpine.kafka.processor.repo.meta.analysis.result.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultretryinitialdelayms","title":"alpine.kafka.processor.repo.meta.analysis.result.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultretrymaxdelayms","title":"alpine.kafka.processor.repo.meta.analysis.result.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultretrymultiplier","title":"alpine.kafka.processor.repo.meta.analysis.result.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultretryrandomizationfactor","title":"alpine.kafka.processor.repo.meta.analysis.result.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorconsumerautooffsetreset","title":"alpine.kafka.processor.vuln.mirror.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorconsumergroupid","title":"alpine.kafka.processor.vuln.mirror.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrormaxconcurrency","title":"alpine.kafka.processor.vuln.mirror.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorprocessingorder","title":"alpine.kafka.processor.vuln.mirror.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>partition</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorretryinitialdelayms","title":"alpine.kafka.processor.vuln.mirror.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorretrymaxdelayms","title":"alpine.kafka.processor.vuln.mirror.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorretrymultiplier","title":"alpine.kafka.processor.vuln.mirror.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorretryrandomizationfactor","title":"alpine.kafka.processor.vuln.mirror.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultconsumerautooffsetreset","title":"alpine.kafka.processor.vuln.scan.result.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultconsumergroupid","title":"alpine.kafka.processor.vuln.scan.result.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultmaxconcurrency","title":"alpine.kafka.processor.vuln.scan.result.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedconsumerautooffsetreset","title":"alpine.kafka.processor.vuln.scan.result.processed.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedconsumerfetchminbytes","title":"alpine.kafka.processor.vuln.scan.result.processed.consumer.fetch.min.bytes","text":"Required true Type <code>integer</code> Default <code>524288</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_FETCH_MIN_BYTES</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedconsumergroupid","title":"alpine.kafka.processor.vuln.scan.result.processed.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedconsumermaxpollrecords","title":"alpine.kafka.processor.vuln.scan.result.processed.consumer.max.poll.records","text":"Required true Type <code>integer</code> Default <code>10000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_MAX_POLL_RECORDS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedmaxbatchsize","title":"alpine.kafka.processor.vuln.scan.result.processed.max.batch.size","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_MAX_BATCH_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedmaxconcurrency","title":"alpine.kafka.processor.vuln.scan.result.processed.max.concurrency","text":"Required true Type <code>integer</code> Default <code>1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedprocessingorder","title":"alpine.kafka.processor.vuln.scan.result.processed.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>unordered</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedretryinitialdelayms","title":"alpine.kafka.processor.vuln.scan.result.processed.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedretrymaxdelayms","title":"alpine.kafka.processor.vuln.scan.result.processed.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedretrymultiplier","title":"alpine.kafka.processor.vuln.scan.result.processed.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedretryrandomizationfactor","title":"alpine.kafka.processor.vuln.scan.result.processed.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessingorder","title":"alpine.kafka.processor.vuln.scan.result.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultretryinitialdelayms","title":"alpine.kafka.processor.vuln.scan.result.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultretrymaxdelayms","title":"alpine.kafka.processor.vuln.scan.result.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultretrymultiplier","title":"alpine.kafka.processor.vuln.scan.result.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultretryrandomizationfactor","title":"alpine.kafka.processor.vuln.scan.result.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#applicationid","title":"application.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver</code> ENV <code>APPLICATION_ID</code>"},{"location":"reference/configuration/api-server/#kafkaautooffsetreset","title":"kafka.auto.offset.reset","text":"Required false Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"Required true Type <code>string</code> Default <code>null</code> Example <code>localhost:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/api-server/#kafkakeystorepassword","title":"kafka.keystore.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_KEYSTORE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#kafkakeystorepath","title":"kafka.keystore.path","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_KEYSTORE_PATH</code>"},{"location":"reference/configuration/api-server/#kafkamtlsenabled","title":"kafka.mtls.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>KAFKA_MTLS_ENABLED</code>"},{"location":"reference/configuration/api-server/#kafkasecurityprotocol","title":"kafka.security.protocol","text":"Required false Type <code>enum</code> Valid Values <code>[PLAINTEXT, SASL_SSL_PLAINTEXT, SASL_PLAINTEXT, SSL]</code> Default <code>null</code> ENV <code>KAFKA_SECURITY_PROTOCOL</code>"},{"location":"reference/configuration/api-server/#kafkatlsenabled","title":"kafka.tls.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>KAFKA_TLS_ENABLED</code>"},{"location":"reference/configuration/api-server/#kafkatopicprefix","title":"kafka.topic.prefix","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/api-server/#kafkatruststorepassword","title":"kafka.truststore.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TRUSTSTORE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#kafkatruststorepath","title":"kafka.truststore.path","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TRUSTSTORE_PATH</code>"},{"location":"reference/configuration/api-server/#ldap","title":"LDAP","text":""},{"location":"reference/configuration/api-server/#alpineldapattributemail","title":"alpine.ldap.attribute.mail","text":"<p>Specifies the LDAP attribute used to store a users email address  </p> Required false Type <code>string</code> Default <code>mail</code> ENV <code>ALPINE_LDAP_ATTRIBUTE_MAIL</code>"},{"location":"reference/configuration/api-server/#alpineldapattributename","title":"alpine.ldap.attribute.name","text":"<p>Specifies the Attribute that identifies a users ID.    Example (Microsoft Active Directory):  <ul><li><code>userPrincipalName</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>uid</code></li></ul> </p> Required false Type <code>string</code> Default <code>userPrincipalName</code> ENV <code>ALPINE_LDAP_ATTRIBUTE_NAME</code>"},{"location":"reference/configuration/api-server/#alpineldapauthusernameformat","title":"alpine.ldap.auth.username.format","text":"<p>Specifies if the username entered during login needs to be formatted prior  to asserting credentials against the directory. For Active Directory, the  userPrincipal attribute typically ends with the domain, whereas the  samAccountName attribute and other directory server implementations do not.  The %s variable will be substituted with the username asserted during login.    Example (Microsoft Active Directory):  <ul><li><code>%s@example.com</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>%s</code></li></ul> </p> Required false Type <code>string</code> Default <code>null</code> Example <code>%s@example.com</code> ENV <code>ALPINE_LDAP_AUTH_USERNAME_FORMAT</code>"},{"location":"reference/configuration/api-server/#alpineldapbasedn","title":"alpine.ldap.basedn","text":"<p>Specifies the base DN that all queries should search from  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>dc=example,dc=com</code> ENV <code>ALPINE_LDAP_BASEDN</code>"},{"location":"reference/configuration/api-server/#alpineldapbindpassword","title":"alpine.ldap.bind.password","text":"<p>If anonymous access is not permitted, specify a password for the username  used to bind.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_BIND_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpineldapbindusername","title":"alpine.ldap.bind.username","text":"<p>If anonymous access is not permitted, specify a username with limited access  to the directory, just enough to perform searches. This should be the fully  qualified DN of the user.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_BIND_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpineldapenabled","title":"alpine.ldap.enabled","text":"<p>Defines if LDAP will be used for user authentication. If enabled,  <code>alpine.ldap.*</code> properties should be set accordingly.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpineldapgroupsfilter","title":"alpine.ldap.groups.filter","text":"<p>Specifies the LDAP search filter used to retrieve all groups from the directory.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=group)(objectCategory=Group))</code> ENV <code>ALPINE_LDAP_GROUPS_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapgroupssearchfilter","title":"alpine.ldap.groups.search.filter","text":"<p>Specifies the LDAP search filter used to search for groups by their name.  The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(cn={SEARCH_TERM}))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames)(cn={SEARCH_TERM}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))</code> ENV <code>ALPINE_LDAP_GROUPS_SEARCH_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapsecurityauth","title":"alpine.ldap.security.auth","text":"<p>Specifies the LDAP security authentication level to use. Its value is one of  the following strings: \"none\", \"simple\", \"strong\". If this property is empty  or unspecified, the behaviour is determined by the service provider.  </p> Required false Type <code>enum</code> Valid Values <code>[none, simple, strong]</code> Default <code>simple</code> ENV <code>ALPINE_LDAP_SECURITY_AUTH</code>"},{"location":"reference/configuration/api-server/#alpineldapserverurl","title":"alpine.ldap.server.url","text":"<p>Specifies the LDAP server URL.    Examples (Microsoft Active Directory):  <ul> <li><code>ldap://ldap.example.com:3268</code></li> <li><code>ldaps://ldap.example.com:3269</code></li> </ul>  Examples (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul> <li><code>ldap://ldap.example.com:389</code></li> <li><code>ldaps://ldap.example.com:636</code></li> </ul> </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_SERVER_URL</code>"},{"location":"reference/configuration/api-server/#alpineldapteamsynchronization","title":"alpine.ldap.team.synchronization","text":"<p>This option will ensure that team memberships for LDAP users are dynamic and  synchronized with membership of LDAP groups. When a team is mapped to an LDAP  group, all local LDAP users will automatically be assigned to the team if  they are a member of the group the team is mapped to. If the user is later  removed from the LDAP group, they will also be removed from the team. This  option provides the ability to dynamically control user permissions via an  external directory.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_TEAM_SYNCHRONIZATION</code>"},{"location":"reference/configuration/api-server/#alpineldapusergroupsfilter","title":"alpine.ldap.user.groups.filter","text":"<p>Specifies the LDAP search filter to use to query a user and retrieve a list  of groups the user is a member of. The <code>{USER_DN}</code> variable will be substituted  with the actual value of the users DN at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(member={USER_DN}))</code></li></ul>  Example (Microsoft Active Directory - with nested group support):  <ul><li><code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code> ENV <code>ALPINE_LDAP_USER_GROUPS_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapuserprovisioning","title":"alpine.ldap.user.provisioning","text":"<p>Specifies if mapped LDAP accounts are automatically created upon successful  authentication. When a user logs in with valid credentials but an account has  not been previously provisioned, an authentication failure will be returned.  This allows admins to control specifically which ldap users can access the  system and which users cannot. When this value is set to true, a local ldap  user will be created and mapped to the ldap account automatically. This  automatic provisioning only affects authentication, not authorization.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_USER_PROVISIONING</code>"},{"location":"reference/configuration/api-server/#alpineldapuserssearchfilter","title":"alpine.ldap.users.search.filter","text":"<p>Specifies the LDAP search filter used to search for users by their name.  The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(cn={SEARCH_TERM}))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=inetOrgPerson)(cn={SEARCH_TERM}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*))</code> ENV <code>ALPINE_LDAP_USERS_SEARCH_FILTER</code>"},{"location":"reference/configuration/api-server/#observability","title":"Observability","text":""},{"location":"reference/configuration/api-server/#alpinemetricsauthpassword","title":"alpine.metrics.auth.password","text":"<p>Defines the password required to access metrics.  Has no effect when <code>alpine.metrics.auth.username</code> is not set.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_METRICS_AUTH_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinemetricsauthusername","title":"alpine.metrics.auth.username","text":"<p>Defines the username required to access metrics.  Has no effect when <code>alpine.metrics.auth.password</code> is not set.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_METRICS_AUTH_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpinemetricsenabled","title":"alpine.metrics.enabled","text":"<p>Defines whether Prometheus metrics will be exposed.  If enabled, metrics will be available via the /metrics endpoint.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_METRICS_ENABLED</code>"},{"location":"reference/configuration/api-server/#openid-connect","title":"OpenID Connect","text":""},{"location":"reference/configuration/api-server/#alpineoidcclientid","title":"alpine.oidc.client.id","text":"<p>Defines the client ID to be used for OpenID Connect.  The client ID should be the same as the one configured for the frontend,  and will only be used to validate ID tokens.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_OIDC_CLIENT_ID</code>"},{"location":"reference/configuration/api-server/#alpineoidcenabled","title":"alpine.oidc.enabled","text":"<p>Defines if OpenID Connect will be used for user authentication.  If enabled, <code>alpine.oidc.*</code> properties should be set accordingly.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpineoidcissuer","title":"alpine.oidc.issuer","text":"<p>Defines the issuer URL to be used for OpenID Connect.  This issuer MUST support provider configuration via the <code>/.well-known/openid-configuration</code> endpoint.  See also:  <ul> <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata</li> <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig</li> </ul> </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_OIDC_ISSUER</code>"},{"location":"reference/configuration/api-server/#alpineoidcteamsynchronization","title":"alpine.oidc.team.synchronization","text":"<p>This option will ensure that team memberships for OpenID Connect users are dynamic and  synchronized with membership of OpenID Connect groups or assigned roles. When a team is  mapped to an OpenID Connect group, all local OpenID Connect users will automatically be  assigned to the team if they are a member of the group the team is mapped to. If the user  is later removed from the OpenID Connect group, they will also be removed from the team. This  option provides the ability to dynamically control user permissions via the identity provider.  Note that team synchronization is only performed during user provisioning and after successful  authentication.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_TEAM_SYNCHRONIZATION</code>"},{"location":"reference/configuration/api-server/#alpineoidcteamsclaim","title":"alpine.oidc.teams.claim","text":"<p>Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint.  The claim must be an array of strings. Most public identity providers do not support group or role management.  When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint  will most likely need to be configured.  </p> Required false Type <code>string</code> Default <code>groups</code> ENV <code>ALPINE_OIDC_TEAMS_CLAIM</code>"},{"location":"reference/configuration/api-server/#alpineoidcuserprovisioning","title":"alpine.oidc.user.provisioning","text":"<p>Specifies if mapped OpenID Connect accounts are automatically created upon successful  authentication. When a user logs in with a valid access token but an account has  not been previously provisioned, an authentication failure will be returned.  This allows admins to control specifically which OpenID Connect users can access the  system and which users cannot. When this value is set to true, a local OpenID Connect  user will be created and mapped to the OpenID Connect account automatically. This  automatic provisioning only affects authentication, not authorization.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_USER_PROVISIONING</code>"},{"location":"reference/configuration/api-server/#alpineoidcusernameclaim","title":"alpine.oidc.username.claim","text":"<p>Defines the name of the claim that contains the username in the provider's userinfo endpoint.  Common claims are <code>name</code>, <code>username</code>, <code>preferred_username</code> or <code>nickname</code>.  See also:  <ul> <li>https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse</li> </ul> </p> Required false Type <code>string</code> Default <code>name</code> ENV <code>ALPINE_OIDC_USERNAME_CLAIM</code>"},{"location":"reference/configuration/api-server/#task-execution","title":"Task Execution","text":""},{"location":"reference/configuration/api-server/#alpineworkerthreadmultiplier","title":"alpine.worker.thread.multiplier","text":"<p>Defines a multiplier that is used to calculate the number of threads used  by the event subsystem. This property is only used when <code>alpine.worker.threads</code>  is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most)  16 worker threads.  </p> Required true Type <code>integer</code> Default <code>4</code> ENV <code>ALPINE_WORKER_THREAD_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpineworkerthreads","title":"alpine.worker.threads","text":"<p>Defines the number of worker threads that the event subsystem will consume.  Events occur asynchronously and are processed by the Event subsystem. This  value should be large enough to handle most production situations without  introducing much delay, yet small enough not to pose additional load on an  already resource-constrained server.  A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This  can further be tweaked using the <code>alpine.worker.thread.multiplier</code> property.  </p> Required true Type <code>integer</code> Default <code>0</code> ENV <code>ALPINE_WORKER_THREADS</code>"},{"location":"reference/configuration/api-server/#task-scheduling","title":"Task Scheduling","text":""},{"location":"reference/configuration/api-server/#integritymetainitializerlockatleastforinmillis","title":"integrityMetaInitializer.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>INTEGRITYMETAINITIALIZER_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#integritymetainitializerlockatmostforinmillis","title":"integrityMetaInitializer.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>INTEGRITYMETAINITIALIZER_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskcomponentidentificationlockatleastforinmillis","title":"task.componentIdentification.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_COMPONENTIDENTIFICATION_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskcomponentidentificationlockatmostforinmillis","title":"task.componentIdentification.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_COMPONENTIDENTIFICATION_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskcroncomponentidentification","title":"task.cron.componentIdentification","text":"<p>Schedule task every 6 hrs at 25th min  </p> Required true Type <code>cron</code> Default <code>25 */6 * * *</code> ENV <code>TASK_CRON_COMPONENTIDENTIFICATION</code>"},{"location":"reference/configuration/api-server/#taskcrondefectdojosync","title":"task.cron.defectdojo.sync","text":"<p>Schedule task every 24 hrs at 02:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_CRON_DEFECTDOJO_SYNC</code>"},{"location":"reference/configuration/api-server/#taskcronfortifysscsync","title":"task.cron.fortify.ssc.sync","text":"<p>Schedule task every 24 hrs at 02:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_CRON_FORTIFY_SSC_SYNC</code>"},{"location":"reference/configuration/api-server/#taskcronintegrityinitializer","title":"task.cron.integrityInitializer","text":"<p>Schedule task at 0 min past every 12th hr  </p> Required true Type <code>cron</code> Default <code>0 */12 * * *</code> ENV <code>TASK_CRON_INTEGRITYINITIALIZER</code>"},{"location":"reference/configuration/api-server/#taskcronkennasync","title":"task.cron.kenna.sync","text":"<p>Schedule task every 24 hrs at 02:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_CRON_KENNA_SYNC</code>"},{"location":"reference/configuration/api-server/#taskcronldapsync","title":"task.cron.ldapSync","text":"<p>Schedule task every 6 hrs at 0th min  </p> Required true Type <code>cron</code> Default <code>0 */6 * * *</code> ENV <code>TASK_CRON_LDAPSYNC</code>"},{"location":"reference/configuration/api-server/#taskcronmetricsportfolio","title":"task.cron.metrics.portfolio","text":"<p>Schedule task for 10th minute of every hour  </p> Required true Type <code>cron</code> Default <code>10 * * * *</code> ENV <code>TASK_CRON_METRICS_PORTFOLIO</code>"},{"location":"reference/configuration/api-server/#taskcronmetricsvulnerability","title":"task.cron.metrics.vulnerability","text":"<p>Schedule task for 40th minute of every hour  </p> Required true Type <code>cron</code> Default <code>40 * * * *</code> ENV <code>TASK_CRON_METRICS_VULNERABILITY</code>"},{"location":"reference/configuration/api-server/#taskcronmirrorgithub","title":"task.cron.mirror.github","text":"<p>Schedule task every 24 hrs at 02:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_CRON_MIRROR_GITHUB</code>"},{"location":"reference/configuration/api-server/#taskcronmirrornist","title":"task.cron.mirror.nist","text":"<p>Schedule task every 24 hrs at 04:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 4 * * *</code> ENV <code>TASK_CRON_MIRROR_NIST</code>"},{"location":"reference/configuration/api-server/#taskcronmirrorosv","title":"task.cron.mirror.osv","text":"<p>Schedule task every 24 hrs at 03:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 3 * * *</code> ENV <code>TASK_CRON_MIRROR_OSV</code>"},{"location":"reference/configuration/api-server/#taskcronrepometaanalysis","title":"task.cron.repoMetaAnalysis","text":"<p>Schedule task every 24 hrs at 01:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 1 * * *</code> ENV <code>TASK_CRON_REPOMETAANALYSIS</code>"},{"location":"reference/configuration/api-server/#taskcronvulnanalysis","title":"task.cron.vulnAnalysis","text":"<p>Schedule task every 24hrs at 06:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 6 * * *</code> ENV <code>TASK_CRON_VULNANALYSIS</code>"},{"location":"reference/configuration/api-server/#taskcronvulnscancleanup","title":"task.cron.vulnScanCleanUp","text":"<p>Schedule task at 8:05 UTC on Wednesday every week  </p> Required true Type <code>cron</code> Default <code>5 8 * * 4</code> ENV <code>TASK_CRON_VULNSCANCLEANUP</code>"},{"location":"reference/configuration/api-server/#taskcronvulnerabilitypolicybundlefetch","title":"task.cron.vulnerability.policy.bundle.fetch","text":"<p>Schedule task every 5 minutes  </p> Required true Type <code>cron</code> Default <code>*/5 * * * *</code> ENV <code>TASK_CRON_VULNERABILITY_POLICY_BUNDLE_FETCH</code>"},{"location":"reference/configuration/api-server/#taskcronworkflowstatecleanup","title":"task.cron.workflow.state.cleanup","text":"<p>Schedule task every 15 minutes  </p> Required true Type <code>cron</code> Default <code>*/15 * * * *</code> ENV <code>TASK_CRON_WORKFLOW_STATE_CLEANUP</code>"},{"location":"reference/configuration/api-server/#taskldapsynclockatleastforinmillis","title":"task.ldapSync.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_LDAPSYNC_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskldapsynclockatmostforinmillis","title":"task.ldapSync.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_LDAPSYNC_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmetricsportfoliolockatleastforinmillis","title":"task.metrics.portfolio.lockAtLeastForInMillis","text":"<p>Specifies minimum amount of time for which the lock should be kept.  Its main purpose is to prevent execution from multiple nodes in case of really short tasks and clock difference between the nodes.  </p> Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_METRICS_PORTFOLIO_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmetricsportfoliolockatmostforinmillis","title":"task.metrics.portfolio.lockAtMostForInMillis","text":"<p>Specifies how long the lock should be kept in case the executing node dies.  This is just a fallback, under normal circumstances the lock is released as soon the tasks finishes.  Set lockAtMostFor to a value which is much longer than normal execution time. Default value is 15min  Lock will be extended dynamically till task execution is finished  </p> Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_METRICS_PORTFOLIO_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmetricsvulnerabilitylockatleastforinmillis","title":"task.metrics.vulnerability.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_METRICS_VULNERABILITY_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmetricsvulnerabilitylockatmostforinmillis","title":"task.metrics.vulnerability.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_METRICS_VULNERABILITY_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmirrorepsslockatleastforinmillis","title":"task.mirror.epss.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_MIRROR_EPSS_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmirrorepsslockatmostforinmillis","title":"task.mirror.epss.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_MIRROR_EPSS_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskportfoliorepometaanalysislockatleastforinmillis","title":"task.portfolio.repoMetaAnalysis.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_PORTFOLIO_REPOMETAANALYSIS_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskportfoliorepometaanalysislockatmostforinmillis","title":"task.portfolio.repoMetaAnalysis.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_PORTFOLIO_REPOMETAANALYSIS_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskportfoliovulnanalysislockatleastforinmillis","title":"task.portfolio.vulnAnalysis.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_PORTFOLIO_VULNANALYSIS_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskportfoliovulnanalysislockatmostforinmillis","title":"task.portfolio.vulnAnalysis.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_PORTFOLIO_VULNANALYSIS_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskschedulerinitialdelay","title":"task.scheduler.initial.delay","text":"<p>Scheduling tasks after 3 minutes (3601000) of starting application  </p> Required true Type <code>integer</code> Default <code>180000</code> ENV <code>TASK_SCHEDULER_INITIAL_DELAY</code>"},{"location":"reference/configuration/api-server/#taskschedulerpollinginterval","title":"task.scheduler.polling.interval","text":"<p>Cron expressions for tasks have the precision of minutes so polling every minute  </p> Required true Type <code>integer</code> Default <code>60000</code> ENV <code>TASK_SCHEDULER_POLLING_INTERVAL</code>"},{"location":"reference/configuration/api-server/#taskworkflowstatecleanuplockatleastforinmillis","title":"task.workflow.state.cleanup.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_WORKFLOW_STATE_CLEANUP_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskworkflowstatecleanuplockatmostforinmillis","title":"task.workflow.state.cleanup.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_WORKFLOW_STATE_CLEANUP_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/mirror-service/","title":"Mirror Service","text":""},{"location":"reference/configuration/mirror-service/#datasource","title":"Datasource","text":""},{"location":"reference/configuration/mirror-service/#mirrordatasourcegithubalias-sync-enabled","title":"mirror.datasource.github.alias-sync-enabled","text":"<p>Defines whether vulnerability aliases should be parsed from GitHub Advisories.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>MIRROR_DATASOURCE_GITHUB_ALIAS_SYNC_ENABLED</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourcegithubapi-key","title":"mirror.datasource.github.api-key","text":"<p>Defines the API key to use for accessing GitHub's GraphQL API.  It is required in order to use the GitHub datasource.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>MIRROR_DATASOURCE_GITHUB_API_KEY</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourcegithubbase-url","title":"mirror.datasource.github.base-url","text":"<p>Defines the URL of the GitHub GraphQL API endpoint.  </p> Required false Type <code>string</code> Default <code>https://api.github.com/graphql</code> ENV <code>MIRROR_DATASOURCE_GITHUB_BASE_URL</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourcenvdapi-key","title":"mirror.datasource.nvd.api-key","text":"<p>Defines the API key to use for accessing the NVD's REST API.  An API key can be requested via the following form: https://nvd.nist.gov/developers/request-an-api-key </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>MIRROR_DATASOURCE_NVD_API_KEY</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourcenvdbase-url","title":"mirror.datasource.nvd.base-url","text":"<p>Defines the URL of the NVD REST API.  </p> Required false Type <code>string</code> Default <code>https://services.nvd.nist.gov/rest/json/cves/2.0</code> ENV <code>MIRROR_DATASOURCE_NVD_BASE_URL</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourcenvdnum-threads","title":"mirror.datasource.nvd.num-threads","text":"<p>Defines the number of threads with which data is being downloaded from the NVD REST API concurrently.  Has no effect unless <code>mirror.datasource.nvd.api-key</code> is provided.  </p> Required false Type <code>integer</code> Default <code>4</code> ENV <code>MIRROR_DATASOURCE_NVD_NUM_THREADS</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourceosvalias-sync-enabled","title":"mirror.datasource.osv.alias-sync-enabled","text":"<p>Defines whether vulnerability aliases should be parsed from OSV.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourceosvbase-url","title":"mirror.datasource.osv.base-url","text":"<p>Defines the URL of the OSV storage bucket.  </p> Required false Type <code>string</code> Default <code>https://osv-vulnerabilities.storage.googleapis.com</code> ENV <code>MIRROR_DATASOURCE_OSV_BASE_URL</code>"},{"location":"reference/configuration/mirror-service/#http","title":"HTTP","text":""},{"location":"reference/configuration/mirror-service/#quarkushttpport","title":"quarkus.http.port","text":"<p>HTTP port to listen on. Application metrics will be available via this port.  </p> Required false Type <code>integer</code> Default <code>8093</code> ENV <code>QUARKUS_HTTP_PORT</code>"},{"location":"reference/configuration/mirror-service/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/mirror-service/#kafka-streamscommitintervalms","title":"kafka-streams.commit.interval.ms","text":"<p>Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.  The Kafka default of <code>30s</code> has been modified to <code>5s</code>.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms for details.  </p> Required true Type <code>integer</code> Default <code>5000</code> ENV <code>KAFKA_STREAMS_COMMIT_INTERVAL_MS</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsdeserializationcount","title":"kafka-streams.exception.thresholds.deserialization.count","text":"<p>Defines the threshold for records failing to be deserialized within <code>kafka-streams.exception.thresholds.deserialization.interval</code>.  Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsdeserializationinterval","title":"kafka-streams.exception.thresholds.deserialization.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.deserialization.count</code> records are  allowed to fail deserialization. Deserialization failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsprocessingcount","title":"kafka-streams.exception.thresholds.processing.count","text":"<p>Defines the threshold for records failing to be processed within <code>kafka-streams.exception.thresholds.processing.interval</code>.  Processing failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>50</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsprocessinginterval","title":"kafka-streams.exception.thresholds.processing.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.processing.count</code> records are  allowed to fail processing. Processing failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsproductioncount","title":"kafka-streams.exception.thresholds.production.count","text":"<p>Defines the threshold for records failing to be produced within <code>kafka-streams.exception.thresholds.production.interval</code>.  Production failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsproductioninterval","title":"kafka-streams.exception.thresholds.production.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.production.count</code> records are  allowed to fail producing. Production failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsnumstreamthreads","title":"kafka-streams.num.stream.threads","text":"<p>The number of threads to allocate for stream processing tasks.  Note that Specifying a number higher than the number of input partitions provides no additional benefit,  as excess threads will simply run idle.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads for details.  </p> Required true Type <code>integer</code> Default <code>3</code> ENV <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code>"},{"location":"reference/configuration/mirror-service/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"<p>Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for details.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>broker-01.acme.com:9092,broker-02.acme.com:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/mirror-service/#kafkamaxrequestsize","title":"kafka.max.request.size","text":"<p>Defines the maximum size of a Kafka producer request in bytes.    Some messages like Bill of Vulnerabilities can be bigger than the default 1MiB.  Since the size check is performed before records are compressed, this value may need to be increased  even though the compressed value is much smaller. The Kafka default of 1MiB has been raised to 2MiB.    Refer to https://kafka.apache.org/documentation/#producerconfigs_max.request.size for details.  </p> Required true Type <code>integer</code> Default <code>2097152</code> ENV <code>KAFKA_MAX_REQUEST_SIZE</code>"},{"location":"reference/configuration/mirror-service/#kafkatopicprefix","title":"kafka.topic.prefix","text":"<p>Defines an optional prefix to assume for all Kafka topics the application  consumes from, or produces to. The prefix will also be prepended to the  application's consumer group ID.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>acme-</code> ENV <code>KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/mirror-service/#quarkuskafka-streamsapplication-id","title":"quarkus.kafka-streams.application-id","text":"<p>Defines the ID to uniquely identify this application in the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_application.id for details.  </p> Required false Type <code>string</code> Default <code>${kafka.topic.prefix}hyades-mirror-service</code> ENV <code>QUARKUS_KAFKA_STREAMS_APPLICATION_ID</code>"},{"location":"reference/configuration/mirror-service/#observability","title":"Observability","text":""},{"location":"reference/configuration/mirror-service/#quarkuslogconsolejson","title":"quarkus.log.console.json","text":"<p>Defines whether logs should be written in JSON format.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>QUARKUS_LOG_CONSOLE_JSON</code>"},{"location":"reference/configuration/overview/","title":"Configuration","text":"<p>All available configuration options used by all applications are listed in their respective <code>application.properties</code>. Options can be provided via environment variables as well, refer to the Quarkus docs for details.</p> <p>Not all options are supposed to be tweaked by users though. This document contains an overview of all options that are expected to be changed by users.</p>"},{"location":"reference/configuration/overview/#kafka-topic-configuration","title":"Kafka Topic Configuration","text":"<p>Kafka topics (including internal topics) can be configured with custom prefix. In order to provide custom prefix, below environment variable can be used.</p> Environment Variable Description Default Required <code>KAFKA_TOPIC_PREFIX</code> Prefix for topic names - \u274c"},{"location":"reference/configuration/overview/#notification-publisher","title":"Notification Publisher","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>PARALLEL_CONSUMER_MAX_CONCURRENCY</code> Number of threads to process notifications with <code>6</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_INITIAL_DELAY</code> Initial delay before retrying notification delivery <code>3S</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_MULTIPLIER</code> Multiplier for retry delays <code>0.3</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_RANDOMIZATION_FACTOR</code> Randomization factory for jitter in retry delays <code>0.3</code> \u274c <code>PARALLEL_CONSUMER_RETRY_MAX_DURATION</code> Maximum duration of delays between retry attempts <code>2M</code> \u2705 <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#repository-meta-analyzer","title":"Repository Meta Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#vulnerability-analyzer","title":"Vulnerability Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>STATE_STORE_TYPE</code> Whether to use in-memory or persistent (RocksDB) Kafka Streams state stores <code>in_memory</code> \u2705 <code>STATE_STORE_ROCKS_DB_COMPACTION_STYLE</code> Compaction style to use for RocksDB state stores - \u274c <code>STATE_STORE_ROCKS_DB_COMPRESSION_TYPE</code> Compression type to use for RocksDB state stores - \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <code>SCANNER_INTERNAL_ENABLED</code> Enable the internal vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_ENABLED</code> Enable the OSS Index vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by OssIndex analyzer. This depends on standard and custom supported package urls <code>cargo,composer,gem,hex,maven,npm,nuget,pypi,rpm,conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by ossindex. These are not part of the standard package urls list <code>conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_API_USERNAME</code> OSS Index API username - \u274c <code>SCANNER_OSSINDEX_API_TOKEN</code> OSS Index API token - \u274c <code>SCANNER_OSSINDEX_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> \u274c <code>SCANNER_OSSINDEX_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSS Index <code>false</code> \u274c <code>SCANNER_SNYK_ENABLED</code> Enable the Snyk vulnerability scanner <code>false</code> \u274c <code>SCANNER_SNYK_API_ORG_ID</code> Snyk organization ID - When Snyk is enabled <code>SCANNER_SNYK_API_TOKENS</code> Comma-separated list of Snyk API tokens - When Snyk is enabled <code>SCANNER_SNYK_API_VERSION</code> Version of the Snyk API to use <code>2022-12-15</code> When Snyk is enabled <code>SCANNER_SNYK_SEVERITY_SOURCE_PRIORITY</code> Priority of preferred source for vulnerability severities <code>nvd,snyk,redhat,suse</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_SIZE</code> Max size of batch at which it will be submitted <code>100</code> When Snyk is enabled <code>SCANNER_SNYK_ALIAS_SYNC_ENABLED</code> Enable alias syncing for Snyk <code>false</code> \u274c <code>SCANNER_SNYK_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by Snyk analyzer. This depends on supported standard and custom package urls <code>cargo,cocoapods,composer,gem,generic,hex,maven,npm,nuget,pypi,swift,golang</code> \u274c <code>SCANNER_SNYK_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by Snyk. These are not part of the standard package urls list <code>cocoapods,apk,swift</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#mirror-service","title":"Mirror Service","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>MIRROR_DATASOURCE_GITHUB_ALIAS_SYNC_ENABLED</code> Enable alias syncing for GitHub Advisories <code>false</code> \u274c <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSV <code>false</code> \u274c <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"usage/policy-compliance/expressions/","title":"Expressions","text":""},{"location":"usage/policy-compliance/expressions/#introduction","title":"Introduction","text":"<p>Dependency-Track allows policy conditions to be defined using the Common Expression Language (CEL), enabling more flexibility, and more control compared to predefined conditions.</p> <p>To use CEL, simply select the subject <code>Expression</code> when adding a new condition. A code editor will appear in which expressions can be provided.</p> <p></p> <p>In addition to the expression itself, it's necessary to specify a violation type, which may be any of <code>License</code>, <code>Operational</code>, or <code>Security</code>. The violation type aids in communicating what kind of risk is introduced by the condition being matched.</p>"},{"location":"usage/policy-compliance/expressions/#syntax","title":"Syntax","text":"<p>The CEL syntax is similar to other C-style languages like Java and JavaScript. However, CEL is not Turing-complete. As such, it does not support constructs like <code>if</code> statements or loops (i.e. <code>for</code>, <code>while</code>).</p> <p>As a compensation for missing loops, CEL offers macros like <code>all</code>, <code>exists</code>, <code>exists_one</code>, <code>map</code>, and <code>filter</code>. Refer to the macros documentation for more details, or have a look at the examples to see how they may be utilized in practice.</p> <p>CEL syntax is described thoroughly in the official language definition.</p>"},{"location":"usage/policy-compliance/expressions/#evaluation-context","title":"Evaluation Context","text":"<p>Conditions are scoped to individual components. Each condition is evaluated for every single component in a project.</p> <p>The context in which expressions are evaluated in contains the following variables:</p> Variable Type Description <code>component</code> <code>Component</code> The component being evaluated <code>project</code> <code>Project</code> The project the component is part of <code>vulns</code> <code>list(Vulnerability)</code> Vulnerabilities the component is affected by"},{"location":"usage/policy-compliance/expressions/#best-practices","title":"Best Practices","text":"<ol> <li>Keep expressions simple and concise. The more complex an expression becomes, the harder it gets to determine why it did or did not match. Use policy operators (<code>Any</code>, <code>All</code>) to chain multiple expressions if practical.</li> <li>Call functions last. Custom functions involve additional computation that is more expensive than simple field accesses. Performing any checks on fields first, and calling functions last, oftentimes allows evaluation to short-circuit.</li> <li>Remove conditions that are no longer needed. Dependency-Track analyzes the configured expressions to determine what data it has to load from the database in order to evaluate them. The more fields are being accessed, the more data has to be loaded. Removal of outdated conditions thus has a direct positive performance impact.</li> </ol>"},{"location":"usage/policy-compliance/expressions/#examples","title":"Examples","text":""},{"location":"usage/policy-compliance/expressions/#component-age","title":"Component age","text":"<p>Besides out-of-date versions, component age is another indicator of potential risk. Components may be on the latest available version, but still be 20 years old. </p> <p>Component age can be evaluated using the <code>compare_age</code> function. The first function argument  is a numeric comparator (<code>&lt;</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&gt;=</code>), and the second is a duration in ISO8601 notation.</p> <p>The following expression matches Components that are two years old, or even older:</p> <pre><code>component.compare_age(\"&gt;=\", \"P2Y\")\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#component-blacklist","title":"Component blacklist","text":"<p>The following expression matches on the Component's Package URL, using a regular expression in RE2 syntax. Additionally, it checks whether the Component's version falls into a given vers range, consisting of multiple constraints.</p> <pre><code>component.purl.matches(\"^pkg:maven/com.acme/acme-lib\\\\b.*\")\n  &amp;&amp; component.matches_range(\"vers:maven/&gt;0|&lt;1|!=0.2.4\")\n</code></pre> <p>The expression will match:</p> <ul> <li><code>pkg:maven/com.acme/acme-lib@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.9.9</code></li> </ul> <p>but not:</p> <ul> <li><code>pkg:maven/com.acme/acme-library@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.2.4</code></li> </ul> <p><code>matches_range</code> currently supports the following versioning schemes:</p> Versioning Scheme Ecosystem <code>deb</code> Debian / Ubuntu <code>generic</code> Generic / Any <code>golang</code> Go <code>maven</code> Java / Maven <code>npm</code> JavaScript / NodeJS <code>rpm</code> CentOS / Fedora / Red Hat / SUSE <p>Note</p> <p>If the ecosystem of the component(s) to match against is known upfront, it's good practice to use the according versioning scheme in <code>matches_range</code>. This helps with accuracy, as versioning schemes have different nuances across ecosystems, which makes comparisons error-prone.</p>"},{"location":"usage/policy-compliance/expressions/#dependency-graph-traversal","title":"Dependency graph traversal","text":"<p>The following expression matches Components that are a (possibly transitive) dependency of a Component with name <code>foo</code>, but only if a Component with name <code>bar</code> is also present in the Project.</p> <pre><code>component.is_dependency_of(v1.Component{name: \"foo\"})\n  &amp;&amp; project.depends_on(v1.Component{name: \"bar\"})\n</code></pre> <p><code>is_dependency_of</code> and <code>depends_on</code> lookups currently support the following Component fields:</p> <ul> <li><code>uuid</code></li> <li><code>group</code></li> <li><code>name</code></li> <li><code>version</code></li> <li><code>classifier</code></li> <li><code>cpe</code></li> <li><code>purl</code></li> <li><code>swid_tag_id</code></li> <li><code>internal</code></li> </ul> <p>Initially, only exact matches on those fields are supported. In the future, more sophisticated matching options will be added.</p> <p>Note</p> <p>When constructing objects like Component on-the-fly, it is necessary to use their version namespace, i.e. <code>v1</code>. This is required in order to perform type checking, as well as ensuring backward compatibility.</p>"},{"location":"usage/policy-compliance/expressions/#license-blacklist","title":"License blacklist","text":"<p>The following expression matches Components that are not internal to the organization, and have either:</p> <ul> <li>No resolved License at all</li> <li>A resolved License that is not part of the <code>Permissive</code> license group</li> </ul> <pre><code>!component.is_internal &amp;&amp; (\n  !has(component.resolved_license)\n    || component.resolved_license.groups.exisits(licenseGroup, \n         licenseGroup.name == \"Permissive\")\n)\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerability-blacklist","title":"Vulnerability blacklist","text":"<p>The following expression matches Components in Projects tagged as <code>3rd-party</code>, with at least one Vulnerability being any of the given blacklisted IDs.</p> <pre><code>\"3rd-party\" in project.tags\n  &amp;&amp; vulns.exists(vuln, vuln.id in [\n       \"CVE-2017-5638\",  // struts RCE\n       \"CVE-2021-44228\", // log4shell\n       \"CVE-2022-22965\", // spring4shell\n     ])\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerabilities-with-high-severity-in-public-facing-projects","title":"Vulnerabilities with high severity in public facing projects","text":"<p>The following expression matches Components in Projects tagged as <code>public-facing</code>, with at least one <code>HIGH</code> or <code>CRITICAL</code> Vulnerability, where the CVSSv3 attack vector is <code>Network</code>.</p> <pre><code>\"public-facing\" in project.tags\n  &amp;&amp; vulns.exists(vuln,\n    vuln.severity in [\"HIGH\", \"CRITICAL\"]\n      &amp;&amp; vuln.cvssv3_vector.matches(\".*/AV:N/.*\")\n  )\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#reference","title":"Reference","text":""},{"location":"usage/policy-compliance/expressions/#types","title":"Types","text":""},{"location":"usage/policy-compliance/expressions/#component","title":"<code>Component</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>is_internal</code> <code>bool</code> Is internal? <code>md5</code> <code>string</code> MD5 hash <code>sha1</code> <code>string</code> SHA1 hash <code>sha256</code> <code>string</code> SHA256 hash <code>sha384</code> <code>string</code> SHA384 hash <code>sha512</code> <code>string</code> SHA512 hash <code>sha3_256</code> <code>string</code> SHA3-256 hash <code>sha3_384</code> <code>string</code> SHA3-384 hash <code>sha3_512</code> <code>string</code> SHA3-512 hash <code>blake2b_256</code> <code>string</code> BLAKE2b-256 hash <code>blake2b_384</code> <code>string</code> BLAKE2b-384 hash <code>blake2b_512</code> <code>string</code> BLAKE2b-512 hash <code>blake3</code> <code>string</code> BLAKE3 hash <code>license_name</code> <code>string</code> License name (if unresolved) <code>license_expression</code> <code>string</code> SPDX license expression <code>resolved_license</code> <code>License</code> Resolved license <code>published_at</code> <code>google.protobuf.Timestamp</code> When the component was published <code>latest_version</code> <code>string</code> Latest known version"},{"location":"usage/policy-compliance/expressions/#license","title":"<code>License</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> SPDX license ID <code>name</code> <code>string</code> License name <code>groups</code> <code>list(License.Group)</code> Groups this license is included in <code>is_osi_approved</code> <code>bool</code> Is OSI-approved? <code>is_fsf_libre</code> <code>bool</code> Is included in FSF license list? <code>is_deprecated_id</code> <code>bool</code> Uses a deprecated SPDX license ID? <code>is_custom</code> <code>bool</code> Is custom / not included in SPDX license list?"},{"location":"usage/policy-compliance/expressions/#licensegroup","title":"<code>License.Group</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>name</code> <code>string</code> Group name"},{"location":"usage/policy-compliance/expressions/#project","title":"<code>Project</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>is_active</code> <code>bool</code> Is active? <code>tags</code> <code>list(string)</code> Tags <code>properties</code> <code>list(Project.Property)</code> Properties <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>last_bom_import</code> <code>google.protobuf.Timestamp</code>"},{"location":"usage/policy-compliance/expressions/#projectproperty","title":"<code>Project.Property</code>","text":"Field Type Description <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>value</code> <code>string</code> <code>type</code> <code>string</code>"},{"location":"usage/policy-compliance/expressions/#vulnerability","title":"<code>Vulnerability</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>CVE-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>NVD</code>) <code>aliases</code> <code>list(Vulnerability.Alias)</code> Known aliases <code>cwes</code> <code>list(int)</code> CWE IDs <code>created</code> <code>google.protobuf.Timestamp</code> When the vulnerability was created <code>published</code> <code>google.protobuf.Timestamp</code> When the vulnerability was published <code>updated</code> <code>google.protobuf.Timestamp</code> Then the vulnerability was updated <code>severity</code> <code>string</code> <code>cvssv2_base_score</code> <code>double</code> CVSSv2 base score <code>cvssv2_impact_subscore</code> <code>double</code> CVSSv2 impact sub score <code>cvssv2_exploitability_subscore</code> <code>double</code> CVSSv2 exploitability sub score <code>cvssv2_vector</code> <code>string</code> CVSSv2 vector <code>cvssv3_base_score</code> <code>double</code> CVSSv3 base score <code>cvssv3_impact_subscore</code> <code>double</code> CVSSv3 impact sub score <code>cvssv3_exploitability_subscore</code> <code>double</code> CVSSv3 exploitability sub score <code>cvssv3_vector</code> <code>string</code> CVSSv3 vector <code>owasp_rr_likelihood_score</code> <code>double</code> OWASP Risk Rating likelihood score <code>owasp_rr_technical_impact_score</code> <code>double</code> OWASP Risk Rating technical impact score <code>owasp_rr_business_impact_score</code> <code>double</code> OWASP Risk Rating business impact score <code>owasp_rr_vector</code> <code>string</code> OWASP Risk Rating vector <code>epss_score</code> <code>double</code> EPSS score <code>epss_percentile</code> <code>double</code> EPSS percentile"},{"location":"usage/policy-compliance/expressions/#vulnerabilityalias","title":"<code>Vulnerability.Alias</code>","text":"Field Type Description <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>GHSA-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>GITHUB</code>)"},{"location":"usage/policy-compliance/expressions/#function-definitions","title":"Function Definitions","text":"<p>In addition to the standard definitions of the CEL specification, Dependency-Track offers additional functions to unlock even more use cases:</p> Symbol Type Description <code>depends_on</code> <code>(Project, Component)</code> -&gt; <code>bool</code> Check if <code>Project</code> depends on <code>Component</code> <code>compare_age</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s age matches a given duration <code>is_dependency_of</code> <code>(Component, Component)</code> -&gt; <code>bool</code> Check if a <code>Component</code> is a dependency of another <code>Component</code> <code>matches_range</code> <code>(Project, string)</code> -&gt; <code>bool</code><code>(Component, string)</code> -&gt; <code>bool</code> Check if a <code>Project</code> or <code>Component</code> matches a vers range <code>matches_version_distance</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s version matches a given distance"},{"location":"usage/policy-compliance/overview/","title":"Overview","text":"<p>Organizations can create policies and measure policy violations across the portfolio, and against individual projects and components. Policies are configurable and can be enforced for the portfolio, or can be limited to specific projects. Policies are evaluated when an SBOM is uploaded.</p> <p>There are three types of policy violations:</p> <ul> <li>License</li> <li>Security</li> <li>Operational</li> </ul>"},{"location":"usage/policy-compliance/overview/#license-violation","title":"License Violation","text":"<p>Policy conditions can specify zero or more SPDX license IDs as well as license groups. Dependency-Track comes with pre-configured groups of related licenses (e.g. Copyleft) that provide a starting point for organizations to create custom license policies.</p>"},{"location":"usage/policy-compliance/overview/#security-violation","title":"Security Violation","text":"<p>Policy conditions can specify the severity of vulnerabilities. A vulnerability affecting a component can result in a policy violation if the policy condition matches the severity of the vulnerability. Vulnerabilities that are suppressed will not result in a policy violation.</p>"},{"location":"usage/policy-compliance/overview/#operational-violation","title":"Operational Violation","text":"<p>Policy conditions can specify zero or more:</p> <ul> <li>Coordinates (group, name, version)</li> <li>Package URL</li> <li>CPE</li> <li>SWID Tag ID</li> <li>Hash (MD5, SHA, SHA3, Blake2b, Blake3)</li> </ul> <p>This allows organizations to create lists of allowable and/or prohibited components. Future versions of Dependency-Track will incorporate additional operational parameters into the policy framework.</p>"}]}