{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hyades","text":""},{"location":"#what-is-this","title":"What is this? \ud83e\udd14","text":"<p>Hyades, named after the star cluster closest to earth,  decouples responsibilities from Dependency-Track's monolithic API server into separate,  scalable\u2122 services. We're using Kafka (or Kafka-compatible brokers like Redpanda) for communicating between API  server and Hyades services.</p> <p>If you're interested in the technical background of this project, please refer to \ud83d\udc49 <code>WTF.md</code> \ud83d\udc48.</p> <p>As of now, Hyades is capable of:</p> <ul> <li>Performing vulnerability analysis using scanners that leverage:</li> <li>Dependency-Track's internal vulnerability database</li> <li>OSS Index</li> <li>Snyk</li> <li>Gathering component metadata (e.g. latest available version) from remote repositories</li> <li>Sending notifications via all channels supported by the original API server (E-Mail, Webhook, etc.)</li> </ul> <p>Here's a rough overview of the architecture:</p> <p></p> <p>To read more about the individual services, refer to their respective <code>REAMDE.md</code>:</p> <ul> <li>Repository Metadata Analyzer</li> <li>Vulnerability Analyzer</li> </ul>"},{"location":"#great-can-i-try-it","title":"Great, can I try it? \ud83d\ude4c","text":"<p>Yes! We prepared demo setup that you can use to play around with Hyades. Check out \ud83d\udc49 <code>DEMO.md</code> \ud83d\udc48 for details!</p>"},{"location":"#technical-documentation","title":"Technical Documentation \ud83d\udcbb","text":""},{"location":"#configuration","title":"Configuration \ud83d\udcdd","text":"<p>See <code>CONFIGURATION.md</code>.</p>"},{"location":"#development","title":"Development","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>JDK 17+</li> <li>Docker</li> </ul>"},{"location":"#building","title":"Building","text":"<pre><code>mvn clean install -DskipTests\n</code></pre>"},{"location":"#running-locally","title":"Running locally","text":"<p>Running the Hyades services locally requires both a Kafka broker and a database server to be present. Containers for Redpanda and PostgreSQL can be launched using Docker Compose:</p> <pre><code>docker compose up -d\n</code></pre> <p>To launch individual services execute the <code>quarkus:dev</code> Maven goal for the respective module:</p> <pre><code>mvn -pl vulnerability-analyzer quarkus:dev\n</code></pre> <p>Make sure you've built the project at least once, otherwise the above command will fail.</p> <p>Note If you're unfamiliar with Quarkus' Dev Mode, you can read more about it  here</p>"},{"location":"#testing","title":"Testing \ud83e\udd1e","text":""},{"location":"#unit-testing","title":"Unit Testing \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"<p>To execute the unit tests for all Hyades modules:</p> <pre><code>mvn clean verify\n</code></pre>"},{"location":"#end-to-end-testing","title":"End-To-End Testing \ud83e\udddf","text":"<p>Note End-to-end tests are based on container images. The tags of those images are currently hardcoded. For the Hyades services, the tags are set to <code>latest</code>. If you want to test local changes, you'll have to first: * Build container images locally * Update the tags in <code>AbstractE2ET</code></p> <p>To execute end-to-end tests as part of the build:</p> <pre><code>mvn clean verify -Pe2e-all\n</code></pre> <p>To execute only the end-to-end tests:</p> <pre><code>mvn -pl e2e clean verify -Pe2e-all\n</code></pre>"},{"location":"#load-testing","title":"Load Testing \ud83d\ude80","text":"<p>See <code>load-tests</code>.</p>"},{"location":"#deployment","title":"Deployment \ud83d\udea2","text":"<p>The recommended way to deploy Hyades is via Helm. Our chart is not officially published to any repository yet, so for now you'll have to clone this repository to access it.</p> <p>The chart does not include:</p> <ul> <li>a database</li> <li>a Kafka-compatible broker</li> <li>the API server</li> <li>the frontend</li> </ul> <p>While API server and frontend will eventually be included, database and Kafka broker will not.</p> <p>Helm charts to deploy Kafka brokers to Kubernetes are provided by both Strimzi  and Redpanda. </p>"},{"location":"#minikube","title":"Minikube","text":"<p>Deploying to a local Minikube cluster is a great way to get started.</p> <p>Note For now, services not included in the Helm chart are deployed using Docker Compose.</p> <ol> <li>Start PostgreSQL and Redpanda via Docker Compose <pre><code>docker compose up -d\n</code></pre></li> <li>Start the API server and frontend <pre><code>docker compose up -d apiserver frontend\n</code></pre></li> <li>Start a local Minikube cluster <pre><code>minikube start\n</code></pre></li> <li>Deploy Hyades <pre><code>helm install hyades ./helm-charts/ \\\n  -n hyades --create-namespace \\\n  -f ./helm-charts/hyades/values.yaml \\\n  -f ./helm-charts/hyades/values-minikube.yaml\n</code></pre></li> </ol>"},{"location":"#monitoring","title":"Monitoring \ud83d\udcca","text":""},{"location":"#metrics","title":"Metrics","text":"<p>A basic metrics monitoring stack is provided, consisting of Prometheus and Grafana. To start both services, run:</p> <pre><code>docker compose --profile monitoring up -d\n</code></pre> <p>The services will be available locally at the following locations:</p> <ul> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000</li> </ul> <p>Prometheus is configured to scrape metrics from the following services in a 5s intervals:</p> <ul> <li>Redpanda Broker</li> <li>API Server</li> <li>Notification Publisher</li> <li>Repository Meta Analyzer</li> <li>Vulnerability Analyzer</li> </ul> <p>The Grafana instance will be automatically provisioned to use Prometheus as data source. Additionally, dashboards for the following services are automatically set up:</p> <ul> <li>Redpanda Broker</li> <li>API Server</li> <li>Vulnerability Analyzer</li> </ul>"},{"location":"#redpanda-console","title":"Redpanda Console \ud83d\udc3c","text":"<p>The provided <code>docker-compose.yml</code> includes an instance of Redpanda Console to aid with gaining insight into what's happening in the message broker. Among many other things, it can be used to inspect messages inside any given topic.</p> <p></p> <p>The console is exposed at <code>http://127.0.0.1:28080</code> and does not require authentication. It's intended for local use only.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#why","title":"Why?","text":"<p>tl;dr: Dependency-Track's architecture prevents it from scaling past a certain workload.</p> <p>Dependency-Track, for the most part, is an event-based system. As a platform for ingesting data (in the form of BOMs), listening for and itself emitting signals on it, an event-based architecture makes sense conceptually. The majority of operations Dependency-Track performs happen asynchronously, without client interaction.</p> <p>On a technical level, Dependency-Track uses an in-memory publish-subscribe architecture, implemented using Java <code>ExecutorService</code>s. An <code>ExecutorService</code> can be thought of as a pool of worker threads, consuming from an internal task queue. Tasks can be submitted to an <code>ExecutorService</code>, which will then execute them one-by-one. As multiple threads work on the queue in  parallel, the order in which tasks are being processed is not guaranteed. Thread pool sizes can vary from one,  up to unbounded numbers of threads.</p> <p>In Dependency-Track, when an event is published, subscribers to the event are looked up.  Per API contract,  event subscribers must implement an <code>inform</code> method, which takes the published event as argument. For any given event, 0-N tasks will be enqueued to the <code>ExecutorService</code>'s task queue - one for each subscriber.</p> <p></p> <p>There are three <code>ExecutorService</code> instances in Dependency-Track:</p> <ul> <li><code>EventService</code></li> <li><code>SingleThreadedEventService</code></li> <li><code>NotificationService</code></li> </ul> <p><code>EventService</code> forms the primary worker pool. Its thread pool size defaults to <code>&lt;NUM_CPU&gt; * 4</code>.  A machine with a 4-core CPU will thus have a thread pool size of <code>16</code>. The size is configurable. Common tasks handled by this worker pool include:</p> <ul> <li>Processing of uploaded BOMs and VEXs</li> <li>Performing vulnerability analysis of components, projects, or the entire portfolio</li> <li>Performing repository meta analysis of components, projects, or the entire portfolio</li> <li>Calculation of metrics for components, projects, or the entire portfolio</li> </ul> <p><code>SingleThreadedEventService</code> is a worker pool with only a single thread. The purpose of this worker pool is execute tasks that must not be run in parallel. As such, it serves as a means to serialize task execution.  Common tasks handled by this worker pool include:</p> <ul> <li>Mirroring of the NVD vulnerability database</li> <li>Updating Lucene indexes on disk</li> </ul> <p><code>NotificationService</code> is a dedicated worker pool for dispatching notifications. Its thread pool size defaults to <code>4</code> and is not configurable.</p>"},{"location":"about/#limitations","title":"Limitations","text":"<p>While this architecture works great for small to medium workloads, it presents various challenges for larger ones:</p> <ol> <li>Not horizontally scalable. As pub-sub is happening entirely in-memory, it is not possible to distribute the work to multiple application instances. The only way to handle more load using this architecture is to scale vertically, e.g.</li> <li>Increasing <code>ExecutorService</code> thread pool sizes (<code>alpine.worker.threads</code>, <code>alpine.worker.thread.multiplier</code>)</li> <li>Increasing database connection pool sizes</li> <li>Increasing resource allocations for CPU, RAM, and potentially disk / network</li> <li>No ordering guarantees of events. As multiple threads work on a shared queue of tasks in parallel, there is no way of enforcing a reliable ordering of events. </li> <li>Limited fault-tolerance. If an instance of Dependency-Track goes down, planned or unplanned, all queued tasks are  gone. Not only does this impact business-as-usual operation, but also limits the times when upgrades can be applied.</li> <li>Shared, multipurpose task queue. A single task queue is used to process all kinds of events. This means that lots of events of a certain type can \"clog\" the task queue, preventing other types of events from being processed. This is further amplified if processing of events \"clogging\" the queue relies on external services, introducing further latency. Ideally, there should be a dedicated queue per event type, so that one busy queue doesn't block others.</li> <li>Prone to race conditions. As a consequence of (2), it is possible that multiple events addressing the same thing are processed in parallel, leading to race conditions in cache lookups or database operations. Race conditions would be an even bigger problem if the work was shared across multiple application instances, and would require distributed locking as a countermeasure, which is inherently hard to get right.</li> </ol> <p>In order to scale Dependency-Track beyond its current capabilities, a distributed messaging service is required.</p>"},{"location":"about/#related-issues","title":"Related Issues","text":"<p>On multiple occasions in the past, the Dependency-Track community raised questions about high availability (HA) deployments, and / or how to better scale the platform:</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/issues/218</li> <li>https://github.com/DependencyTrack/dependency-track/issues/903</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1210</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1856</li> </ul> <p>Note The work we've done so far does not make the API server highly available. However, it does address a substantial chunk of work that is required to make that happen.</p>"},{"location":"about/#why-kafka","title":"Why Kafka?","text":"<p>Kafka was chosen because it employs various concepts  that are advantageous for Dependency-Track:</p> <ul> <li>It supports publish-subscribe use cases based on topics and partitions</li> <li>Events with the same key are guaranteed to be sent to the same partition</li> <li>Order of events is guaranteed on the partition level</li> <li>Consumers can share the load of consuming from a topic by forming consumer groups<ul> <li>Minor drawback: maximum concurrency is bound to the number of partitions</li> </ul> </li> <li>It is distributed and fault-tolerant by design, replication is built-in</li> <li>Events are stored durably on the brokers, with various options to control retention</li> <li>Log compaction allows for fault-tolerant, stateful processing,   by streaming changes of a local key-value database to Kafka</li> <li>In certain cases, this can aid in reducing load on the database server</li> <li>Mature ecosystem around it, including a vast landscape of client libraries for various languages</li> <li>Kafka Streams with its support for     stateful transformations     in particular turned out to be a unique selling point for the Kafka ecosystem</li> <li>Mature cloud offerings for fully managed instances (see Options for running Kafka)</li> </ul> <p>The concept of partitioned topics turned out to be especially useful: We can rely on the fact that events with the same key always end up in the same partition, and are processed by only one consumer (within a consumer group) at a time. In case of vulnerability scanning, by choosing the component's PURL as event key, it can be guaranteed that only the first event triggers an HTTP request to OSS Index, while later events can be handled immediately from cache. There is no race condition anymore between lookup and population of the cache.</p> <p>We also found the first-class support for stateful processing incredibly useful in some cases, e.g.:</p> <ul> <li>Scatter-gather.    As used for scanning one component with multiple analyzers. Waiting for all analyzers to complete is a stateful   operation, that otherwise would require database access.</li> <li>Batching. Some external services allow for submitting multiple component identifiers per request.   With OSS Index, up to 128 package URLs can be sent in a single request. Submitting only one package URL at a   time would drastically increase end-to-end latency. It'd also present a greater risk of getting rate limited.</li> </ul> <p>That being said, Kafka does add a considerable amount of operational complexity. Historically, Kafka has depended on Apache ZooKeeper. Operating both Kafka and ZooKeeper is not something we wanted  to force DT users to do. Luckily, the Apache Kafka project has been working on removing the ZooKeeper dependency,  and replacing it with Kafka's own raft consensus protocol (KRaft).</p> <p>There are other, more light-weight, yet Kafka API-compatible broker implementations available, too.  Redpanda being the most popular. Redpanda is distributed in a single, self-contained binary and is optimal for deployments with limited resources. Having options like Redpanda available makes building a system on Kafka much more viable.</p> <p>For this reason in fact, we primarily develop with, and test against, Redpanda.</p>"},{"location":"about/#considered-alternatives","title":"Considered alternatives","text":"<p>Before choosing for Kafka, we looked at various other messaging systems.</p>"},{"location":"about/#apache-pulsar","title":"Apache Pulsar","text":"<p>Among all options, Pulsar was the most promising besides Kafka. Pulsar prides itself in  being truly cloud native, supporting tiered storage, and multiple messaging paradigms (pub-sub and queueing).  It has native support for negative acknowledgment  of messages, and message retries.  However, the Pulsar architecture consists not  only of Pulsar brokers, but also requires Apache ZooKeeper and Apache BookKeeper  clusters. We had to dismiss Pulsar for its operational complexity.</p>"},{"location":"about/#rabbitmq","title":"RabbitMQ","text":"<p>RabbitMQ is a popular message broker. It exceeds in cases where multiple worker processes need to work on a shared queue of tasks (similar to how Dependency-Track does it today). It can achieve a high level of concurrency, as there's no limit to how many consumers can consume from a queue. This high grade of concurrency comes with the cost of lost ordering, and high potential for race conditions. </p> <p>RabbitMQ supports Kafka-like partitioned streams via its Streams plugin. In the end, we decided against RabbitMQ, because brokers do not support key-based compaction, and its consumer libraries  in turn lack adequate support for fault-tolerant stateful operations.</p>"},{"location":"about/#liftbridge","title":"Liftbridge","text":"<p>Liftbridge is built on top of NATS and provides Kafka-like features. It is however not compatible with the Kafka API, as it uses a custom envelope protocol, and is heavily focused on Go. There are no managed service offerings for Liftbridge, leaving self-hosting as only option to run it.</p>"},{"location":"about/#options-for-running-kafka","title":"Options for running Kafka","text":"<p>When it comes to running Kafka in production, users will have the choice between various self-hosted and fully managed  Infrastructure as a Service (IaaS) solutions. The following table lists a few, but there will be more we don't know of:</p> Solution Type URL Apache Kafka Self-Hosted https://kafka.apache.org/quickstart Redpanda Self-Hosted / IaaS https://redpanda.com/ Strimzi Self-Hosted https://strimzi.io/ Aiven IaaS https://aiven.io/kafka AWS MSK IaaS https://aws.amazon.com/msk/ Azure Event Hubs IaaS https://azure.microsoft.com/en-us/products/event-hubs/ Confluent Cloud IaaS https://www.confluent.io/ Red Hat OpenShift Streams IaaS https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview <p>The wide range of mature IaaS offerings is a very important benefit of Kafka over other messaging systems we evaluated.</p>"},{"location":"about/#why-java","title":"Why Java?","text":"<p>We went with Java for now because it was the path of the least resistance for us. There is no intention to exclusively  use Java though. We are considering to use Go, and generally  are open to any technology that makes sense.</p>"},{"location":"about/#why-not-microservices","title":"Why not microservices?","text":"<p>The proposed architecture is based on the rough idea of domain services for now. This keeps the number of independent services manageable, while still allowing us to distribute the overall system load. If absolutely necessary, it is  possible to break this up even further. For example, instead of having one vulnerability-analyzer service,  the scanners for each vulnerability source (e.g. OSS Index, Snyk) could be separated out into dedicated microservices.</p>"},{"location":"architecture/design/workflow-state-tracking/","title":"Tracking of Workflow State for BOM Processing and Analysis","text":"<p>Note This document was extracted from #664.</p> <p>For CI/CD use-cases, Dependency-Track offers a mechanism that allows clients to poll whether the BOM they just uploaded is still being processed. \"Processing\" in this context refers to:</p> <ul> <li>Consumption (Parsing)</li> <li>Ingestion (Sync parsed data with database)</li> <li>Vulnerability analysis</li> <li>Policy Evaluation</li> </ul> <p>This is an important capability, allowing for implementation of quality gates and reporting in CI/CD pipelines.</p> <p>The mechanism works by assigning identifiers (UUIDs) to events in Alpine's in-memory event system. As long as an event associated to a given identifier can be found in its internal queue, the identifier is considered to be \"processing\":</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/blob/6153d286d1ac806462bc76cfe17d84a57c224671/src/main/java/org/dependencytrack/resources/v1/BomResource.java#L323-L342</li> <li>https://github.com/stevespringett/Alpine/blob/cd6aa7ed05376935ab32bc43819eba0e3a525b7f/alpine-infra/src/main/java/alpine/event/framework/BaseEventService.java#L158-L161</li> </ul> <p>Events can be chained, such that a <code>BomUploadEvent</code> will trigger a <code>VulnerabilityAnalysisEvent</code>, which will trigger a <code>PolicyEvaluationEvent</code>, and so on. The event identifier is inherited by chained events.</p> <p>As everything happens in-memory, this mechanism does not work when multiple instances of the API server are used in an active-active deployment. For the functionality to continue to work, the state of processing steps must be persistent to external storage.</p> <p>Decoupled from #633.</p>"},{"location":"architecture/design/workflow-state-tracking/#design","title":"Design","text":"<p>Note The goal for now is not to build a multi-purpose workflow engine, but to track state of one specific workflow. In a future iteration, we may invest more time into coming up with a generic workflow engine concept.</p>"},{"location":"architecture/design/workflow-state-tracking/#what-to-track","title":"What to track","text":"<ol> <li>BOM consumption (parsing, de-duplication, etc.)</li> <li>BOM processing (ingestion into database)</li> <li>Vulnerability analysis</li> <li>Repository metadata analysis</li> <li>Policy evaluation</li> <li>Metrics update</li> </ol> <p>The order of execution as of today is as follows:</p> <pre><code>flowchart LR\n    A(BOM_CONSUMPTION) --&gt; B(BOM_PROCESSING)\n    B --&gt; C(VULN_ANALYSIS)\n    B --&gt; D(REPO_META_ANALYSIS)\n    C --&gt; E(POLICY_EVALUATION)\n    E --&gt; F(METRICS_UPDATE)\n    B -. BOM contains no &lt;br/&gt;components to analyze .-&gt; F\n</code></pre> <p>Note Completion of repository metadata analysis can currently not be tracked. We'll need something similar to what we introduced in https://github.com/DependencyTrack/hyades-apiserver/pull/40 for vulnerability analysis completion tracking. For the initial implementation, it may be OK to not track it.</p>"},{"location":"architecture/design/workflow-state-tracking/#states","title":"States","text":"<p>There are multiple states a processing step can be in:</p> <ul> <li><code>PENDING</code>: Initial state</li> <li><code>COMPLETED</code>: Completion detected; No failures</li> <li><code>FAILED</code>: Completion detected; Failures</li> <li><code>CANCELLED</code>: Never started because a precondition failed<ul> <li>e.g. vulnerability analysis will be cancelled when BOM ingestion failed</li> </ul> </li> <li><code>NOT_APPLICABLE</code>: The step is not applicable to the subject of the analysis<ul> <li>e.g. the analysis was triggered for an existing project, without BOM upload</li> </ul> </li> <li><code>TIMED_OUT</code>: The deadline for the step completion was exceeded; Step is unlikely to complete</li> </ul> <pre><code>stateDiagram-v2\n    [*] --&gt; PENDING\n    PENDING --&gt; COMPLETED\n    COMPLETED --&gt; [*]\n    PENDING --&gt; FAILED\n    FAILED --&gt; [*]\n    PENDING --&gt; CANCELLED\n    CANCELLED --&gt; [*]\n    [*] --&gt; NOT_APPLICABLE\n    NOT_APPLICABLE --&gt; [*]\n    PENDING --&gt; TIMED_OUT\n    TIMED_OUT --&gt; COMPLETED\n    TIMED_OUT --&gt; FAILED\n</code></pre> <p>Each step is responsible for updating its own state.</p> <p>The overall state can be considered to be complete, if there's no step in <code>PENDING</code> state.</p> <p>When a step failure is detected, a \"failure reason\" message must be persisted. If multiple steps fail, (rough) failure details for each step must be available.</p> <p>There should be a deadline mechanism, which automatically transitions steps from <code>PENDING</code> into <code>TIMED_OUT</code> state. Steps in <code>TIMED_OUT</code> state communicate that it is unlikely that a terminal state can be reached (<code>COMPLETED</code>, <code>FAILED</code>). However, it is still possible (e.g. due to significant consumer lag, events may arrive late).</p>"},{"location":"architecture/design/workflow-state-tracking/#workflow","title":"Workflow","text":"<p>The API server will act as workflow orchestrator, kicking off new steps as needed. This implies that completion of a step must be registered by the API server.</p> <p>To illustrate how the initial stage of the workflow should look like:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ BomResource: Upload BOM\n    BomResource -&gt;&gt; BomResource: Validate BOM\n    BomResource -&gt;&gt; BomResource: Generate Correlation ID (UUID)\n    BomResource -&gt;&gt; Database: Create workflow steps\n    Note over BomResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=BOM_CONSUMPTION&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=BOM_PROCESSING&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Status=PENDING\n    BomResource -&gt;&gt; BomUploadProcessingTask: Dispatch BomUploadEvent\n    BomResource -&gt;&gt;- Client: Correlation ID\n    par\n        loop Continuously\n            Client -&gt;&gt; BomResource: Poll Status\n            Note over Client, BomResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n        end\n    and\n        BomUploadProcessingTask -&gt;&gt; Database: Update step start time\n        activate BomUploadProcessingTask\n        Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;StartedAt=&lt;NOW&gt;\n        BomUploadProcessingTask -&gt;&gt; BomUploadProcessingTask: Consume BOM&lt;br/&gt;(Parse, De-dupe)\n        alt Consumption succeeded\n            BomUploadProcessingTask -&gt;&gt; Database: Update step status\n            Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Process BOM&lt;br/&gt;(Insert, Update, Delete)\n            alt Processing succeeded\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.repo-meta-analysis.component\" topic\n            else Processing failed\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=VULN_ANALYSIS&lt;br/&gt;Status=CANCELLED\n            end\n        else Consumption failed\n            BomUploadProcessingTask -&gt;&gt; Database: Update step Status\n            Note over BomUploadProcessingTask, Database: Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=CANCELLED\n        end\n        deactivate BomUploadProcessingTask\n    end\n</code></pre> <p>When triggering a re-analysis of an already existing project, the workflow may be kicked off as follows:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ FindingResource: Re-Analyze project\n    FindingResource -&gt;&gt; FindingResource: Generate Correlation ID (UUID)\n    FindingResource -&gt;&gt; Database: Create workflow steps\n    Note over FindingResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=POLICY_EVALUATION&lt;br/&gt;Status=PENDING\n    FindingResource -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n    FindingResource -&gt;&gt;- Client: Correlation ID\n    loop Continuously\n        Client -&gt;&gt; FindingResource: Poll Status\n        Note over Client, FindingResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n    end\n</code></pre>"},{"location":"architecture/design/workflow-state-tracking/#proposed-table-schema","title":"Proposed Table Schema","text":"<p>Each step of the workflow will be represented in a dedicated row. This allows us to add or remove steps without altering the database schema (see original version of the schema further down below), or even add steps while the workflow is running. It also plays better with concurrent writes, as no two threads / instances will need to modify the same row.</p> Name Type Nullable Example ID <code>SERIAL</code> \u274c 1 PARENT_STEP_ID <code>SERIAL FK</code> \u2705 0 TOKEN <code>VARCHAR(36)</code> \u274c <code>484d9eaa-7ea4-4476-97d6-f36327b5a626</code> STARTED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> UPDATED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> STEP <code>VARCHAR(64)</code> \u274c <code>METRICS_UPDATE</code> STATUS <code>VARCHAR(64)</code> \u274c <code>PENDING</code> FAILURE_REASON <code>TEXT</code> \u2705 <code>Failed to acquire database connection</code> <p>Potential Future Improvements: * Do we need/want to capture the order in which steps are supposed to be executed? * Do we need/want to capture metadata of the overall workflow (who triggered it, when was it triggered, correlation id, ...)?</p> Original Version  | Name | Type | Nullable | Example | | :--- | :--- | :---: | :--- | | TOKEN | `VARCHAR(36)` | \u274c | `484d9eaa-7ea4-4476-97d6-f36327b5a626` | | STARTED_AT | `TIMESTAMP` | \u274c | `1999-01-08 04:05:06` | | UPDATED_AT | `TIMESTAMP` | \u274c | `1999-01-08 04:05:06` | | BOM_CONSUMPTION | `VARCHAR(64)` | \u274c | `PENDING` | | BOM_PROCESSING | `VARCHAR(64)` | \u274c | `PENDING` | | VULN_ANALYSIS | `VARCHAR(64)` | \u274c | `PENDING` | | REPO_META_ANALYSIS | `VARCHAR(64)` | \u274c | `PENDING` | | POLICY_EVALUATION | `VARCHAR(64)` | \u274c | `PENDING` | | METRICS_UPDATE | `VARCHAR(64)` | \u274c | `PENDING` | | FAILURE_REASON | `TEXT` | \u2705 | - |  `FAILURE_REASON` is a field of unlimited length. It either holds no value (`NULL`), or a JSON object listing failure details per step, e.g.:  <pre><code>{\n  \"BOM_PROCESSING\": \"Failed to acquire database connection\"\n}\n</code></pre> <p>Where applicable, the \"detailed\" status of a step is tracked in a dedicated table.</p>"},{"location":"architecture/design/workflow-state-tracking/#retention","title":"Retention","text":"<p>Rows in the table should be cleaned up on a recurring basis.</p> <p>This could be as simple as scheduling a job that executes this SQL query:</p> <pre><code>DELETE FROM org.dependencytrack.repometaanalyzer.model.WorkflowStep WHERE this.updatedAt &lt; :threshold\n</code></pre> <p>A retention time of 1-3 days since the last update should be reasonable.</p>"},{"location":"architecture/design/workflow-state-tracking/#rest-api-endpoints","title":"REST API endpoints","text":"<p>The existing endpoint to check whether a BOM is still being processed should continue to work as expected:</p> <p><pre><code>GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;\n</code></pre> <pre><code>{\n  \"processing\": true\n}\n</code></pre></p> <p>An additional endpoint may be added, which allows for retrieval of the individual step states:</p> <p><pre><code>GET /api/v1/workflow/token/&lt;CORRELATION_ID&gt;/status\n</code></pre> <pre><code>[\n  {\n    \"step\": \"BOM_CONSUMPTION\",\n    \"status\": \"COMPLETED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\"\n  },\n  {\n    \"step\": \"BOM_PROCESSING\",\n    \"status\": \"FAILED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\",\n    \"failureReason\": \"Failed to acquire database connection\"\n  },\n  {\n    \"step\": \"VULN_ANALYSIS\",\n    \"status\": \"CANCELLED\"\n  }\n]\n</code></pre></p> <p>If all a client cares about are vulnerability analysis results, they could stop polling immediately after <code>vulnerabilityAnalysis</code> transitions into the <code>COMPLETED</code> state.</p>"},{"location":"development/building/","title":"Building","text":"<p>TBD</p>"},{"location":"development/building/#containers","title":"Containers","text":"<p>TBD</p>"},{"location":"development/building/#native-executables","title":"Native Executables","text":"<p>TBD</p>"},{"location":"development/database-migrations/","title":"Database Migrations","text":""},{"location":"development/database-migrations/#introduction","title":"Introduction","text":"<p>In contrast to Dependency-Track v4 and earlier, Dependency-Track v5 manages database migrations with Liquibase. The database schema is still owned by the API server though. It will execute migrations upon startup, unless explicitly disabled via <code>database.run.migrations</code>.</p> <p>Liquibase operates with the concept of changelogs. For the sake of better visibility, Dependency-Track uses separate changelogs for each release version. Individual changelogs are referenced by <code>changelog-main.xml</code>.</p> <p>Stored procedures and custom SQL functions are treated differently: They are re-created whenever their content changes. Their sources are located in the <code>procedures</code> directory.</p>"},{"location":"development/database-migrations/#adding-migrations","title":"Adding Migrations","text":"<ol> <li>If it doesn't exist already, create a <code>changelog-vX.Y.Z.xml</code> file<ul> <li><code>X</code>, <code>Y</code>, and <code>Z</code> must correspond to the current release version</li> </ul> </li> <li>Ensure the <code>changelog-vX.Y.Z.xml</code> file is referenced via <code>include</code> in <code>changelog-main.xml</code></li> <li>Add your changeset to <code>changelog-vX.Y.Z.xml</code></li> </ol> <p>When adding a new <code>changeset</code>, consider the following guidelines:</p> <ul> <li>The changeset ID must follow the <code>vX.Y.Z-&lt;NUM&gt;</code> format, where:<ul> <li><code>X</code>, <code>Y</code>, and <code>Z</code> match the changelog's version</li> <li><code>NUM</code> is an incrementing number, starting at <code>1</code> for the first <code>changeset</code> of the release</li> </ul> </li> <li>The <code>author</code> must correspond to your GitHub username</li> <li>Prefer built-in change types<ul> <li>Use the <code>sql</code> change type if no fitting built-in exists</li> <li>Use a custom change in edge cases, when additional computation is required</li> </ul> </li> <li>When using custom changes:<ul> <li>Use the <code>org.dependencytrack.persistence.migration.change</code> package</li> <li>Changes must not depend on domain logic</li> </ul> </li> <li>You must not modify <code>changeset</code>s that were already committed to <code>main</code></li> </ul>"},{"location":"development/database-migrations/#making-schema-changes-available-to-hyades-services","title":"Making Schema Changes Available to Hyades Services","text":"<p>Because the schema is owned by the API server, and the API server is also responsible for executing migrations, other services that access the database must replicate the current schema, in order to run tests against it.</p> <p>Currently, this is achieved by:</p> <ol> <li>Having Liquibase generate the schema SQL based on the changelog</li> <li>Adding the <code>schema.sql</code> file as resource to the <code>commons-persistence</code> module</li> <li>Having all services that require database access depend on <code>commons-persistence</code></li> <li>Configuring Quarkus Dev Services to initialize new database containers with <code>schema.sql</code><ul> <li>Using <code>quarkus.datasource.devservices.init-script-path</code></li> </ul> </li> </ol> <p>The schema can be generated using the <code>dbschema-generate.sh</code> script in the <code>hyades-apiserver</code> repository:</p> <pre><code>./dev/scripts/dbschema-generate.sh\n</code></pre> <p>Note</p> <ul> <li>You may need to build the API server project once before running the script</li> <li>Because Liquibase requires database to run against, the script will launch a temporary PostgreSQL container</li> </ul> <p>The output is written to <code>target/liquibase/migrate.sql</code>.</p>"},{"location":"development/documentation/","title":"Documentation","text":""},{"location":"development/documentation/#introduction","title":"Introduction","text":"<p>User-facing documentation is implemented with MkDocs and Material for MkDocs. The sources are located in <code>docs</code>. Changes to the documentation are automatically deployed to GitHub pages, using the <code>deploy-docs.yml</code> GitHub Actions workflow. Once deployed, the documentation is available at https://dependencytrack.github.io/hyades/snapshot.</p>"},{"location":"development/documentation/#versioning","title":"Versioning","text":"<p>Documentation is published for each version of the project, including unstable <code>SNAPSHOT</code> versions. This allows users to browse the docs most relevant to their Dependency-Track deployment.</p> Version selection on the documentation site <p>Documentation for unstable versions is aliased as <code>snapshot</code>, whereas for stable builds it is aliased as <code>latest</code>. They are accessible via <code>/snapshot</code> and <code>/latest</code> respectively.</p> <p>Tip</p> <p>When sharing links to the docs with others, prefer using specific versions instead of <code>latest</code> or <code>snapshot</code>. For example https://dependencytrack.github.io/hyades/0.4.0. This ensures that your links will not break as documentation evolves.</p> <p>The versioning logic is handled by mike as part of the <code>deploy-docs.yml</code> workflow.</p>"},{"location":"development/documentation/#local-development","title":"Local Development","text":"<p>For local building and rendering of the docs, use the  <code>docs-dev.sh</code> script:</p> <pre><code>./scripts/docs-dev.sh\n</code></pre> <p>It will launch a development server that listens on http://localhost:8000 and reloads whenever changes are made to the documentation sources. The script requires the <code>docker</code> command to be available.</p>"},{"location":"development/documentation/#configuration-documentation","title":"Configuration Documentation","text":"<p>To make it easier for users to discover available configuration options (i.e. environment variables), we generate human-readable documentation for it. You can see the result of this here.</p>"},{"location":"development/documentation/#applicationproperties-annotations","title":"<code>application.properties</code> Annotations","text":"<p>We leverage comments on property definitions to gather metadata. Other than a property's description, the following annotations are supported to provide further information:</p> Annotation Description <code>@category</code> Allows for categorization / grouping of related properties <code>@default</code> To be used for cases where the default value is implicit, for example when it is inherited from the framework or other properties <code>@example</code> To give an idea of what a valid value may look like, when it's not possible to provide a sensible default value <code>@hidden</code> Marks a property as to-be-excluded from the generated docs <code>@required</code> Marks a property as required <code>@type</code> Defines the type of the property <p>For example, a properly annotated property might look like this:</p> <pre><code># Defines the path to the secret key to be used for data encryption and decryption.\n# The key will be generated upon first startup if it does not exist.\n#\n# @category: General\n# @default:  ${alpine.data.directory}/keys/secret.key\n# @type:     string\nalpine.secret.key.path=\n</code></pre> <p>It is also possible to index properties that are commented out, for example:</p> <pre><code># Foo bar baz.\n#\n# @category: General\n# @example:  Some example value\n# @type:     string\n# foo.bar.baz=\n</code></pre> <p>This can be useful when it's not possible to provide a sensible default, and providing the property without a value would break something. Generally though, you should always prefer setting a sensible default.</p> <p>If a property depends on another property, or relates to it, mention it in the description. A deep-link will automatically be generated for it. For example:</p> Example of a generated deep-link for <code>alpine.cors.enabled</code>"},{"location":"development/documentation/#generation","title":"Generation","text":"<p>Configuration documentation is generated from <code>application.properties</code> files. We use the <code>GenerateConfigDocs</code> JBang script for this:</p> <pre><code>Usage: GenerateConfigDocs [--include-hidden] [-o=OUTPUT_PATH] -t=TEMPLATE_FILE\n                          PROPERTIES_FILE\n      PROPERTIES_FILE        The properties file to generate documentation for\n      --include-hidden       Include hidden properties in the output\n  -o, --output=OUTPUT_PATH   Path to write the output to, will write to STDOUT\n                               if not provided\n  -t, --template=TEMPLATE_FILE\n                             The Pebble template file to use for generation\n</code></pre> <p>Tip</p> <p>Usually you do not need to run the script yourself. We have a GitHub Actions workflow (<code>update-config-docs.yml</code>)  that does that automatically whenever a modification to <code>application.properties</code> files is pushed to the <code>main</code> branch. It also works across repositories, i.e. it will be triggered for changes in the <code>hyades-apiserver</code> repository as well.</p> <p>To generate documentation for the API server, you would run:</p> <pre><code>jbang scripts/GenerateConfigDocs.java \\\n    -t ./scripts/config-docs.md.peb \\\n    -o ./docs/reference/configuration/api-server.md \\\n    ../hyades-apiserver/src/main/resources/application.properties\n</code></pre> <p>Output is generated based on a customizable Pebble template  (currently <code>config-docs.md.peb</code>).</p>"},{"location":"development/overview/","title":"Overview","text":"<p>Want to hack on Hyades, the upcoming Dependency-Track v5? Awesome, here's what you need to know to get started!</p> <p>Important</p> <p>Please be sure to read <code>CONTRIBUTING.md</code> and <code>CODE_OF_CONDUCT.md</code> as well.</p>"},{"location":"development/overview/#repositories","title":"Repositories","text":"<p>The project consists of the following repositories:</p> Repository Description DependencyTrack/hyades Main repository. Includes Hyades services, end-to-end tests, documentation, and deployment manifests. GitHub issues and discussions are managed here. DependencyTrack/hyades-apiserver Fork of <code>DependencyTrack/dependency-track</code>.  GitHub issues and discussions are disabled. DependencyTrack/hyades-frontend Fork of <code>DependencyTrack/frontend</code>.  GitHub issues and discussions are disabled. <p>Note</p> <p>The <code>hyades</code> and <code>hyades-apiserver</code> repositories are split for historical reasons. We are planning to merge them, which should result in less overhead and more opportunities for code sharing.</p> <p>To clone them all:</p> <pre><code>git clone https://github.com/DependencyTrack/hyades.git\ngit clone https://github.com/DependencyTrack/hyades-apiserver.git\ngit clone https://github.com/DependencyTrack/hyades-frontend.git\n</code></pre>"},{"location":"development/overview/#prerequisites","title":"Prerequisites","text":"<p>There are a few things you'll need on your journey:</p> <ul> <li>Java Development Kit<sup>1</sup> &gt;=21 (Temurin distribution recommended)</li> <li>Maven<sup>1</sup> &gt;=3.9 (comes bundled with IntelliJ and Eclipse)</li> <li>NodeJS<sup>2</sup> &gt;=20</li> <li>A Java and JavaScript capable editor or IDE of your preference (we recommend IntelliJ<sup>3</sup>)</li> <li>Docker or Podman</li> <li>Docker Compose or Podman Compose</li> </ul> Tip <p><sup>1</sup> We recommend sdkman to install Java and Maven. When working in a corporate environment, you should obviously prefer the packages provided by your organization.</p> <p><sup>2</sup> If you need to juggle multiple NodeJS versions on your system, consider using nvm to make this more bearable.</p> <p><sup>3</sup> We provide common run configurations for IntelliJ in the <code>.idea/runConfigurations</code> directories of each repository for convenience. IntelliJ will automatically pick those up when you open this repository.</p>"},{"location":"development/overview/#core-technologies","title":"Core Technologies","text":"<p>Knowing about the core technologies may help you with understanding the code base.</p>"},{"location":"development/overview/#infrastructure","title":"Infrastructure","text":"Technology Purpose PostgreSQL Database Apache Kafka Messaging / Streaming"},{"location":"development/overview/#api-server","title":"API Server","text":"Technology Purpose JAX-RS REST API specification Jersey JAX-RS implementation Java Data Objects (JDO) Persistence specification DataNucleus JDO implementation JDBI Lightweight database operations Liquibase Database migrations Confluent Parallel Consumer Kafka message processing Jetty Servlet Container Alpine Framework / Scaffolding Maven Package manager / Build tool Java Programming language"},{"location":"development/overview/#hyades-services","title":"Hyades Services","text":"Technology Purpose Kafka Streams Stream processing Quarkus Framework / Scaffolding Maven Package manager / Build tool Java Programming language"},{"location":"development/overview/#frontend","title":"Frontend","text":"Technology Purpose webpack Asset bundler Vue.js Framework NPM Package manager / Build tool JavaScript Programming language"},{"location":"development/testing/","title":"Testing","text":""},{"location":"development/testing/#introduction","title":"Introduction","text":"<p>We generally aim for a test coverage of ~80%. This is also true for new code introduced through pull requests. We value integration tests more than unit tests, and try to avoid using mocks as much as possible. If reaching the 80% test coverage requires us to write tests that don't really test anything meaningful, or require loads of mocking, we rather take lower coverage than writing those tests.</p> <p>We use Testcontainers, Wiremock, and GreenMail to test how the system interacts with the outside world.</p>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":""},{"location":"development/testing/#hyades","title":"<code>hyades</code>","text":"<p>TBD</p>"},{"location":"development/testing/#hyades-apiserver","title":"<code>hyades-apiserver</code>","text":"<p>Warning</p> <p>To reduce execution time of the test suite in CI, the PostgreSQL Testcontainer is reused. While tables are truncated after each test, sequences (e.g. for <code>ID</code> columns) won't be reset. As a consequence, you should not assert on IDs of database records.</p>"},{"location":"development/testing/#hyades-frontend","title":"<code>hyades-frontend</code>","text":"<p>There are currently no unit tests for the frontend.</p>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":""},{"location":"development/testing/#hyades_1","title":"<code>hyades</code>","text":"<p>Integration tests in the <code>hyades</code> repository are implemented as @QuarkusIntegrationTest.  As such, they are executed against an actual build artifact (JAR, container, or native executable).</p> <p>Class names of integration tests are suffixed with <code>IT</code> instead of <code>Test</code>.</p>"},{"location":"development/testing/#execution","title":"Execution","text":"<p>Integration tests can be launched individually through your IDE, or all at once using Maven:</p> <pre><code>mvn -pl '!e2e' clean verify failsafe:integration-test -DskipITs=false\n</code></pre> <p>To limit the test run to specific modules, use <code>-pl &lt;module&gt;</code>, for example:</p> <pre><code>mvn -pl vulnerability-analyzer clean verify failsafe:integration-test -DskipITs=false\n</code></pre>"},{"location":"development/testing/#execution-in-ci","title":"Execution in CI","text":"<p>In CI, integration tests are executed:</p> <ul> <li>Against all JARs, as part of the <code>CI / Test</code> workflow</li> <li>Against native executables, as part of the <code>CI / Test Native Image</code> workflow(s)</li> </ul> <p>Both workflows run for pushes and pull requests to the <code>main</code> branch.</p>"},{"location":"development/testing/#hyades-apiserver_1","title":"<code>hyades-apiserver</code>","text":"<p>The API server repository does not currently differentiate between unit- and integration-tests.</p>"},{"location":"development/testing/#hyades-frontend_1","title":"<code>hyades-frontend</code>","text":"<p>There are currently no integration tests for the frontend.</p>"},{"location":"development/testing/#end-to-end-tests","title":"End-to-End Tests","text":"<p>End-to-End tests spin up containers for all services of the system. The test environment is torn down and rebuilt for every test case. Containers are started and managed using Testcontainers.</p> <p>The tests are located in the <code>e2e</code> module of the <code>hyades</code> repository. Container images used are defined in the <code>AbstractE2ET</code> class.</p> <p>Image versions can be overwritten using the following environment variables:</p> <ul> <li><code>APISERVER_VERSION</code></li> <li><code>HYADES_VERSION</code></li> </ul>"},{"location":"development/testing/#execution_1","title":"Execution","text":"<p>Tests can be launched individually through your IDE, or all at once using Maven:</p> <pre><code>mvn -pl e2e clean verify -Pe2e-all\n</code></pre> <p>To test against local changes:</p> <ol> <li>Build container images for the modified services</li> <li>Update the image tags in <code>AbstractE2ET</code> accordingly</li> <li>Run e2e tests as detailed above</li> </ol>"},{"location":"development/testing/#execution-in-ci_1","title":"Execution in CI","text":"<p>In CI, end-to-end tests are executed for every push to the <code>main</code> branch, as well as every night at 12AM.</p> <p>They can additionally be run manually, via the GitHub Actions UI. Both the API server and Hyades version can be customized before execution.</p>"},{"location":"operations/database/","title":"Database","text":"<p>Dependency-Track requires a PostgreSQL, or PostgreSQL-compatible database to operate.</p> <p>The lowest supported version is 11. You are encouraged to use the newest available version.</p> <p>Depending on available resources, individual preferences, or organizational policies, you will have to choose between a managed, or self-hosted solution.</p>"},{"location":"operations/database/#managed-solutions","title":"Managed Solutions","text":"<p>The official PostgreSQL website hosts a list of well-known commercial hosting providers.</p> <p>Popular choices include:</p> <ul> <li>Amazon RDS for PostgreSQL</li> <li>Aiven for PostgreSQL</li> <li>Azure Database for PostgreSQL</li> <li>Google Cloud SQL for PostgreSQL</li> </ul> <p>We are not actively testing against cloud offerings. But as a rule of thumb, solutions offering \"vanilla\" PostgreSQL,  or extensions of it (for example Neon or Timescale), will most definitely work with Dependency-Track.</p> <p>The same is not necessarily true for platforms based on heavily modified PostgreSQL, or even entire re-implementations such as CockroachDB or YugabyteDB. Such solutions make certain trade-offs to achieve higher levels of scalability, which might impact functionality that Dependency-Track relies on. If you'd like to see support for those, please let us know!</p>"},{"location":"operations/database/#self-hosting","title":"Self-Hosting","text":""},{"location":"operations/database/#bare-metal-docker","title":"Bare Metal / Docker","text":"<p>For Docker deployments, use the official <code>postgres</code> image.</p> <p>Warning</p> <p>Do not use the <code>latest</code> tag! You may end up doing a major version upgrade without knowing it, ultimately breaking your database! Pin the tag to at least the major version (e.g. <code>16</code>), or better yet the minor version (e.g. <code>16.2</code>). Refer to Upgrades to upgrade instructions.</p> <p>For bare metal deloyments, it's usually best to install PostgreSQL from your distribution's package repository. See for example:</p> <ul> <li>PostgreSQL instructions for Debian</li> <li>Install and configure PostgreSQL on Ubuntu</li> <li>Using PostgreSQL with Red Hat Enterprise Linux</li> </ul> <p>To get the most out of your Dependency-Track installation, we recommend to run PostgreSQL on a separate machine than the application containers. You want PostgreSQL to be able to leverage the entire machine's resources, without being impacted by other applications.</p> <p>For smaller and non-critical deployments, it is totally fine to run everything on a single machine.</p>"},{"location":"operations/database/#basic-configuration","title":"Basic Configuration","text":"<p>You should be aware that the default PostgreSQL configuration is extremely conservative. It is intended to make PostgreSQL usable on minimal hardware, which is great for testing, but can seriously cripple performance in production environments. Not adjusting it to your specific setup will most certainly leave performance on the table.</p> <p>If you're lucky enough to have access to professional database administrators, ask them for help. They will know your organisation's best practices and can guide you in adjusting it for Dependency-Track.</p> <p>If you're not as lucky, we can wholeheartedly recommend PGTune. Given a bit of basic info about your system, it will provide a sensible baseline configuration. For the DB Type option, select <code>Online transaction processing system</code>.</p> <p></p> <p>The <code>postgresql.conf</code> is usually located at <code>/var/lib/postgresql/data/postgresql.conf</code>. Most of these settings require a restart of the application.</p> <p>In a Docker Compose setup, you can alternatively apply the desired configuration via command line flags. For example:</p> <pre><code>services:\n  postgres:\n    image: postgres:16\n    command: &gt;-\n        -c 'shared_buffers=2GB'\n        -c 'effective_cache_size=6GB'\n</code></pre>"},{"location":"operations/database/#advanced-configuration","title":"Advanced Configuration","text":"<p>For larger deployments, you may eventually run into situations where database performance degrades with just the basic configuration applied. Oftentimes, tweaking advanced settings can resolve such problems. But knowing which knobs to turn is a challenge in itself.</p> <p>If you happen to be in this situation, make sure you have database monitoring set up. Changing advanced configuration options blindly can potentially cause more damage than it helps.</p> <p>Below, you'll find a few options that, based on our observations with large-scale deployments, make sense to tweak. Note that some settings are applied system-wide, while others are only applied for certain tables.</p> <p>Note</p> <p>Got more tips to configure or tune PostgreSQL, that may be helpful to others? We'd love to include it in the docs, please do raise a PR!</p>"},{"location":"operations/database/#checkpoint_completion_target","title":"checkpoint_completion_target","text":"Default <ul> <li><code>0.5</code> (PostgreSQL &lt;= 13)</li> <li><code>0.9</code> (PostgreSQL &gt;= 14)</li> </ul> Recommendation <code>0.9</code> Tables <code>*</code> References Documentation <p>Spreads the WAL checkpoint creation across a longer period of time, resulting in a more evenly distributed I/O load. A lower value has been observed to cause undesirable spikes in I/O usage on the database server.</p> <pre><code>ALTER SYSTEM SET CHECKPOINT_COMPLETION_TARGET = 0.9;\n</code></pre>"},{"location":"operations/database/#autovacuum_vacuum_scale_factor","title":"autovacuum_vacuum_scale_factor","text":"Default <code>0.2</code> Recommendation <code>0.02</code> Tables <ul> <li><code>COMPONENT</code></li> <li><code>DEPENDENCYMETRICS</code></li> </ul> References Documentation <p>The default causes Autovacuum to start way too late on large tables with lots of churn, yielding long execution times. Reduction in scale factor causes autovacuum to happen more often, making each execution less time-intensive.</p> <p>The <code>COMPONENT</code> and <code>DEPENDENCYMETRICS</code> table are very frequently inserted into, updated, and deleted from. This causes lots of dead tuples that PostgreSQL needs to clean up. Because autovacuum also performs <code>ANALYZE</code>, slow vacuuming can cause the query planner to choose inefficient execution plans.</p> <pre><code>ALTER TABLE \"COMPONENT\" SET (AUTOVACUUM_VACUUM_SCALE_FACTOR = 0.02);\nALTER TABLE \"DEPENDENCYMETRICS\" SET (AUTOVACUUM_VACUUM_SCALE_FACTOR = 0.02);\n</code></pre>"},{"location":"operations/database/#upgrades","title":"Upgrades","text":"<p>Follow the official upgrading guide. Be sure to select the version of the documentation that corresponds to the PostgreSQL version you are running.</p> <p>Warning</p> <p>Pay attention to the fact that major version upgrades usually require a backup-and-restore cycle, due to potentially breaking changes in the underlying data storage format. Minor version upgrades are usually safe to perform in a rolling manor.</p>"},{"location":"operations/database/#kubernetes","title":"Kubernetes","text":"<p>We generally advise against running PostgreSQL on Kubernetes, unless you really know what you're doing. Wielding heavy machinery such as Postgres Operator is not something you should do lightheartedly.</p> <p>If you know what you're doing, you definitely don't need advice from us. Smooth sailing! \u2693\ufe0f</p>"},{"location":"operations/database/#schema-migrations","title":"Schema Migrations","text":"<p>Schema migrations are performed automatically by the API server upon startup. It leverages Liquibase for doing so. There is usually no manual action required when upgrading from an older Dependency-Track version, unless explicitly stated otherwise in the release notes.</p> <p>This behavior can be turned off by setting <code>database.run.migrations</code>  on the API server container to <code>false</code>.</p> <p>It is possible to use different credentials for migrations than for the application itself. This can be achieved with the following options:</p> <ul> <li><code>database.migration.url</code></li> <li><code>database.migration.username</code></li> <li><code>database.migration.password</code></li> </ul> <p>The above with default to the main database credentials if not provided explicitly.</p>"},{"location":"reference/topics/","title":"Topics","text":"Name Partitions Config <code>dtrack.notification.analyzer</code> 3 <code>dtrack.notification.bom</code> 3 <code>dtrack.notification.configuration</code> 3 <code>dtrack.notification.datasource-mirroring</code> 3 <code>dtrack.notification.file-system</code> 3 <code>dtrack.notification.integration</code> 3 <code>dtrack.notification.new-vulnerability</code> 3 <code>dtrack.notification.new-vulnerable-dependency</code> 3 <code>dtrack.notification.policy-violation</code> 3 <code>dtrack.notification.project-audit-change</code> 3 <code>dtrack.notification.project-created</code> 3 <code>dtrack.notification.repository</code> 3 <code>dtrack.notification.vex</code> 3 <code>dtrack.notification.user</code> 3 <code>dtrack.notification.project-vuln-analysis-complete</code> <sup>3</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>dtrack.repo-meta-analysis.component</code><sup>1A</sup> 3 <code>dtrack.repo-meta-analysis.result</code> 3 <code>dtrack.vuln-analysis.component</code><sup>1B</sup> 3 <code>dtrack.vuln-analysis.result</code> 3 <code>dtrack.vuln-analysis.result.processed</code> 3 <code>dtrack.vuln-analysis.scanner.result</code><sup>1B</sup> 3 <code>dtrack.vulnerability</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.epss</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.digest</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.mirror.command</code><sup>2</sup> 1 <code>dtrack.vulnerability.mirror.state</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>hyades-repository-meta-analyzer-command-by-purl-coordinates-repartition</code><sup>1A</sup> 3 <code>hyades-vulnerability-analyzer-completed-scans-table-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-last-update-store-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-table-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-ossindex-batch-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-ossindex-retry-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-scan-task-internal-repartition</code> 3 <code>hyades-vulnerability-analyzer-scan-task-ossindex-repartition</code><sup>1C</sup> 3 <code>hyades-vulnerability-analyzer-scan-task-snyk-repartition</code><sup>1D</sup> 3 <code>hyades-vulnerability-analyzer-snyk-batch-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-snyk-retry-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <p><sup>1X</sup> The topic is subject to co-partitioning requirements <sup>2</sup> The partition number of this topic should not be changed <sup>3</sup> To use this notification, the PROJECT_VULN_ANALYSIS_COMPLETE group needs to be manually added through api call to the notify on condition The put request to be executed is here</p>"},{"location":"reference/topics/#co-partitioning-requirements","title":"Co-Partitioning Requirements","text":"<p>Some topics must be co-partitioned, meaning they must share the exact same number of partitions. Applications using those topics will not work correctly when this is not the case.</p>"},{"location":"reference/configuration/api-server/","title":"API Server","text":""},{"location":"reference/configuration/api-server/#cors","title":"CORS","text":""},{"location":"reference/configuration/api-server/#alpinecorsallowcredentials","title":"alpine.cors.allow.credentials","text":"<p>Controls the content of the <code>Access-Control-Allow-Credentials</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_CORS_ALLOW_CREDENTIALS</code>"},{"location":"reference/configuration/api-server/#alpinecorsallowheaders","title":"alpine.cors.allow.headers","text":"<p>Controls the content of the <code>Access-Control-Allow-Headers</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, *</code> ENV <code>ALPINE_CORS_ALLOW_HEADERS</code>"},{"location":"reference/configuration/api-server/#alpinecorsallowmethods","title":"alpine.cors.allow.methods","text":"<p>Controls the content of the <code>Access-Control-Allow-Methods</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>GET POST PUT DELETE OPTIONS</code> ENV <code>ALPINE_CORS_ALLOW_METHODS</code>"},{"location":"reference/configuration/api-server/#alpinecorsalloworigin","title":"alpine.cors.allow.origin","text":"<p>Controls the content of the <code>Access-Control-Allow-Origin</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>*</code> ENV <code>ALPINE_CORS_ALLOW_ORIGIN</code>"},{"location":"reference/configuration/api-server/#alpinecorsenabled","title":"alpine.cors.enabled","text":"<p>Defines whether Cross Origin Resource Sharing  (CORS) headers shall be included in REST API responses.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_CORS_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpinecorsexposeheaders","title":"alpine.cors.expose.headers","text":"<p>Controls the content of the <code>Access-Control-Expose-Headers</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count</code> ENV <code>ALPINE_CORS_EXPOSE_HEADERS</code>"},{"location":"reference/configuration/api-server/#alpinecorsmaxage","title":"alpine.cors.max.age","text":"<p>Controls the content of the <code>Access-Control-Max-Age</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>integer</code> Default <code>3600</code> ENV <code>ALPINE_CORS_MAX_AGE</code>"},{"location":"reference/configuration/api-server/#database","title":"Database","text":""},{"location":"reference/configuration/api-server/#alpinedatabasepassword","title":"alpine.database.password","text":"<p>Specifies the password to use when authenticating to the database.  </p> Required false Type <code>string</code> Default <code>dtrack</code> ENV <code>ALPINE_DATABASE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolenabled","title":"alpine.database.pool.enabled","text":"<p>Specifies if the database connection pool is enabled.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_DATABASE_POOL_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolidletimeout","title":"alpine.database.pool.idle.timeout","text":"<p>This property controls the maximum amount of time that a connection is  allowed to sit idle in the pool.  The property can be set globally for both transactional and non-transactional  connection pools, or for each pool type separately. When both global and pool-specific  properties are set, the pool-specific properties take precedence.  </p> Required false Type <code>integer</code> Default <code>300000</code> ENV <code>ALPINE_DATABASE_POOL_IDLE_TIMEOUT</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolmaxlifetime","title":"alpine.database.pool.max.lifetime","text":"<p>This property controls the maximum lifetime of a connection in the pool.  An in-use connection will never be retired, only when it is closed will  it then be removed.  The property can be set globally for both transactional and non-transactional  connection pools, or for each pool type separately. When both global and pool-specific  properties are set, the pool-specific properties take precedence.  </p> Required false Type <code>integer</code> Default <code>600000</code> ENV <code>ALPINE_DATABASE_POOL_MAX_LIFETIME</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolmaxsize","title":"alpine.database.pool.max.size","text":"<p>This property controls the maximum size that the pool is allowed to reach,  including both idle and in-use connections.  The property can be set globally for both transactional and non-transactional  connection pools, or for each pool type separately. When both global and pool-specific  properties are set, the pool-specific properties take precedence.  </p> Required false Type <code>integer</code> Default <code>20</code> ENV <code>ALPINE_DATABASE_POOL_MAX_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolminidle","title":"alpine.database.pool.min.idle","text":"<p>This property controls the minimum number of idle connections in the pool.  This value should be equal to or less than <code>alpine.database.pool.max.size</code>.  Warning: If the value is less than <code>alpine.database.pool.max.size</code>,  <code>alpine.database.pool.idle.timeout</code> will have no effect.  The property can be set globally for both transactional and non-transactional  connection pools, or for each pool type separately. When both global and pool-specific  properties are set, the pool-specific properties take precedence.  </p> Required false Type <code>integer</code> Default <code>10</code> ENV <code>ALPINE_DATABASE_POOL_MIN_IDLE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolnontxidletimeout","title":"alpine.database.pool.nontx.idle.timeout","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.idle.timeout}</code> ENV <code>ALPINE_DATABASE_POOL_NONTX_IDLE_TIMEOUT</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolnontxmaxlifetime","title":"alpine.database.pool.nontx.max.lifetime","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.max.lifetime}</code> ENV <code>ALPINE_DATABASE_POOL_NONTX_MAX_LIFETIME</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolnontxmaxsize","title":"alpine.database.pool.nontx.max.size","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.max.size}</code> ENV <code>ALPINE_DATABASE_POOL_NONTX_MAX_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolnontxminidle","title":"alpine.database.pool.nontx.min.idle","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.min.idle}</code> ENV <code>ALPINE_DATABASE_POOL_NONTX_MIN_IDLE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepooltxidletimeout","title":"alpine.database.pool.tx.idle.timeout","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.idle.timeout}</code> ENV <code>ALPINE_DATABASE_POOL_TX_IDLE_TIMEOUT</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepooltxmaxlifetime","title":"alpine.database.pool.tx.max.lifetime","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.max.lifetime}</code> ENV <code>ALPINE_DATABASE_POOL_TX_MAX_LIFETIME</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepooltxmaxsize","title":"alpine.database.pool.tx.max.size","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.max.size}</code> ENV <code>ALPINE_DATABASE_POOL_TX_MAX_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepooltxminidle","title":"alpine.database.pool.tx.min.idle","text":"Required false Type <code>integer</code> Default <code>${alpine.database.pool.min.idle}</code> ENV <code>ALPINE_DATABASE_POOL_TX_MIN_IDLE</code>"},{"location":"reference/configuration/api-server/#alpinedatabaseurl","title":"alpine.database.url","text":"<p>Specifies the JDBC URL to use when connecting to the database.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>jdbc:postgresql://localhost:5432/dtrack</code> ENV <code>ALPINE_DATABASE_URL</code>"},{"location":"reference/configuration/api-server/#alpinedatabaseusername","title":"alpine.database.username","text":"<p>Specifies the username to use when authenticating to the database.  </p> Required false Type <code>string</code> Default <code>dtrack</code> ENV <code>ALPINE_DATABASE_USERNAME</code>"},{"location":"reference/configuration/api-server/#databasemigrationpassword","title":"database.migration.password","text":"<p>Defines the database password for executing migrations.  If not set, the value of <code>alpine.database.password</code> will be used.  </p> Required false Type <code>string</code> Default <code>${alpine.database.password}</code> ENV <code>DATABASE_MIGRATION_PASSWORD</code>"},{"location":"reference/configuration/api-server/#databasemigrationurl","title":"database.migration.url","text":"<p>Defines the database JDBC URL to use when executing migrations.  If not set, the value of <code>alpine.database.url</code> will be used.  Should generally not be set, unless TLS authentication is used,  and custom connection variables are required.  </p> Required false Type <code>string</code> Default <code>${alpine.database.url}</code> ENV <code>DATABASE_MIGRATION_URL</code>"},{"location":"reference/configuration/api-server/#databasemigrationusername","title":"database.migration.username","text":"<p>Defines the database user for executing migrations.  If not set, the value of <code>alpine.database.username</code> will be used.  </p> Required false Type <code>string</code> Default <code>${alpine.database.username}</code> ENV <code>DATABASE_MIGRATION_USERNAME</code>"},{"location":"reference/configuration/api-server/#databaserunmigrations","title":"database.run.migrations","text":"<p>Defines whether database migrations should be executed on startup.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>DATABASE_RUN_MIGRATIONS</code>"},{"location":"reference/configuration/api-server/#development","title":"Development","text":""},{"location":"reference/configuration/api-server/#devservicesenabled","title":"dev.services.enabled","text":"<p>Whether dev services shall be enabled.    When enabled, Dependency-Track will automatically launch containers for:  <ul> <li>Frontend</li> <li>Kafka</li> <li>PostgreSQL</li> </ul>  at startup, and configures itself to use them. They are disposed when  Dependency-Track stops. The containers are exposed on randomized ports,  which will be logged during startup.    Trying to enable dev services in a production build will prevent  the application from starting.    Note that the containers launched by the API server can not currently  be discovered and re-used by other Hyades services. This is a future  enhancement tracked in https://github.com/DependencyTrack/hyades/issues/1188.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>DEV_SERVICES_ENABLED</code>"},{"location":"reference/configuration/api-server/#devservicesimagefrontend","title":"dev.services.image.frontend","text":"<p>The image to use for the frontend dev services container.  </p> Required false Type <code>string</code> Default <code>ghcr.io/dependencytrack/hyades-frontend:snapshot</code> ENV <code>DEV_SERVICES_IMAGE_FRONTEND</code>"},{"location":"reference/configuration/api-server/#devservicesimagekafka","title":"dev.services.image.kafka","text":"<p>The image to use for the Kafka dev services container.  </p> Required false Type <code>string</code> Default <code>docker.redpanda.com/vectorized/redpanda:v24.1.7</code> ENV <code>DEV_SERVICES_IMAGE_KAFKA</code>"},{"location":"reference/configuration/api-server/#devservicesimagepostgres","title":"dev.services.image.postgres","text":"<p>The image to use for the PostgreSQL dev services container.  </p> Required false Type <code>string</code> Default <code>postgres:16</code> ENV <code>DEV_SERVICES_IMAGE_POSTGRES</code>"},{"location":"reference/configuration/api-server/#general","title":"General","text":""},{"location":"reference/configuration/api-server/#alpineapikeyprefix","title":"alpine.api.key.prefix","text":"<p>Defines the prefix to be used for API keys. A maximum prefix length of 251  characters is supported. The prefix may also be left empty.  </p> Required false Type <code>string</code> Default <code>odt_</code> ENV <code>ALPINE_API_KEY_PREFIX</code>"},{"location":"reference/configuration/api-server/#alpinebcryptrounds","title":"alpine.bcrypt.rounds","text":"<p>Specifies the number of bcrypt rounds to use when hashing a user's password.  The higher the number the more secure the password, at the expense of  hardware resources and additional time to generate the hash.  </p> Required true Type <code>integer</code> Default <code>14</code> ENV <code>ALPINE_BCRYPT_ROUNDS</code>"},{"location":"reference/configuration/api-server/#alpinedatadirectory","title":"alpine.data.directory","text":"<p>Defines the path to the data directory. This directory will hold logs,  keys, and any database or index files along with application-specific  files or directories.  </p> Required true Type <code>string</code> Default <code>~/.dependency-track</code> ENV <code>ALPINE_DATA_DIRECTORY</code>"},{"location":"reference/configuration/api-server/#alpineprivatekeypath","title":"alpine.private.key.path","text":"<p>Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.  The keys will be generated upon first startup if they do not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/private.key</code> Example <code>/var/run/secrets/private.key</code> ENV <code>ALPINE_PRIVATE_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#alpinepublickeypath","title":"alpine.public.key.path","text":"<p>Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.  The keys will be generated upon first startup if they do not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/public.key</code> Example <code>/var/run/secrets/public.key</code> ENV <code>ALPINE_PUBLIC_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#alpinesecretkeypath","title":"alpine.secret.key.path","text":"<p>Defines the path to the secret key to be used for data encryption and decryption.  The key will be generated upon first startup if it does not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/secret.key</code> ENV <code>ALPINE_SECRET_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#bomuploadprocessingtrxflushthreshold","title":"bom.upload.processing.trx.flush.threshold","text":"<p>Defines the number of write operations to perform during BOM processing before changes are flushed to the database.  Smaller values may lower memory usage of the API server, whereas higher values will improve performance as fewer  network round-trips to the database are necessary.  </p> Required false Type <code>integer</code> Default <code>10000</code> ENV <code>BOM_UPLOAD_PROCESSING_TRX_FLUSH_THRESHOLD</code>"},{"location":"reference/configuration/api-server/#integritycheckenabled","title":"integrity.check.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INTEGRITY_CHECK_ENABLED</code>"},{"location":"reference/configuration/api-server/#integrityinitializerenabled","title":"integrity.initializer.enabled","text":"<p>Specifies whether the Integrity Initializer shall be enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INTEGRITY_INITIALIZER_ENABLED</code>"},{"location":"reference/configuration/api-server/#tmpdelaybomprocessednotification","title":"tmp.delay.bom.processed.notification","text":"<p>Delays the BOM_PROCESSED notification until the vulnerability analysis associated with a given BOM upload  is completed. The intention being that it is then \"safe\" to query the API for any identified vulnerabilities.  This is specifically for cases where polling the /api/v1/bom/token/ endpoint is not feasible.  THIS IS A TEMPORARY FUNCTIONALITY AND MAY BE REMOVED IN FUTURE RELEASES WITHOUT FURTHER NOTICE.   Required false Type <code>boolean</code> Default <code>false</code> ENV <code>TMP_DELAY_BOM_PROCESSED_NOTIFICATION</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicyanalysisenabled","title":"vulnerability.policy.analysis.enabled","text":"<p>Defines whether vulnerability policy analysis is enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>VULNERABILITY_POLICY_ANALYSIS_ENABLED</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleauthpassword","title":"vulnerability.policy.bundle.auth.password","text":"<p>For nginx server, if username and bearer token both are provided, basic auth will be used,  else the auth header will be added based on the not null values  Defines the password to be used for basic authentication against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_AUTH_PASSWORD</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleauthusername","title":"vulnerability.policy.bundle.auth.username","text":"<p>Defines the username to be used for basic authentication against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_AUTH_USERNAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundlebearertoken","title":"vulnerability.policy.bundle.bearer.token","text":"<p>Defines the token to be used as bearerAuth against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_BEARER_TOKEN</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundlesourcetype","title":"vulnerability.policy.bundle.source.type","text":"<p>Defines the type of source from which policy bundles are being fetched from.  Required when <code>vulnerability.policy.bundle.url</code> is set.  </p> Required false Type <code>enum</code> Valid Values <code>[nginx, s3]</code> Default <code>NGINX</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_SOURCE_TYPE</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleurl","title":"vulnerability.policy.bundle.url","text":"<p>Defines where to fetch the policy bundle from.For S3, just the base url needs to be provided with port  For nginx, the whole url with bundle name needs to be given  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>http://example.com:80/bundles/bundle.zip</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_URL</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3accesskey","title":"vulnerability.policy.s3.access.key","text":"<p>S3 related details. Access key, secret key, bucket name and bundle names are mandatory if S3 is chosen. Region is optional  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_ACCESS_KEY</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3bucketname","title":"vulnerability.policy.s3.bucket.name","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_BUCKET_NAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3bundlename","title":"vulnerability.policy.s3.bundle.name","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_BUNDLE_NAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3region","title":"vulnerability.policy.s3.region","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_REGION</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3secretkey","title":"vulnerability.policy.s3.secret.key","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_SECRET_KEY</code>"},{"location":"reference/configuration/api-server/#workflowretentionduration","title":"workflow.retention.duration","text":"<p>Defines the duration for how long workflow data is being retained, after all steps transitioned into a non-terminal  state (CANCELLED, COMPLETED, FAILED, NOT_APPLICABLE).  The duration must be specified in ISO8601 notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).  </p> Required false Type <code>duration</code> Default <code>P3D</code> ENV <code>WORKFLOW_RETENTION_DURATION</code>"},{"location":"reference/configuration/api-server/#workflowsteptimeoutduration","title":"workflow.step.timeout.duration","text":"<p>Defines the duration for how long a workflow step is allowed to remain in PENDING state  after being started. If this duration is exceeded, workflow steps will transition into the TIMED_OUT state.  If they remain in TIMED_OUT for the same duration, they will transition to the FAILED state.  The duration must be specified in ISO8601 notation (https://en.wikipedia.org/wiki/ISO_8601#Durations).  </p> Required false Type <code>duration</code> Default <code>PT1H</code> ENV <code>WORKFLOW_STEP_TIMEOUT_DURATION</code>"},{"location":"reference/configuration/api-server/#http","title":"HTTP","text":""},{"location":"reference/configuration/api-server/#alpinehttpproxyaddress","title":"alpine.http.proxy.address","text":"<p>HTTP proxy address. If set, then <code>alpine.http.proxy.port</code> must be set too.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>proxy.example.com</code> ENV <code>ALPINE_HTTP_PROXY_ADDRESS</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxypassword","title":"alpine.http.proxy.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_HTTP_PROXY_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxyport","title":"alpine.http.proxy.port","text":"Required false Type <code>integer</code> Default <code>null</code> Example <code>8888</code> ENV <code>ALPINE_HTTP_PROXY_PORT</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxyusername","title":"alpine.http.proxy.username","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_HTTP_PROXY_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutconnection","title":"alpine.http.timeout.connection","text":"<p>Defines the connection timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>30</code> ENV <code>ALPINE_HTTP_TIMEOUT_CONNECTION</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutpool","title":"alpine.http.timeout.pool","text":"<p>Defines the request timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>60</code> ENV <code>ALPINE_HTTP_TIMEOUT_POOL</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutsocket","title":"alpine.http.timeout.socket","text":"<p>Defines the socket / read timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>30</code> ENV <code>ALPINE_HTTP_TIMEOUT_SOCKET</code>"},{"location":"reference/configuration/api-server/#alpinenoproxy","title":"alpine.no.proxy","text":"Required false Type <code>string</code> Default <code>null</code> Example <code>localhost,127.0.0.1</code> ENV <code>ALPINE_NO_PROXY</code>"},{"location":"reference/configuration/api-server/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorconsumerautooffsetreset","title":"alpine.kafka.processor.epss.mirror.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorconsumergroupid","title":"alpine.kafka.processor.epss.mirror.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrormaxbatchsize","title":"alpine.kafka.processor.epss.mirror.max.batch.size","text":"Required true Type <code>integer</code> Default <code>500</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_MAX_BATCH_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrormaxconcurrency","title":"alpine.kafka.processor.epss.mirror.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorprocessingorder","title":"alpine.kafka.processor.epss.mirror.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorretryinitialdelayms","title":"alpine.kafka.processor.epss.mirror.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorretrymaxdelayms","title":"alpine.kafka.processor.epss.mirror.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorretrymultiplier","title":"alpine.kafka.processor.epss.mirror.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorepssmirrorretryrandomizationfactor","title":"alpine.kafka.processor.epss.mirror.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultconsumerautooffsetreset","title":"alpine.kafka.processor.repo.meta.analysis.result.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultconsumergroupid","title":"alpine.kafka.processor.repo.meta.analysis.result.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultmaxconcurrency","title":"alpine.kafka.processor.repo.meta.analysis.result.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultprocessingorder","title":"alpine.kafka.processor.repo.meta.analysis.result.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultretryinitialdelayms","title":"alpine.kafka.processor.repo.meta.analysis.result.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultretrymaxdelayms","title":"alpine.kafka.processor.repo.meta.analysis.result.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultretrymultiplier","title":"alpine.kafka.processor.repo.meta.analysis.result.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorrepometaanalysisresultretryrandomizationfactor","title":"alpine.kafka.processor.repo.meta.analysis.result.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorconsumerautooffsetreset","title":"alpine.kafka.processor.vuln.mirror.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorconsumergroupid","title":"alpine.kafka.processor.vuln.mirror.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrormaxconcurrency","title":"alpine.kafka.processor.vuln.mirror.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorprocessingorder","title":"alpine.kafka.processor.vuln.mirror.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>partition</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorretryinitialdelayms","title":"alpine.kafka.processor.vuln.mirror.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorretrymaxdelayms","title":"alpine.kafka.processor.vuln.mirror.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorretrymultiplier","title":"alpine.kafka.processor.vuln.mirror.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnmirrorretryrandomizationfactor","title":"alpine.kafka.processor.vuln.mirror.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_MIRROR_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultconsumerautooffsetreset","title":"alpine.kafka.processor.vuln.scan.result.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultconsumergroupid","title":"alpine.kafka.processor.vuln.scan.result.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultmaxconcurrency","title":"alpine.kafka.processor.vuln.scan.result.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedconsumerautooffsetreset","title":"alpine.kafka.processor.vuln.scan.result.processed.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedconsumerfetchminbytes","title":"alpine.kafka.processor.vuln.scan.result.processed.consumer.fetch.min.bytes","text":"Required true Type <code>integer</code> Default <code>524288</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_FETCH_MIN_BYTES</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedconsumergroupid","title":"alpine.kafka.processor.vuln.scan.result.processed.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedconsumermaxpollrecords","title":"alpine.kafka.processor.vuln.scan.result.processed.consumer.max.poll.records","text":"Required true Type <code>integer</code> Default <code>10000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_MAX_POLL_RECORDS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedmaxbatchsize","title":"alpine.kafka.processor.vuln.scan.result.processed.max.batch.size","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_MAX_BATCH_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedmaxconcurrency","title":"alpine.kafka.processor.vuln.scan.result.processed.max.concurrency","text":"Required true Type <code>integer</code> Default <code>1</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedprocessingorder","title":"alpine.kafka.processor.vuln.scan.result.processed.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>unordered</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedretryinitialdelayms","title":"alpine.kafka.processor.vuln.scan.result.processed.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedretrymaxdelayms","title":"alpine.kafka.processor.vuln.scan.result.processed.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedretrymultiplier","title":"alpine.kafka.processor.vuln.scan.result.processed.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessedretryrandomizationfactor","title":"alpine.kafka.processor.vuln.scan.result.processed.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultprocessingorder","title":"alpine.kafka.processor.vuln.scan.result.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultretryinitialdelayms","title":"alpine.kafka.processor.vuln.scan.result.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultretrymaxdelayms","title":"alpine.kafka.processor.vuln.scan.result.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultretrymultiplier","title":"alpine.kafka.processor.vuln.scan.result.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpinekafkaprocessorvulnscanresultretryrandomizationfactor","title":"alpine.kafka.processor.vuln.scan.result.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>ALPINE_KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaautooffsetreset","title":"kafka.auto.offset.reset","text":"Required false Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"Required true Type <code>string</code> Default <code>null</code> Example <code>localhost:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/api-server/#kafkakeystorepassword","title":"kafka.keystore.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_KEYSTORE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#kafkakeystorepath","title":"kafka.keystore.path","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_KEYSTORE_PATH</code>"},{"location":"reference/configuration/api-server/#kafkamtlsenabled","title":"kafka.mtls.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>KAFKA_MTLS_ENABLED</code>"},{"location":"reference/configuration/api-server/#kafkasecurityprotocol","title":"kafka.security.protocol","text":"Required false Type <code>enum</code> Valid Values <code>[PLAINTEXT, SASL_SSL_PLAINTEXT, SASL_PLAINTEXT, SSL]</code> Default <code>null</code> ENV <code>KAFKA_SECURITY_PROTOCOL</code>"},{"location":"reference/configuration/api-server/#kafkatlsenabled","title":"kafka.tls.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>KAFKA_TLS_ENABLED</code>"},{"location":"reference/configuration/api-server/#kafkatopicprefix","title":"kafka.topic.prefix","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/api-server/#kafkatruststorepassword","title":"kafka.truststore.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TRUSTSTORE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#kafkatruststorepath","title":"kafka.truststore.path","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TRUSTSTORE_PATH</code>"},{"location":"reference/configuration/api-server/#ldap","title":"LDAP","text":""},{"location":"reference/configuration/api-server/#alpineldapattributemail","title":"alpine.ldap.attribute.mail","text":"<p>Specifies the LDAP attribute used to store a users email address  </p> Required false Type <code>string</code> Default <code>mail</code> ENV <code>ALPINE_LDAP_ATTRIBUTE_MAIL</code>"},{"location":"reference/configuration/api-server/#alpineldapattributename","title":"alpine.ldap.attribute.name","text":"<p>Specifies the Attribute that identifies a users ID.    Example (Microsoft Active Directory):  <ul><li><code>userPrincipalName</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>uid</code></li></ul> </p> Required false Type <code>string</code> Default <code>userPrincipalName</code> ENV <code>ALPINE_LDAP_ATTRIBUTE_NAME</code>"},{"location":"reference/configuration/api-server/#alpineldapauthusernameformat","title":"alpine.ldap.auth.username.format","text":"<p>Specifies if the username entered during login needs to be formatted prior  to asserting credentials against the directory. For Active Directory, the  userPrincipal attribute typically ends with the domain, whereas the  samAccountName attribute and other directory server implementations do not.  The %s variable will be substituted with the username asserted during login.    Example (Microsoft Active Directory):  <ul><li><code>%s@example.com</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>%s</code></li></ul> </p> Required false Type <code>string</code> Default <code>null</code> Example <code>%s@example.com</code> ENV <code>ALPINE_LDAP_AUTH_USERNAME_FORMAT</code>"},{"location":"reference/configuration/api-server/#alpineldapbasedn","title":"alpine.ldap.basedn","text":"<p>Specifies the base DN that all queries should search from  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>dc=example,dc=com</code> ENV <code>ALPINE_LDAP_BASEDN</code>"},{"location":"reference/configuration/api-server/#alpineldapbindpassword","title":"alpine.ldap.bind.password","text":"<p>If anonymous access is not permitted, specify a password for the username  used to bind.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_BIND_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpineldapbindusername","title":"alpine.ldap.bind.username","text":"<p>If anonymous access is not permitted, specify a username with limited access  to the directory, just enough to perform searches. This should be the fully  qualified DN of the user.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_BIND_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpineldapenabled","title":"alpine.ldap.enabled","text":"<p>Defines if LDAP will be used for user authentication. If enabled,  <code>alpine.ldap.*</code> properties should be set accordingly.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpineldapgroupsfilter","title":"alpine.ldap.groups.filter","text":"<p>Specifies the LDAP search filter used to retrieve all groups from the directory.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=group)(objectCategory=Group))</code> ENV <code>ALPINE_LDAP_GROUPS_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapgroupssearchfilter","title":"alpine.ldap.groups.search.filter","text":"<p>Specifies the LDAP search filter used to search for groups by their name.  The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(cn={SEARCH_TERM}))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames)(cn={SEARCH_TERM}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))</code> ENV <code>ALPINE_LDAP_GROUPS_SEARCH_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapsecurityauth","title":"alpine.ldap.security.auth","text":"<p>Specifies the LDAP security authentication level to use. Its value is one of  the following strings: \"none\", \"simple\", \"strong\". If this property is empty  or unspecified, the behaviour is determined by the service provider.  </p> Required false Type <code>enum</code> Valid Values <code>[none, simple, strong]</code> Default <code>simple</code> ENV <code>ALPINE_LDAP_SECURITY_AUTH</code>"},{"location":"reference/configuration/api-server/#alpineldapserverurl","title":"alpine.ldap.server.url","text":"<p>Specifies the LDAP server URL.    Examples (Microsoft Active Directory):  <ul> <li><code>ldap://ldap.example.com:3268</code></li> <li><code>ldaps://ldap.example.com:3269</code></li> </ul>  Examples (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul> <li><code>ldap://ldap.example.com:389</code></li> <li><code>ldaps://ldap.example.com:636</code></li> </ul> </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_SERVER_URL</code>"},{"location":"reference/configuration/api-server/#alpineldapteamsynchronization","title":"alpine.ldap.team.synchronization","text":"<p>This option will ensure that team memberships for LDAP users are dynamic and  synchronized with membership of LDAP groups. When a team is mapped to an LDAP  group, all local LDAP users will automatically be assigned to the team if  they are a member of the group the team is mapped to. If the user is later  removed from the LDAP group, they will also be removed from the team. This  option provides the ability to dynamically control user permissions via an  external directory.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_TEAM_SYNCHRONIZATION</code>"},{"location":"reference/configuration/api-server/#alpineldapusergroupsfilter","title":"alpine.ldap.user.groups.filter","text":"<p>Specifies the LDAP search filter to use to query a user and retrieve a list  of groups the user is a member of. The <code>{USER_DN}</code> variable will be substituted  with the actual value of the users DN at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(member={USER_DN}))</code></li></ul>  Example (Microsoft Active Directory - with nested group support):  <ul><li><code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code> ENV <code>ALPINE_LDAP_USER_GROUPS_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapuserprovisioning","title":"alpine.ldap.user.provisioning","text":"<p>Specifies if mapped LDAP accounts are automatically created upon successful  authentication. When a user logs in with valid credentials but an account has  not been previously provisioned, an authentication failure will be returned.  This allows admins to control specifically which ldap users can access the  system and which users cannot. When this value is set to true, a local ldap  user will be created and mapped to the ldap account automatically. This  automatic provisioning only affects authentication, not authorization.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_USER_PROVISIONING</code>"},{"location":"reference/configuration/api-server/#alpineldapuserssearchfilter","title":"alpine.ldap.users.search.filter","text":"<p>Specifies the LDAP search filter used to search for users by their name.  The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(cn={SEARCH_TERM}))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=inetOrgPerson)(cn={SEARCH_TERM}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*))</code> ENV <code>ALPINE_LDAP_USERS_SEARCH_FILTER</code>"},{"location":"reference/configuration/api-server/#observability","title":"Observability","text":""},{"location":"reference/configuration/api-server/#alpinemetricsauthpassword","title":"alpine.metrics.auth.password","text":"<p>Defines the password required to access metrics.  Has no effect when <code>alpine.metrics.auth.username</code> is not set.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_METRICS_AUTH_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinemetricsauthusername","title":"alpine.metrics.auth.username","text":"<p>Defines the username required to access metrics.  Has no effect when <code>alpine.metrics.auth.password</code> is not set.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_METRICS_AUTH_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpinemetricsenabled","title":"alpine.metrics.enabled","text":"<p>Defines whether Prometheus metrics will be exposed.  If enabled, metrics will be available via the /metrics endpoint.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_METRICS_ENABLED</code>"},{"location":"reference/configuration/api-server/#openid-connect","title":"OpenID Connect","text":""},{"location":"reference/configuration/api-server/#alpineoidcclientid","title":"alpine.oidc.client.id","text":"<p>Defines the client ID to be used for OpenID Connect.  The client ID should be the same as the one configured for the frontend,  and will only be used to validate ID tokens.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_OIDC_CLIENT_ID</code>"},{"location":"reference/configuration/api-server/#alpineoidcenabled","title":"alpine.oidc.enabled","text":"<p>Defines if OpenID Connect will be used for user authentication.  If enabled, <code>alpine.oidc.*</code> properties should be set accordingly.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpineoidcissuer","title":"alpine.oidc.issuer","text":"<p>Defines the issuer URL to be used for OpenID Connect.  This issuer MUST support provider configuration via the <code>/.well-known/openid-configuration</code> endpoint.  See also:  <ul> <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata</li> <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig</li> </ul> </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_OIDC_ISSUER</code>"},{"location":"reference/configuration/api-server/#alpineoidcteamsynchronization","title":"alpine.oidc.team.synchronization","text":"<p>This option will ensure that team memberships for OpenID Connect users are dynamic and  synchronized with membership of OpenID Connect groups or assigned roles. When a team is  mapped to an OpenID Connect group, all local OpenID Connect users will automatically be  assigned to the team if they are a member of the group the team is mapped to. If the user  is later removed from the OpenID Connect group, they will also be removed from the team. This  option provides the ability to dynamically control user permissions via the identity provider.  Note that team synchronization is only performed during user provisioning and after successful  authentication.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_TEAM_SYNCHRONIZATION</code>"},{"location":"reference/configuration/api-server/#alpineoidcteamsclaim","title":"alpine.oidc.teams.claim","text":"<p>Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint.  The claim must be an array of strings. Most public identity providers do not support group or role management.  When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint  will most likely need to be configured.  </p> Required false Type <code>string</code> Default <code>groups</code> ENV <code>ALPINE_OIDC_TEAMS_CLAIM</code>"},{"location":"reference/configuration/api-server/#alpineoidcuserprovisioning","title":"alpine.oidc.user.provisioning","text":"<p>Specifies if mapped OpenID Connect accounts are automatically created upon successful  authentication. When a user logs in with a valid access token but an account has  not been previously provisioned, an authentication failure will be returned.  This allows admins to control specifically which OpenID Connect users can access the  system and which users cannot. When this value is set to true, a local OpenID Connect  user will be created and mapped to the OpenID Connect account automatically. This  automatic provisioning only affects authentication, not authorization.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_USER_PROVISIONING</code>"},{"location":"reference/configuration/api-server/#alpineoidcusernameclaim","title":"alpine.oidc.username.claim","text":"<p>Defines the name of the claim that contains the username in the provider's userinfo endpoint.  Common claims are <code>name</code>, <code>username</code>, <code>preferred_username</code> or <code>nickname</code>.  See also:  <ul> <li>https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse</li> </ul> </p> Required false Type <code>string</code> Default <code>name</code> ENV <code>ALPINE_OIDC_USERNAME_CLAIM</code>"},{"location":"reference/configuration/api-server/#task-execution","title":"Task Execution","text":""},{"location":"reference/configuration/api-server/#alpineworkerthreadmultiplier","title":"alpine.worker.thread.multiplier","text":"<p>Defines a multiplier that is used to calculate the number of threads used  by the event subsystem. This property is only used when <code>alpine.worker.threads</code>  is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most)  16 worker threads.  </p> Required true Type <code>integer</code> Default <code>4</code> ENV <code>ALPINE_WORKER_THREAD_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpineworkerthreads","title":"alpine.worker.threads","text":"<p>Defines the number of worker threads that the event subsystem will consume.  Events occur asynchronously and are processed by the Event subsystem. This  value should be large enough to handle most production situations without  introducing much delay, yet small enough not to pose additional load on an  already resource-constrained server.  A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This  can further be tweaked using the <code>alpine.worker.thread.multiplier</code> property.  </p> Required true Type <code>integer</code> Default <code>0</code> ENV <code>ALPINE_WORKER_THREADS</code>"},{"location":"reference/configuration/api-server/#task-scheduling","title":"Task Scheduling","text":""},{"location":"reference/configuration/api-server/#integritymetainitializerlockatleastforinmillis","title":"integrityMetaInitializer.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>INTEGRITYMETAINITIALIZER_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#integritymetainitializerlockatmostforinmillis","title":"integrityMetaInitializer.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>INTEGRITYMETAINITIALIZER_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskcomponentidentificationlockatleastforinmillis","title":"task.componentIdentification.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_COMPONENTIDENTIFICATION_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskcomponentidentificationlockatmostforinmillis","title":"task.componentIdentification.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_COMPONENTIDENTIFICATION_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskcroncomponentidentification","title":"task.cron.componentIdentification","text":"<p>Schedule task every 6 hrs at 25th min  </p> Required true Type <code>cron</code> Default <code>25 */6 * * *</code> ENV <code>TASK_CRON_COMPONENTIDENTIFICATION</code>"},{"location":"reference/configuration/api-server/#taskcrondefectdojosync","title":"task.cron.defectdojo.sync","text":"<p>Schedule task every 24 hrs at 02:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_CRON_DEFECTDOJO_SYNC</code>"},{"location":"reference/configuration/api-server/#taskcronfortifysscsync","title":"task.cron.fortify.ssc.sync","text":"<p>Schedule task every 24 hrs at 02:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_CRON_FORTIFY_SSC_SYNC</code>"},{"location":"reference/configuration/api-server/#taskcronintegrityinitializer","title":"task.cron.integrityInitializer","text":"<p>Schedule task at 0 min past every 12th hr  </p> Required true Type <code>cron</code> Default <code>0 */12 * * *</code> ENV <code>TASK_CRON_INTEGRITYINITIALIZER</code>"},{"location":"reference/configuration/api-server/#taskcronkennasync","title":"task.cron.kenna.sync","text":"<p>Schedule task every 24 hrs at 02:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_CRON_KENNA_SYNC</code>"},{"location":"reference/configuration/api-server/#taskcronldapsync","title":"task.cron.ldapSync","text":"<p>Schedule task every 6 hrs at 0th min  </p> Required true Type <code>cron</code> Default <code>0 */6 * * *</code> ENV <code>TASK_CRON_LDAPSYNC</code>"},{"location":"reference/configuration/api-server/#taskcronmetricsportfolio","title":"task.cron.metrics.portfolio","text":"<p>Schedule task for 10th minute of every hour  </p> Required true Type <code>cron</code> Default <code>10 * * * *</code> ENV <code>TASK_CRON_METRICS_PORTFOLIO</code>"},{"location":"reference/configuration/api-server/#taskcronmetricsvulnerability","title":"task.cron.metrics.vulnerability","text":"<p>Schedule task for 40th minute of every hour  </p> Required true Type <code>cron</code> Default <code>40 * * * *</code> ENV <code>TASK_CRON_METRICS_VULNERABILITY</code>"},{"location":"reference/configuration/api-server/#taskcronmirrorgithub","title":"task.cron.mirror.github","text":"<p>Schedule task every 24 hrs at 02:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_CRON_MIRROR_GITHUB</code>"},{"location":"reference/configuration/api-server/#taskcronmirrornist","title":"task.cron.mirror.nist","text":"<p>Schedule task every 24 hrs at 04:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 4 * * *</code> ENV <code>TASK_CRON_MIRROR_NIST</code>"},{"location":"reference/configuration/api-server/#taskcronmirrorosv","title":"task.cron.mirror.osv","text":"<p>Schedule task every 24 hrs at 03:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 3 * * *</code> ENV <code>TASK_CRON_MIRROR_OSV</code>"},{"location":"reference/configuration/api-server/#taskcronrepometaanalysis","title":"task.cron.repoMetaAnalysis","text":"<p>Schedule task every 24 hrs at 01:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 1 * * *</code> ENV <code>TASK_CRON_REPOMETAANALYSIS</code>"},{"location":"reference/configuration/api-server/#taskcronvulnanalysis","title":"task.cron.vulnAnalysis","text":"<p>Schedule task every 24hrs at 06:00 UTC  </p> Required true Type <code>cron</code> Default <code>0 6 * * *</code> ENV <code>TASK_CRON_VULNANALYSIS</code>"},{"location":"reference/configuration/api-server/#taskcronvulnscancleanup","title":"task.cron.vulnScanCleanUp","text":"<p>Schedule task at 8:05 UTC on Wednesday every week  </p> Required true Type <code>cron</code> Default <code>5 8 * * 4</code> ENV <code>TASK_CRON_VULNSCANCLEANUP</code>"},{"location":"reference/configuration/api-server/#taskcronvulnerabilitypolicybundlefetch","title":"task.cron.vulnerability.policy.bundle.fetch","text":"<p>Schedule task every 5 minutes  </p> Required true Type <code>cron</code> Default <code>*/5 * * * *</code> ENV <code>TASK_CRON_VULNERABILITY_POLICY_BUNDLE_FETCH</code>"},{"location":"reference/configuration/api-server/#taskcronworkflowstatecleanup","title":"task.cron.workflow.state.cleanup","text":"<p>Schedule task every 15 minutes  </p> Required true Type <code>cron</code> Default <code>*/15 * * * *</code> ENV <code>TASK_CRON_WORKFLOW_STATE_CLEANUP</code>"},{"location":"reference/configuration/api-server/#taskldapsynclockatleastforinmillis","title":"task.ldapSync.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_LDAPSYNC_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskldapsynclockatmostforinmillis","title":"task.ldapSync.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_LDAPSYNC_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmetricsportfoliolockatleastforinmillis","title":"task.metrics.portfolio.lockAtLeastForInMillis","text":"<p>Specifies minimum amount of time for which the lock should be kept.  Its main purpose is to prevent execution from multiple nodes in case of really short tasks and clock difference between the nodes.  </p> Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_METRICS_PORTFOLIO_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmetricsportfoliolockatmostforinmillis","title":"task.metrics.portfolio.lockAtMostForInMillis","text":"<p>Specifies how long the lock should be kept in case the executing node dies.  This is just a fallback, under normal circumstances the lock is released as soon the tasks finishes.  Set lockAtMostFor to a value which is much longer than normal execution time. Default value is 15min  Lock will be extended dynamically till task execution is finished  </p> Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_METRICS_PORTFOLIO_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmetricsvulnerabilitylockatleastforinmillis","title":"task.metrics.vulnerability.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_METRICS_VULNERABILITY_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmetricsvulnerabilitylockatmostforinmillis","title":"task.metrics.vulnerability.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_METRICS_VULNERABILITY_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmirrorepsslockatleastforinmillis","title":"task.mirror.epss.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_MIRROR_EPSS_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskmirrorepsslockatmostforinmillis","title":"task.mirror.epss.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_MIRROR_EPSS_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskportfoliorepometaanalysislockatleastforinmillis","title":"task.portfolio.repoMetaAnalysis.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_PORTFOLIO_REPOMETAANALYSIS_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskportfoliorepometaanalysislockatmostforinmillis","title":"task.portfolio.repoMetaAnalysis.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_PORTFOLIO_REPOMETAANALYSIS_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskportfoliovulnanalysislockatleastforinmillis","title":"task.portfolio.vulnAnalysis.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>90000</code> ENV <code>TASK_PORTFOLIO_VULNANALYSIS_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskportfoliovulnanalysislockatmostforinmillis","title":"task.portfolio.vulnAnalysis.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_PORTFOLIO_VULNANALYSIS_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskschedulerinitialdelay","title":"task.scheduler.initial.delay","text":"<p>Scheduling tasks after 3 minutes (3601000) of starting application  </p> Required true Type <code>integer</code> Default <code>180000</code> ENV <code>TASK_SCHEDULER_INITIAL_DELAY</code>"},{"location":"reference/configuration/api-server/#taskschedulerpollinginterval","title":"task.scheduler.polling.interval","text":"<p>Cron expressions for tasks have the precision of minutes so polling every minute  </p> Required true Type <code>integer</code> Default <code>60000</code> ENV <code>TASK_SCHEDULER_POLLING_INTERVAL</code>"},{"location":"reference/configuration/api-server/#taskworkflowstatecleanuplockatleastforinmillis","title":"task.workflow.state.cleanup.lockAtLeastForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_WORKFLOW_STATE_CLEANUP_LOCKATLEASTFORINMILLIS</code>"},{"location":"reference/configuration/api-server/#taskworkflowstatecleanuplockatmostforinmillis","title":"task.workflow.state.cleanup.lockAtMostForInMillis","text":"Required true Type <code>integer</code> Default <code>900000</code> ENV <code>TASK_WORKFLOW_STATE_CLEANUP_LOCKATMOSTFORINMILLIS</code>"},{"location":"reference/configuration/mirror-service/","title":"Mirror Service","text":""},{"location":"reference/configuration/mirror-service/#datasource","title":"Datasource","text":""},{"location":"reference/configuration/mirror-service/#mirrordatasourceosvalias-sync-enabled","title":"mirror.datasource.osv.alias-sync-enabled","text":"<p>Defines whether vulnerability aliases should be parsed from OSV.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourceosvbase-url","title":"mirror.datasource.osv.base-url","text":"<p>Defines the URL of the OSV storage bucket.  </p> Required false Type <code>string</code> Default <code>https://osv-vulnerabilities.storage.googleapis.com</code> ENV <code>MIRROR_DATASOURCE_OSV_BASE_URL</code>"},{"location":"reference/configuration/mirror-service/#http","title":"HTTP","text":""},{"location":"reference/configuration/mirror-service/#quarkushttpport","title":"quarkus.http.port","text":"<p>HTTP port to listen on. Application metrics will be available via this port.  </p> Required false Type <code>integer</code> Default <code>8093</code> ENV <code>QUARKUS_HTTP_PORT</code>"},{"location":"reference/configuration/mirror-service/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/mirror-service/#kafka-streamscommitintervalms","title":"kafka-streams.commit.interval.ms","text":"<p>Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.  The Kafka default of <code>30s</code> has been modified to <code>5s</code>.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms for details.  </p> Required false Type <code>integer</code> Default <code>5000</code> ENV <code>KAFKA_STREAMS_COMMIT_INTERVAL_MS</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsdeserializationcount","title":"kafka-streams.exception.thresholds.deserialization.count","text":"<p>Defines the threshold for records failing to be deserialized within <code>kafka-streams.exception.thresholds.deserialization.interval</code>.  Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsdeserializationinterval","title":"kafka-streams.exception.thresholds.deserialization.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.deserialization.count</code> records are  allowed to fail deserialization. Deserialization failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsprocessingcount","title":"kafka-streams.exception.thresholds.processing.count","text":"<p>Defines the threshold for records failing to be processed within <code>kafka-streams.exception.thresholds.processing.interval</code>.  Processing failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>50</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsprocessinginterval","title":"kafka-streams.exception.thresholds.processing.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.processing.count</code> records are  allowed to fail processing. Processing failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsproductioncount","title":"kafka-streams.exception.thresholds.production.count","text":"<p>Defines the threshold for records failing to be produced within <code>kafka-streams.exception.thresholds.production.interval</code>.  Production failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsproductioninterval","title":"kafka-streams.exception.thresholds.production.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.production.count</code> records are  allowed to fail producing. Production failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsnumstreamthreads","title":"kafka-streams.num.stream.threads","text":"<p>The number of threads to allocate for stream processing tasks.  Note that Specifying a number higher than the number of input partitions provides no additional benefit,  as excess threads will simply run idle.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads for details.  </p> Required true Type <code>integer</code> Default <code>3</code> ENV <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code>"},{"location":"reference/configuration/mirror-service/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"<p>Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for details.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>broker-01.acme.com:9092,broker-02.acme.com:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/mirror-service/#kafkamaxrequestsize","title":"kafka.max.request.size","text":"<p>Defines the maximum size of a Kafka producer request in bytes.    Some messages like Bill of Vulnerabilities can be bigger than the default 1MiB.  Since the size check is performed before records are compressed, this value may need to be increased  even though the compressed value is much smaller. The Kafka default of 1MiB has been raised to 2MiB.    Refer to https://kafka.apache.org/documentation/#producerconfigs_max.request.size for details.  </p> Required true Type <code>integer</code> Default <code>2097152</code> ENV <code>KAFKA_MAX_REQUEST_SIZE</code>"},{"location":"reference/configuration/mirror-service/#kafkatopicprefix","title":"kafka.topic.prefix","text":"<p>Defines an optional prefix to assume for all Kafka topics the application  consumes from, or produces to. The prefix will also be prepended to the  application's consumer group ID.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>acme-</code> ENV <code>KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/mirror-service/#quarkuskafka-streamsapplication-id","title":"quarkus.kafka-streams.application-id","text":"<p>Defines the ID to uniquely identify this application in the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_application.id for details.  </p> Required false Type <code>string</code> Default <code>${kafka.topic.prefix}hyades-mirror-service</code> ENV <code>QUARKUS_KAFKA_STREAMS_APPLICATION_ID</code>"},{"location":"reference/configuration/mirror-service/#observability","title":"Observability","text":""},{"location":"reference/configuration/mirror-service/#quarkuslogconsolejson","title":"quarkus.log.console.json","text":"<p>Defines whether logs should be written in JSON format.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>QUARKUS_LOG_CONSOLE_JSON</code>"},{"location":"reference/configuration/overview/","title":"Configuration","text":"<p>All available configuration options used by all applications are listed in their respective <code>application.properties</code>. Options can be provided via environment variables as well, refer to the Quarkus docs for details.</p> <p>Not all options are supposed to be tweaked by users though. This document contains an overview of all options that are expected to be changed by users.</p>"},{"location":"reference/configuration/overview/#kafka-topic-configuration","title":"Kafka Topic Configuration","text":"<p>Kafka topics (including internal topics) can be configured with custom prefix. In order to provide custom prefix, below environment variable can be used.</p> Environment Variable Description Default Required <code>KAFKA_TOPIC_PREFIX</code> Prefix for topic names - \u274c"},{"location":"reference/configuration/overview/#notification-publisher","title":"Notification Publisher","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>PARALLEL_CONSUMER_MAX_CONCURRENCY</code> Number of threads to process notifications with <code>6</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_INITIAL_DELAY</code> Initial delay before retrying notification delivery <code>3S</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_MULTIPLIER</code> Multiplier for retry delays <code>0.3</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_RANDOMIZATION_FACTOR</code> Randomization factory for jitter in retry delays <code>0.3</code> \u274c <code>PARALLEL_CONSUMER_RETRY_MAX_DURATION</code> Maximum duration of delays between retry attempts <code>2M</code> \u2705 <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#repository-meta-analyzer","title":"Repository Meta Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#vulnerability-analyzer","title":"Vulnerability Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>STATE_STORE_TYPE</code> Whether to use in-memory or persistent (RocksDB) Kafka Streams state stores <code>in_memory</code> \u2705 <code>STATE_STORE_ROCKS_DB_COMPACTION_STYLE</code> Compaction style to use for RocksDB state stores - \u274c <code>STATE_STORE_ROCKS_DB_COMPRESSION_TYPE</code> Compression type to use for RocksDB state stores - \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <code>SCANNER_INTERNAL_ENABLED</code> Enable the internal vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_ENABLED</code> Enable the OSS Index vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by OssIndex analyzer. This depends on standard and custom supported package urls <code>cargo,composer,gem,hex,maven,npm,nuget,pypi,rpm,conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by ossindex. These are not part of the standard package urls list <code>conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_API_USERNAME</code> OSS Index API username - \u274c <code>SCANNER_OSSINDEX_API_TOKEN</code> OSS Index API token - \u274c <code>SCANNER_OSSINDEX_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> \u274c <code>SCANNER_OSSINDEX_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSS Index <code>false</code> \u274c <code>SCANNER_SNYK_ENABLED</code> Enable the Snyk vulnerability scanner <code>false</code> \u274c <code>SCANNER_SNYK_API_ORG_ID</code> Snyk organization ID - When Snyk is enabled <code>SCANNER_SNYK_API_TOKENS</code> Comma-separated list of Snyk API tokens - When Snyk is enabled <code>SCANNER_SNYK_API_VERSION</code> Version of the Snyk API to use <code>2022-12-15</code> When Snyk is enabled <code>SCANNER_SNYK_SEVERITY_SOURCE_PRIORITY</code> Priority of preferred source for vulnerability severities <code>nvd,snyk,redhat,suse</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_SIZE</code> Max size of batch at which it will be submitted <code>100</code> When Snyk is enabled <code>SCANNER_SNYK_ALIAS_SYNC_ENABLED</code> Enable alias syncing for Snyk <code>false</code> \u274c <code>SCANNER_SNYK_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by Snyk analyzer. This depends on supported standard and custom package urls <code>cargo,cocoapods,composer,gem,generic,hex,maven,npm,nuget,pypi,swift,golang</code> \u274c <code>SCANNER_SNYK_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by Snyk. These are not part of the standard package urls list <code>cocoapods,apk,swift</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#mirror-service","title":"Mirror Service","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>MIRROR_DATASOURCE_GITHUB_ALIAS_SYNC_ENABLED</code> Enable alias syncing for GitHub Advisories <code>false</code> \u274c <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSV <code>false</code> \u274c <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/repo-meta-analyzer/","title":"Repository Metadata Analyzer","text":""},{"location":"reference/configuration/repo-meta-analyzer/#cache","title":"Cache","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscachecaffeinemetaanalyzerexpire-after-write","title":"quarkus.cache.caffeine.\"metaAnalyzer\".expire-after-write","text":"<p>Defines the time-to-live of cache entries.  </p> Required true Type <code>duration</code> Default <code>PT2H</code> ENV <code>QUARKUS_CACHE_CAFFEINE__METAANALYZER__EXPIRE_AFTER_WRITE</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscachecaffeinemetaanalyzerinitial-capacity","title":"quarkus.cache.caffeine.\"metaAnalyzer\".initial-capacity","text":"<p>Defines the initial capacity of the cache.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>QUARKUS_CACHE_CAFFEINE__METAANALYZER__INITIAL_CAPACITY</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscacheenabled","title":"quarkus.cache.enabled","text":"<p>Defines whether caching of analysis results shall be enabled.  </p> Required true Type <code>boolean</code> Default <code>true</code> ENV <code>QUARKUS_CACHE_ENABLED</code>"},{"location":"reference/configuration/repo-meta-analyzer/#database","title":"Database","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourcejdbcurl","title":"quarkus.datasource.jdbc.url","text":"<p>Specifies the JDBC URL to use when connecting to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_JDBC_URL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourcepassword","title":"quarkus.datasource.password","text":"<p>Specifies the password to use when authenticating to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_PASSWORD</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourceusername","title":"quarkus.datasource.username","text":"<p>Specifies the username to use when authenticating to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_USERNAME</code>"},{"location":"reference/configuration/repo-meta-analyzer/#general","title":"General","text":""},{"location":"reference/configuration/repo-meta-analyzer/#secretkeypath","title":"secret.key.path","text":"<p>Defines the path to the secret key to be used for data encryption and decryption.  </p> Required false Type <code>string</code> Default <code>~/.dependency-track/keys/secret.key</code> ENV <code>SECRET_KEY_PATH</code>"},{"location":"reference/configuration/repo-meta-analyzer/#http","title":"HTTP","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkushttpport","title":"quarkus.http.port","text":"<p>HTTP port to listen on. Application metrics will be available via this port.  </p> Required false Type <code>integer</code> Default <code>8091</code> ENV <code>QUARKUS_HTTP_PORT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsautooffsetreset","title":"kafka-streams.auto.offset.reset","text":"<p>Refer to https://kafka.apache.org/documentation/#consumerconfigs_auto.offset.reset for details.  </p> Required false Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_STREAMS_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamscommitintervalms","title":"kafka-streams.commit.interval.ms","text":"<p>Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.  The Kafka default of <code>30s</code> has been modified to <code>5s</code>.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms for details.  </p> Required false Type <code>integer</code> Default <code>5000</code> ENV <code>KAFKA_STREAMS_COMMIT_INTERVAL_MS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsdeserializationcount","title":"kafka-streams.exception.thresholds.deserialization.count","text":"<p>Defines the threshold for records failing to be deserialized within <code>kafka-streams.exception.thresholds.deserialization.interval</code>.  Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsdeserializationinterval","title":"kafka-streams.exception.thresholds.deserialization.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.deserialization.count</code> records are  allowed to fail deserialization. Deserialization failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsprocessingcount","title":"kafka-streams.exception.thresholds.processing.count","text":"<p>Defines the threshold for records failing to be processed within <code>kafka-streams.exception.thresholds.processing.interval</code>.  Processing failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>50</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsprocessinginterval","title":"kafka-streams.exception.thresholds.processing.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.processing.count</code> records are  allowed to fail processing. Processing failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsproductioncount","title":"kafka-streams.exception.thresholds.production.count","text":"<p>Defines the threshold for records failing to be produced within <code>kafka-streams.exception.thresholds.production.interval</code>.  Production failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsproductioninterval","title":"kafka-streams.exception.thresholds.production.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.production.count</code> records are  allowed to fail producing. Production failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsmetricsrecordinglevel","title":"kafka-streams.metrics.recording.level","text":"<p>Refer to https://kafka.apache.org/documentation/#adminclientconfigs_metrics.recording.level for details.  </p> Required false Type <code>enum</code> Valid Values <code>[INFO, DEBUG, TRACE]</code> Default <code>DEBUG</code> ENV <code>KAFKA_STREAMS_METRICS_RECORDING_LEVEL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsnumstreamthreads","title":"kafka-streams.num.stream.threads","text":"<p>Refer to https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads for details.  </p> Required true Type <code>integer</code> Default <code>3</code> ENV <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"<p>Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for details.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>broker-01.acme.com:9092,broker-02.acme.com:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafkatopicprefix","title":"kafka.topic.prefix","text":"<p>Defines an optional prefix to assume for all Kafka topics the application  consumes from, or produces to. The prefix will also be prepended to the  application's consumer group ID.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>acme-</code> ENV <code>KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuskafka-streamsapplication-id","title":"quarkus.kafka-streams.application-id","text":"<p>Defines the ID to uniquely identify this application in the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_application.id for details.  </p> Required false Type <code>string</code> Default <code>${kafka.topic.prefix}hyades-repository-meta-analyzer</code> ENV <code>QUARKUS_KAFKA_STREAMS_APPLICATION_ID</code>"},{"location":"reference/configuration/repo-meta-analyzer/#observability","title":"Observability","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkuslogconsolejson","title":"quarkus.log.console.json","text":"<p>Defines whether logs should be written in JSON format.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>QUARKUS_LOG_CONSOLE_JSON</code>"},{"location":"usage/policy-compliance/expressions/","title":"Expressions","text":""},{"location":"usage/policy-compliance/expressions/#introduction","title":"Introduction","text":"<p>Dependency-Track allows policy conditions to be defined using the Common Expression Language (CEL), enabling more flexibility, and more control compared to predefined conditions.</p> <p>To use CEL, simply select the subject <code>Expression</code> when adding a new condition. A code editor will appear in which expressions can be provided.</p> <p></p> <p>In addition to the expression itself, it's necessary to specify a violation type, which may be any of <code>License</code>, <code>Operational</code>, or <code>Security</code>. The violation type aids in communicating what kind of risk is introduced by the condition being matched.</p>"},{"location":"usage/policy-compliance/expressions/#syntax","title":"Syntax","text":"<p>The CEL syntax is similar to other C-style languages like Java and JavaScript. However, CEL is not Turing-complete. As such, it does not support constructs like <code>if</code> statements or loops (i.e. <code>for</code>, <code>while</code>).</p> <p>As a compensation for missing loops, CEL offers macros like <code>all</code>, <code>exists</code>, <code>exists_one</code>, <code>map</code>, and <code>filter</code>. Refer to the macros documentation for more details, or have a look at the examples to see how they may be utilized in practice.</p> <p>CEL syntax is described thoroughly in the official language definition.</p>"},{"location":"usage/policy-compliance/expressions/#evaluation-context","title":"Evaluation Context","text":"<p>Conditions are scoped to individual components. Each condition is evaluated for every single component in a project.</p> <p>The context in which expressions are evaluated in contains the following variables:</p> Variable Type Description <code>component</code> <code>Component</code> The component being evaluated <code>project</code> <code>Project</code> The project the component is part of <code>vulns</code> <code>list(Vulnerability)</code> Vulnerabilities the component is affected by"},{"location":"usage/policy-compliance/expressions/#best-practices","title":"Best Practices","text":"<ol> <li>Keep expressions simple and concise. The more complex an expression becomes, the harder it gets to determine why it did or did not match. Use policy operators (<code>Any</code>, <code>All</code>) to chain multiple expressions if practical.</li> <li>Call functions last. Custom functions involve additional computation that is more expensive than simple field accesses. Performing any checks on fields first, and calling functions last, oftentimes allows evaluation to short-circuit.</li> <li>Remove conditions that are no longer needed. Dependency-Track analyzes the configured expressions to determine what data it has to load from the database in order to evaluate them. The more fields are being accessed, the more data has to be loaded. Removal of outdated conditions thus has a direct positive performance impact.</li> </ol>"},{"location":"usage/policy-compliance/expressions/#examples","title":"Examples","text":""},{"location":"usage/policy-compliance/expressions/#component-age","title":"Component age","text":"<p>Besides out-of-date versions, component age is another indicator of potential risk. Components may be on the latest available version, but still be 20 years old. </p> <p>Component age can be evaluated using the <code>compare_age</code> function. The first function argument  is a numeric comparator (<code>&lt;</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&gt;=</code>), and the second is a duration in ISO8601 notation.</p> <p>The following expression matches Components that are two years old, or even older:</p> <pre><code>component.compare_age(\"&gt;=\", \"P2Y\")\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#component-blacklist","title":"Component blacklist","text":"<p>The following expression matches on the Component's Package URL, using a regular expression in RE2 syntax. Additionally, it checks whether the Component's version falls into a given vers range, consisting of multiple constraints.</p> <pre><code>component.purl.matches(\"^pkg:maven/com.acme/acme-lib\\\\b.*\")\n  &amp;&amp; component.matches_range(\"vers:maven/&gt;0|&lt;1|!=0.2.4\")\n</code></pre> <p>The expression will match:</p> <ul> <li><code>pkg:maven/com.acme/acme-lib@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.9.9</code></li> </ul> <p>but not:</p> <ul> <li><code>pkg:maven/com.acme/acme-library@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.2.4</code></li> </ul> <p><code>matches_range</code> currently supports the following versioning schemes:</p> Versioning Scheme Ecosystem <code>deb</code> Debian / Ubuntu <code>generic</code> Generic / Any <code>golang</code> Go <code>maven</code> Java / Maven <code>npm</code> JavaScript / NodeJS <code>rpm</code> CentOS / Fedora / Red Hat / SUSE <p>Note</p> <p>If the ecosystem of the component(s) to match against is known upfront, it's good practice to use the according versioning scheme in <code>matches_range</code>. This helps with accuracy, as versioning schemes have different nuances across ecosystems, which makes comparisons error-prone.</p>"},{"location":"usage/policy-compliance/expressions/#dependency-graph-traversal","title":"Dependency graph traversal","text":"<p>The following expression matches Components that are a (possibly transitive) dependency of a Component with name <code>foo</code>, but only if a Component with name <code>bar</code> is also present in the Project.</p> <pre><code>component.is_dependency_of(v1.Component{name: \"foo\"})\n  &amp;&amp; project.depends_on(v1.Component{name: \"bar\"})\n</code></pre> <p><code>is_dependency_of</code> and <code>depends_on</code> lookups currently support the following Component fields:</p> <ul> <li><code>uuid</code></li> <li><code>group</code></li> <li><code>name</code></li> <li><code>version</code></li> <li><code>classifier</code></li> <li><code>cpe</code></li> <li><code>purl</code></li> <li><code>swid_tag_id</code></li> <li><code>internal</code></li> </ul> <p>Initially, only exact matches on those fields are supported. In the future, more sophisticated matching options will be added.</p> <p>Note</p> <p>When constructing objects like Component on-the-fly, it is necessary to use their version namespace, i.e. <code>v1</code>. This is required in order to perform type checking, as well as ensuring backward compatibility.</p>"},{"location":"usage/policy-compliance/expressions/#license-blacklist","title":"License blacklist","text":"<p>The following expression matches Components that are not internal to the organization, and have either:</p> <ul> <li>No resolved License at all</li> <li>A resolved License that is not part of the <code>Permissive</code> license group</li> </ul> <pre><code>!component.is_internal &amp;&amp; (\n  !has(component.resolved_license)\n    || component.resolved_license.groups.exisits(licenseGroup, \n         licenseGroup.name == \"Permissive\")\n)\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerability-blacklist","title":"Vulnerability blacklist","text":"<p>The following expression matches Components in Projects tagged as <code>3rd-party</code>, with at least one Vulnerability being any of the given blacklisted IDs.</p> <pre><code>\"3rd-party\" in project.tags\n  &amp;&amp; vulns.exists(vuln, vuln.id in [\n       \"CVE-2017-5638\",  // struts RCE\n       \"CVE-2021-44228\", // log4shell\n       \"CVE-2022-22965\", // spring4shell\n     ])\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerabilities-with-high-severity-in-public-facing-projects","title":"Vulnerabilities with high severity in public facing projects","text":"<p>The following expression matches Components in Projects tagged as <code>public-facing</code>, with at least one <code>HIGH</code> or <code>CRITICAL</code> Vulnerability, where the CVSSv3 attack vector is <code>Network</code>.</p> <pre><code>\"public-facing\" in project.tags\n  &amp;&amp; vulns.exists(vuln,\n    vuln.severity in [\"HIGH\", \"CRITICAL\"]\n      &amp;&amp; vuln.cvssv3_vector.matches(\".*/AV:N/.*\")\n  )\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#reference","title":"Reference","text":""},{"location":"usage/policy-compliance/expressions/#types","title":"Types","text":""},{"location":"usage/policy-compliance/expressions/#component","title":"<code>Component</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>is_internal</code> <code>bool</code> Is internal? <code>md5</code> <code>string</code> MD5 hash <code>sha1</code> <code>string</code> SHA1 hash <code>sha256</code> <code>string</code> SHA256 hash <code>sha384</code> <code>string</code> SHA384 hash <code>sha512</code> <code>string</code> SHA512 hash <code>sha3_256</code> <code>string</code> SHA3-256 hash <code>sha3_384</code> <code>string</code> SHA3-384 hash <code>sha3_512</code> <code>string</code> SHA3-512 hash <code>blake2b_256</code> <code>string</code> BLAKE2b-256 hash <code>blake2b_384</code> <code>string</code> BLAKE2b-384 hash <code>blake2b_512</code> <code>string</code> BLAKE2b-512 hash <code>blake3</code> <code>string</code> BLAKE3 hash <code>license_name</code> <code>string</code> License name (if unresolved) <code>license_expression</code> <code>string</code> SPDX license expression <code>resolved_license</code> <code>License</code> Resolved license <code>published_at</code> <code>google.protobuf.Timestamp</code> When the component was published <code>latest_version</code> <code>string</code> Latest known version"},{"location":"usage/policy-compliance/expressions/#license","title":"<code>License</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> SPDX license ID <code>name</code> <code>string</code> License name <code>groups</code> <code>list(License.Group)</code> Groups this license is included in <code>is_osi_approved</code> <code>bool</code> Is OSI-approved? <code>is_fsf_libre</code> <code>bool</code> Is included in FSF license list? <code>is_deprecated_id</code> <code>bool</code> Uses a deprecated SPDX license ID? <code>is_custom</code> <code>bool</code> Is custom / not included in SPDX license list?"},{"location":"usage/policy-compliance/expressions/#licensegroup","title":"<code>License.Group</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>name</code> <code>string</code> Group name"},{"location":"usage/policy-compliance/expressions/#project","title":"<code>Project</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>is_active</code> <code>bool</code> Is active? <code>tags</code> <code>list(string)</code> Tags <code>properties</code> <code>list(Project.Property)</code> Properties <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>last_bom_import</code> <code>google.protobuf.Timestamp</code>"},{"location":"usage/policy-compliance/expressions/#projectproperty","title":"<code>Project.Property</code>","text":"Field Type Description <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>value</code> <code>string</code> <code>type</code> <code>string</code>"},{"location":"usage/policy-compliance/expressions/#vulnerability","title":"<code>Vulnerability</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>CVE-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>NVD</code>) <code>aliases</code> <code>list(Vulnerability.Alias)</code> Known aliases <code>cwes</code> <code>list(int)</code> CWE IDs <code>created</code> <code>google.protobuf.Timestamp</code> When the vulnerability was created <code>published</code> <code>google.protobuf.Timestamp</code> When the vulnerability was published <code>updated</code> <code>google.protobuf.Timestamp</code> Then the vulnerability was updated <code>severity</code> <code>string</code> <code>cvssv2_base_score</code> <code>double</code> CVSSv2 base score <code>cvssv2_impact_subscore</code> <code>double</code> CVSSv2 impact sub score <code>cvssv2_exploitability_subscore</code> <code>double</code> CVSSv2 exploitability sub score <code>cvssv2_vector</code> <code>string</code> CVSSv2 vector <code>cvssv3_base_score</code> <code>double</code> CVSSv3 base score <code>cvssv3_impact_subscore</code> <code>double</code> CVSSv3 impact sub score <code>cvssv3_exploitability_subscore</code> <code>double</code> CVSSv3 exploitability sub score <code>cvssv3_vector</code> <code>string</code> CVSSv3 vector <code>owasp_rr_likelihood_score</code> <code>double</code> OWASP Risk Rating likelihood score <code>owasp_rr_technical_impact_score</code> <code>double</code> OWASP Risk Rating technical impact score <code>owasp_rr_business_impact_score</code> <code>double</code> OWASP Risk Rating business impact score <code>owasp_rr_vector</code> <code>string</code> OWASP Risk Rating vector <code>epss_score</code> <code>double</code> EPSS score <code>epss_percentile</code> <code>double</code> EPSS percentile"},{"location":"usage/policy-compliance/expressions/#vulnerabilityalias","title":"<code>Vulnerability.Alias</code>","text":"Field Type Description <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>GHSA-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>GITHUB</code>)"},{"location":"usage/policy-compliance/expressions/#function-definitions","title":"Function Definitions","text":"<p>In addition to the standard definitions of the CEL specification, Dependency-Track offers additional functions to unlock even more use cases:</p> Symbol Type Description <code>depends_on</code> <code>(Project, Component)</code> -&gt; <code>bool</code> Check if <code>Project</code> depends on <code>Component</code> <code>compare_age</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s age matches a given duration <code>is_dependency_of</code> <code>(Component, Component)</code> -&gt; <code>bool</code> Check if a <code>Component</code> is a dependency of another <code>Component</code> <code>matches_range</code> <code>(Project, string)</code> -&gt; <code>bool</code><code>(Component, string)</code> -&gt; <code>bool</code> Check if a <code>Project</code> or <code>Component</code> matches a vers range <code>matches_version_distance</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s version matches a given distance"},{"location":"usage/policy-compliance/overview/","title":"Overview","text":"<p>Organizations can create policies and measure policy violations across the portfolio, and against individual projects and components. Policies are configurable and can be enforced for the portfolio, or can be limited to specific projects. Policies are evaluated when an SBOM is uploaded.</p> <p>There are three types of policy violations:</p> <ul> <li>License</li> <li>Security</li> <li>Operational</li> </ul>"},{"location":"usage/policy-compliance/overview/#license-violation","title":"License Violation","text":"<p>Policy conditions can specify zero or more SPDX license IDs as well as license groups. Dependency-Track comes with pre-configured groups of related licenses (e.g. Copyleft) that provide a starting point for organizations to create custom license policies.</p>"},{"location":"usage/policy-compliance/overview/#security-violation","title":"Security Violation","text":"<p>Policy conditions can specify the severity of vulnerabilities. A vulnerability affecting a component can result in a policy violation if the policy condition matches the severity of the vulnerability. Vulnerabilities that are suppressed will not result in a policy violation.</p>"},{"location":"usage/policy-compliance/overview/#operational-violation","title":"Operational Violation","text":"<p>Policy conditions can specify zero or more:</p> <ul> <li>Coordinates (group, name, version)</li> <li>Package URL</li> <li>CPE</li> <li>SWID Tag ID</li> <li>Hash (MD5, SHA, SHA3, Blake2b, Blake3)</li> </ul> <p>This allows organizations to create lists of allowable and/or prohibited components. Future versions of Dependency-Track will incorporate additional operational parameters into the policy framework.</p>"}]}