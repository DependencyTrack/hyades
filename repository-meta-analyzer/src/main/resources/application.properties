# @category: General
# @hidden
quarkus.application.name=hyades-repository-meta-analyzer

# HTTP port to listen on. Application metrics will be available via this port.
#
# @category: HTTP
# @type:     integer
quarkus.http.port=8091

# Defines whether logs should be written in JSON format.
#
# @category: Observability
# @type:     boolean
quarkus.log.console.json=false

# @category: Observability
# @hidden
quarkus.log.category."org.apache.kafka".level=WARN

# Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.
# <br/><br/>
# Refer to <https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers> for details.
#
# @category: Kafka
# @example:  broker-01.acme.com:9092,broker-02.acme.com:9092
# @type:     string
# @required
# kafka.bootstrap.servers=

# @category: Kafka
# @hidden
%dev.kafka.bootstrap.servers=localhost:9092

# Defines an optional prefix to assume for all Kafka topics the application
# consumes from, or produces to. The prefix will also be prepended to the
# application's consumer group ID.
#
# @category: Kafka
# @example:  acme-
# @type:     string
dt.kafka.topic.prefix=

# Defines the ID to uniquely identify this application in the Kafka cluster.
# <br/><br/>
# Refer to <https://kafka.apache.org/documentation/#streamsconfigs_application.id> for details.
#
# @category: Kafka
# @type:     string
quarkus.kafka-streams.application-id=${dt.kafka.topic.prefix}hyades-repository-meta-analyzer

# @category: Kafka
# @hidden
quarkus.kafka-streams.topics=\
  ${dt.kafka.topic.prefix}dtrack.repo-meta-analysis.component,\
  ${dt.kafka.topic.prefix}dtrack.repo-meta-analysis.result

# Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.
# The Kafka default of `30s` has been modified to `5s`.
# <br/><br/>
# Refer to <https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms> for details.
#
# @category: Kafka
# @type:     integer
kafka-streams.commit.interval.ms=5000

# Refer to <https://kafka.apache.org/documentation/#consumerconfigs_auto.offset.reset> for details.
#
# @category:     Kafka
# @type:         enum
# @valid-values: [earliest, latest, none]
kafka-streams.auto.offset.reset=earliest

# Refer to <https://kafka.apache.org/documentation/#adminclientconfigs_metrics.recording.level> for details.
#
# @category:     Kafka
# @type:         enum
# @valid-values: [INFO, DEBUG, TRACE]
kafka-streams.metrics.recording.level=DEBUG

# Refer to <https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads> for details.
#
# @category: Kafka
# @type:     integer
# @required
kafka-streams.num.stream.threads=3

# @category: Kafka
# @hidden
kafka-streams.compression.type=snappy

# @category: Kafka
# @hidden
quarkus.kafka.snappy.enabled=true

# Using the default value of 30s in order to make the property configurable via environment variables.
# Without this, Quarkus will interpret "KAFKA_STREAMS" as "kafka.streams", which fails its internal property
# prefix check, which is expecting a "kafka-streams" prefix.
# Overriding this property is required in cases where the Delete topic permission can not be granted to
# Kafka clients (e.g. in multi-tenant Kafka clusters).
#
# @category: Kafka
# @type:     integer
# @hidden
kafka-streams.repartition.purge.interval.ms=30000

# Quarkus' ClassLoader black magic doesn't play well with
# native libraries like the one required by Snappy.
# It's causing failures when multiple tests with different
# TestProfile are executed in the same test run.
# TODO: Remove after upgrade to Quarkus 3.11.x (https://github.com/quarkusio/quarkus/issues/39767)
#
# @category: Kafka
# @hidden
%test.quarkus.kafka.snappy.enabled=false

# @category: Kafka
# @hidden
%test.kafka.compression.type=none

# @category: Kafka
# @hidden
kafka-streams.default.deserialization.exception.handler=org.dependencytrack.kstreams.exception.DeserializationExceptionHandler

# @category: Kafka
# @hidden
kafka-streams.default.production.exception.handler=org.dependencytrack.kstreams.exception.ProductionExceptionHandler

# Defines the threshold for records failing to be deserialized within kafka-streams.exception.thresholds.deserialization.interval.
# Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application
# to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     integer
# @required
kafka-streams.exception.thresholds.deserialization.count=5

# Defines the interval within which up to kafka-streams.exception.thresholds.deserialization.count records are
# allowed to fail deserialization. Deserialization failures within the threshold will be logged,
# failures exceeding the threshold cause the application to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     duration
# @required
kafka-streams.exception.thresholds.deserialization.interval=PT30M

# Defines the threshold for records failing to be processed within kafka-streams.exception.thresholds.processing.interval.
# Processing failures within the threshold will be logged, failures exceeding the threshold cause the application
# to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     integer
# @required
kafka-streams.exception.thresholds.processing.count=50

# Defines the interval within which up to kafka-streams.exception.thresholds.processing.count records are
# allowed to fail processing. Processing failures within the threshold will be logged,
# failures exceeding the threshold cause the application to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     duration
# @required
kafka-streams.exception.thresholds.processing.interval=PT30M

# Defines the threshold for records failing to be produced within kafka-streams.exception.thresholds.production.interval.
# Production failures within the threshold will be logged, failures exceeding the threshold cause the application
# to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     integer
# @required
kafka-streams.exception.thresholds.production.count=5

# Defines the interval within which up to kafka-streams.exception.thresholds.production.count records are
# allowed to fail producing. Production failures within the threshold will be logged,
# failures exceeding the threshold cause the application to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     duration
# @required
kafka-streams.exception.thresholds.production.interval=PT30M

# @category: Kafka
# @hidden
quarkus.kafka.devservices.image-name=docker.redpanda.com/redpandadata/redpanda:v24.2.17

# @category: Kafka
# @hidden
quarkus.kafka.devservices.topic-partitions."dtrack.repo-meta-analysis.component"=3

# @category: Kafka
# @hidden
quarkus.kafka.devservices.topic-partitions."dtrack.repo-meta-analysis.result"=3

# Defines the path to the secret key to be used for data encryption and decryption.
#
# @category: General
# @default:  ~/.dependency-track/keys/secret.key
# @type:     string
# secret.key.path=

# @category: General
# @hidden
%test.secret.key.path=src/test/resources/secret.key

# @category: Database
# @hidden
quarkus.datasource.db-kind=postgresql

# Always use quotes for keywords, column- and table names.
# e.g. SELECT "FOO"."BAR" FROM "BAZ". This matches what the API server does,
# and is required for compatibility with its schema.
#
# @category: Database
# @hidden
quarkus.hibernate-orm.quote-identifiers.strategy=all

# Hibernate should only validate that the existing schema matches our entity classes,
# but it should never generate a schema by itself.
#
# @category: Database
# @hidden
quarkus.hibernate-orm.database.generation=validate

# Specifies the JDBC URL to use when connecting to the database.
#
# @category: Database
# @type:     string
# @required
# quarkus.datasource.jdbc.url=

# Specifies the username to use when authenticating to the database.
#
# @category: Database
# @type:     string
# @required
# quarkus.datasource.username=

# Specifies the password to use when authenticating to the database.
#
# @category: Database
# @type:     string
# @required
# quarkus.datasource.password=

# Use external Postgres DB for dev mode (./mvnw quarkus:dev), but let Quarkus
# take care of test container creation in the test profile.
# See https://quarkus.io/guides/databases-dev-services
#
# @category: Database
# @hidden
%dev.quarkus.datasource.username=dtrack

# @category: Database
# @hidden
%dev.quarkus.datasource.password=dtrack

# @category: Database
# @hidden
%dev.quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/dtrack

# @category: Database
# @hidden
quarkus.datasource.devservices.image-name=postgres:13-alpine

# @category: Database
# @hidden
quarkus.datasource.devservices.init-script-path=schema.sql

# @category: Database
# @hidden
quarkus.hibernate-orm.active=true

# Defines whether caching of analysis results shall be enabled.
#
# @category: Cache
# @type:     boolean
# @required
quarkus.cache.enabled=true

# @category: Cache
# @hidden
quarkus.cache.type=caffeine

# Defines the time-to-live of cache entries.
#
# @category: Cache
# @type:     duration
# @required
quarkus.cache.caffeine."metaAnalyzer".expire-after-write=PT2H

# @category: Cache
# @hidden
quarkus.cache.caffeine."metaAnalyzer".metrics-enabled=true

# Defines the initial capacity of the cache.
#
# @category: Cache
# @type:     integer
# @required
quarkus.cache.caffeine."metaAnalyzer".initial-capacity=5

# @category: General
# @hidden
quarkus.container-image.registry=ghcr.io

# @category: General
# @hidden
quarkus.container-image.group=dependencytrack
