{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hyades","text":""},{"location":"#what-is-this","title":"What is this? \ud83e\udd14","text":"<p>Hyades, named after the star cluster closest to earth,  decouples responsibilities from Dependency-Track's monolithic API server into separate,  scalable\u2122 services. We're using Kafka (or Kafka-compatible brokers like Redpanda) for communicating between API  server and Hyades services.</p> <p>If you're interested in the technical background of this project, please refer to \ud83d\udc49 <code>WTF.md</code> \ud83d\udc48.</p> <p>As of now, Hyades is capable of:</p> <ul> <li>Performing vulnerability analysis using scanners that leverage:</li> <li>Dependency-Track's internal vulnerability database</li> <li>OSS Index</li> <li>Snyk</li> <li>Gathering component metadata (e.g. latest available version) from remote repositories</li> <li>Sending notifications via all channels supported by the original API server (E-Mail, Webhook, etc.)</li> </ul> <p>Here's a rough overview of the architecture:</p> <p></p> <p>To read more about the individual services, refer to their respective <code>REAMDE.md</code>:</p> <ul> <li>Repository Metadata Analyzer</li> <li>Vulnerability Analyzer</li> </ul>"},{"location":"#great-can-i-try-it","title":"Great, can I try it? \ud83d\ude4c","text":"<p>Yes! We prepared demo setup that you can use to play around with Hyades. Check out \ud83d\udc49 <code>DEMO.md</code> \ud83d\udc48 for details!</p>"},{"location":"#technical-documentation","title":"Technical Documentation \ud83d\udcbb","text":""},{"location":"#configuration","title":"Configuration \ud83d\udcdd","text":"<p>See <code>CONFIGURATION.md</code>.</p>"},{"location":"#development","title":"Development","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>JDK 17+</li> <li>Docker</li> </ul>"},{"location":"#building","title":"Building","text":"<pre><code>mvn clean install -DskipTests\n</code></pre>"},{"location":"#running-locally","title":"Running locally","text":"<p>Running the Hyades services locally requires both a Kafka broker and a database server to be present. Containers for Redpanda and PostgreSQL can be launched using Docker Compose:</p> <pre><code>docker compose up -d\n</code></pre> <p>To launch individual services execute the <code>quarkus:dev</code> Maven goal for the respective module:</p> <pre><code>mvn -pl vulnerability-analyzer quarkus:dev\n</code></pre> <p>Make sure you've built the project at least once, otherwise the above command will fail.</p> <p>Note If you're unfamiliar with Quarkus' Dev Mode, you can read more about it  here</p>"},{"location":"#testing","title":"Testing \ud83e\udd1e","text":""},{"location":"#unit-testing","title":"Unit Testing \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"<p>To execute the unit tests for all Hyades modules:</p> <pre><code>mvn clean verify\n</code></pre>"},{"location":"#end-to-end-testing","title":"End-To-End Testing \ud83e\udddf","text":"<p>Note End-to-end tests are based on container images. The tags of those images are currently hardcoded. For the Hyades services, the tags are set to <code>latest</code>. If you want to test local changes, you'll have to first: * Build container images locally * Update the tags in <code>AbstractE2ET</code></p> <p>To execute end-to-end tests as part of the build:</p> <pre><code>mvn clean verify -Pe2e-all\n</code></pre> <p>To execute only the end-to-end tests:</p> <pre><code>mvn -pl e2e clean verify -Pe2e-all\n</code></pre>"},{"location":"#load-testing","title":"Load Testing \ud83d\ude80","text":"<p>See <code>load-tests</code>.</p>"},{"location":"#deployment","title":"Deployment \ud83d\udea2","text":"<p>The recommended way to deploy Hyades is via Helm. Our chart is not officially published to any repository yet, so for now you'll have to clone this repository to access it.</p> <p>The chart does not include:</p> <ul> <li>a database</li> <li>a Kafka-compatible broker</li> <li>the API server</li> <li>the frontend</li> </ul> <p>While API server and frontend will eventually be included, database and Kafka broker will not.</p> <p>Helm charts to deploy Kafka brokers to Kubernetes are provided by both Strimzi  and Redpanda. </p>"},{"location":"#minikube","title":"Minikube","text":"<p>Deploying to a local Minikube cluster is a great way to get started.</p> <p>Note For now, services not included in the Helm chart are deployed using Docker Compose.</p> <ol> <li>Start PostgreSQL and Redpanda via Docker Compose <pre><code>docker compose up -d\n</code></pre></li> <li>Start the API server and frontend <pre><code>docker compose up -d apiserver frontend\n</code></pre></li> <li>Start a local Minikube cluster <pre><code>minikube start\n</code></pre></li> <li>Deploy Hyades <pre><code>helm install hyades ./helm-charts/ \\\n  -n hyades --create-namespace \\\n  -f ./helm-charts/hyades/values.yaml \\\n  -f ./helm-charts/hyades/values-minikube.yaml\n</code></pre></li> </ol>"},{"location":"#monitoring","title":"Monitoring \ud83d\udcca","text":""},{"location":"#metrics","title":"Metrics","text":"<p>A basic metrics monitoring stack is provided, consisting of Prometheus and Grafana. To start both services, run:</p> <pre><code>docker compose --profile monitoring up -d\n</code></pre> <p>The services will be available locally at the following locations:</p> <ul> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000</li> </ul> <p>Prometheus is configured to scrape metrics from the following services in a 5s intervals:</p> <ul> <li>Redpanda Broker</li> <li>API Server</li> <li>Notification Publisher</li> <li>Repository Meta Analyzer</li> <li>Vulnerability Analyzer</li> </ul> <p>The Grafana instance will be automatically provisioned to use Prometheus as data source. Additionally, dashboards for the following services are automatically set up:</p> <ul> <li>Redpanda Broker</li> <li>API Server</li> <li>Vulnerability Analyzer</li> </ul>"},{"location":"#redpanda-console","title":"Redpanda Console \ud83d\udc3c","text":"<p>The provided <code>docker-compose.yml</code> includes an instance of Redpanda Console to aid with gaining insight into what's happening in the message broker. Among many other things, it can be used to inspect messages inside any given topic.</p> <p></p> <p>The console is exposed at <code>http://127.0.0.1:28080</code> and does not require authentication. It's intended for local use only.</p>"},{"location":"DEMO/","title":"Demo","text":""},{"location":"DEMO/#setup","title":"Setup \ud83d\udcbb","text":"<p>All you need is Docker, Docker Compose and a somewhat capable machine. A UNIX-based system is strongly recommended. In case you're bound to Windows, please use WSL.</p> <p>Note A &gt;4 core CPU and &gt;=16GB RAM are recommended for a smooth experience.</p> <ol> <li>In a terminal, clone this repository and navigate to it: <pre><code>git clone https://github.com/DependencyTrack/hyades.git\ncd hyades\n</code></pre></li> <li>Generate a secret key for encryption and decryption of credentials in the database: <pre><code>openssl rand 32 &gt; secret.key\n</code></pre></li> <li>Pull and start all containers: <pre><code>docker compose --profile demo up -d --pull always\n</code></pre></li> <li>Make sure you include the <code>--profile demo</code> flag!</li> </ol> <p>Once completed, the following services will be available:</p> Service URL API Server http://localhost:8080 Frontend http://localhost:8081 Redpanda Console http://localhost:28080 PostgreSQL <code>localhost:5432</code> Redpanda Kafka API <code>localhost:9092</code> <p>Note You'll not need to interact with PostgreSQL or the Kafka API directly to try out the project, but if you're curious \ud83d\udd75\ufe0f of course you can!</p> <p>Finally, to remove everything again, including persistent volumes:</p> <pre><code>docker compose --profile demo down --volumes\n</code></pre>"},{"location":"DEMO/#common-issues","title":"Common Issues","text":""},{"location":"DEMO/#postgres-container-fails-to-start","title":"Postgres container fails to start","text":"<p>If the <code>dt-postgres</code> container fails to start with messages like:</p> <pre><code>ls: can't open '/docker-entrypoint-initdb.d/': Permission denied\n</code></pre> <p>It's likely that the local directory mounted into <code>/docker-entrypoint-initdb.d</code> is not accessible by the postgres process. To fix, make the local directory readable by everyone, and restart the <code>dt-postgres</code> container:</p> <pre><code>chmod -R o+r ./commons/src/main/resources/migrations/postgres\ndocker restart dt-postgres\n</code></pre>"},{"location":"DEMO/#testing","title":"Testing \ud83e\udd1e","text":"<ol> <li>In a web browser, navigate to http://localhost:8081 and login (username: <code>admin</code>, password: <code>admin</code>)</li> <li>Navigate to the Notifications section in the administration panel</li> <li>Create a new alert with publisher Outbound Webhook </li> <li>Select a few notification groups and enter a destination URL (Pipedream is convenient for testing Webhooks)    </li> <li>Navigate to the projects view and click Create Project</li> <li>Provide an arbitrary project name and click Create</li> <li>Select the project you just created from the project list</li> <li>Navigate to the Components tab and click Upload BOM</li> <li>Upload any (S)BOM you like. If you don't have one handy, here are some to try:<ul> <li>Dependency-Track API Server 4.6.2</li> <li>Dependency-Track Frontend 4.6.1</li> <li>CycloneDX SBOM examples</li> </ul> </li> <li>Now navigate to the Audit Vulnerabilities tab and hit the \ud83d\udd04 button to the top right of the table a few times<ul> <li>You should see the table being populated with vulnerability data</li> </ul> </li> <li>Going back to the service you used as Webhook destination, you should see that a few alerts have been delivered     </li> </ol> <p>Overall, this should behave just like what you're used to from Dependency-Track. However in this case, the publishing of notifications and vulnerability analysis was performed by external, individually scalable services.</p>"},{"location":"DEMO/#scaling-up","title":"Scaling up \ud83d\udcc8","text":"<p>Warning This section is still a work in progress and does not necessarily show the current state of the setup.</p> <p>One of the goals of this project is to achieve scalability, remember? Well, we're delighted to report that there are multiple ways to scale! If you're interested, you can find out more about the parallelism model at play here.</p> <p>Per default, when opening the Consumer Groups view in Redpanda Console, you'll see a total of two groups:</p> <p></p> <p>The Members column shows the number of stream threads in each group. Clicking on the dtrack-vuln-analyzer group will reveal a more detailed view:</p> <p></p> <p>Each stream thread got assigned 20 partitions. 20 partitions are a lot to take care of, so being limited to just three stream threads will not yield the best performance.</p>"},{"location":"DEMO/#scaling-a-single-instance","title":"Scaling a single instance \ud83d\ude80","text":"<p>Arguably the easiest option is to simply increase the number of stream threads used by a service instance. By modifying the <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> environment variable in <code>docker-compose.yml</code>, the number of worker threads can be tweaked.</p> <p>Let's change it to <code>3</code> and see what happens! To do this, remove the comment (<code>#</code>) from the <code># KAFKA_STREAMS_NUM_STREAM_THREADS: \"3\"</code> line in <code>docker-compose.yml</code>, and recreate the container with <code>docker compose up -d vulnerability-analyzer</code>.</p>"},{"location":"DEMO/#scaling-to-multiple-instances","title":"Scaling to multiple instances \ud83d\ude80\ud83d\ude80\ud83d\ude80","text":"<p>Putting more load on a single service instance is not always desirable, so oftentimes simply increasing the replica count is the preferable route. In reality this may be done via Kubernetes manifests, but we can do it in Docker Compose, too. Let's scale up to three instances:</p> <pre><code>docker compose --profile demo up -d --scale vulnerability-analyzer=3\n</code></pre>"},{"location":"about/","title":"About","text":""},{"location":"about/#why","title":"Why?","text":"<p>tl;dr: Dependency-Track's architecture prevents it from scaling past a certain workload.</p> <p>Dependency-Track, for the most part, is an event-based system. As a platform for ingesting data (in the form of BOMs), listening for and itself emitting signals on it, an event-based architecture makes sense conceptually. The majority of operations Dependency-Track performs happen asynchronously, without client interaction.</p> <p>On a technical level, Dependency-Track uses an in-memory publish-subscribe architecture, implemented using Java <code>ExecutorService</code>s. An <code>ExecutorService</code> can be thought of as a pool of worker threads, consuming from an internal task queue. Tasks can be submitted to an <code>ExecutorService</code>, which will then execute them one-by-one. As multiple threads work on the queue in  parallel, the order in which tasks are being processed is not guaranteed. Thread pool sizes can vary from one,  up to unbounded numbers of threads.</p> <p>In Dependency-Track, when an event is published, subscribers to the event are looked up.  Per API contract,  event subscribers must implement an <code>inform</code> method, which takes the published event as argument. For any given event, 0-N tasks will be enqueued to the <code>ExecutorService</code>'s task queue - one for each subscriber.</p> <p></p> <p>There are three <code>ExecutorService</code> instances in Dependency-Track:</p> <ul> <li><code>EventService</code></li> <li><code>SingleThreadedEventService</code></li> <li><code>NotificationService</code></li> </ul> <p><code>EventService</code> forms the primary worker pool. Its thread pool size defaults to <code>&lt;NUM_CPU&gt; * 4</code>.  A machine with a 4-core CPU will thus have a thread pool size of <code>16</code>. The size is configurable. Common tasks handled by this worker pool include:</p> <ul> <li>Processing of uploaded BOMs and VEXs</li> <li>Performing vulnerability analysis of components, projects, or the entire portfolio</li> <li>Performing repository meta analysis of components, projects, or the entire portfolio</li> <li>Calculation of metrics for components, projects, or the entire portfolio</li> </ul> <p><code>SingleThreadedEventService</code> is a worker pool with only a single thread. The purpose of this worker pool is execute tasks that must not be run in parallel. As such, it serves as a means to serialize task execution.  Common tasks handled by this worker pool include:</p> <ul> <li>Mirroring of the NVD vulnerability database</li> <li>Updating Lucene indexes on disk</li> </ul> <p><code>NotificationService</code> is a dedicated worker pool for dispatching notifications. Its thread pool size defaults to <code>4</code> and is not configurable.</p>"},{"location":"about/#limitations","title":"Limitations","text":"<p>While this architecture works great for small to medium workloads, it presents various challenges for larger ones:</p> <ol> <li>Not horizontally scalable. As pub-sub is happening entirely in-memory, it is not possible to distribute the work to multiple application instances. The only way to handle more load using this architecture is to scale vertically, e.g.</li> <li>Increasing <code>ExecutorService</code> thread pool sizes (<code>alpine.worker.threads</code>, <code>alpine.worker.thread.multiplier</code>)</li> <li>Increasing database connection pool sizes</li> <li>Increasing resource allocations for CPU, RAM, and potentially disk / network</li> <li>No ordering guarantees of events. As multiple threads work on a shared queue of tasks in parallel, there is no way of enforcing a reliable ordering of events. </li> <li>Limited fault-tolerance. If an instance of Dependency-Track goes down, planned or unplanned, all queued tasks are  gone. Not only does this impact business-as-usual operation, but also limits the times when upgrades can be applied.</li> <li>Shared, multipurpose task queue. A single task queue is used to process all kinds of events. This means that lots of events of a certain type can \"clog\" the task queue, preventing other types of events from being processed. This is further amplified if processing of events \"clogging\" the queue relies on external services, introducing further latency. Ideally, there should be a dedicated queue per event type, so that one busy queue doesn't block others.</li> <li>Prone to race conditions. As a consequence of (2), it is possible that multiple events addressing the same thing are processed in parallel, leading to race conditions in cache lookups or database operations. Race conditions would be an even bigger problem if the work was shared across multiple application instances, and would require distributed locking as a countermeasure, which is inherently hard to get right.</li> </ol> <p>In order to scale Dependency-Track beyond its current capabilities, a distributed messaging service is required.</p>"},{"location":"about/#related-issues","title":"Related Issues","text":"<p>On multiple occasions in the past, the Dependency-Track community raised questions about high availability (HA) deployments, and / or how to better scale the platform:</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/issues/218</li> <li>https://github.com/DependencyTrack/dependency-track/issues/903</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1210</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1856</li> </ul> <p>Note The work we've done so far does not make the API server highly available. However, it does address a substantial chunk of work that is required to make that happen.</p>"},{"location":"about/#why-kafka","title":"Why Kafka?","text":"<p>Kafka was chosen because it employs various concepts  that are advantageous for Dependency-Track:</p> <ul> <li>It supports publish-subscribe use cases based on topics and partitions</li> <li>Events with the same key are guaranteed to be sent to the same partition</li> <li>Order of events is guaranteed on the partition level</li> <li>Consumers can share the load of consuming from a topic by forming consumer groups<ul> <li>Minor drawback: maximum concurrency is bound to the number of partitions</li> </ul> </li> <li>It is distributed and fault-tolerant by design, replication is built-in</li> <li>Events are stored durably on the brokers, with various options to control retention</li> <li>Log compaction allows for fault-tolerant, stateful processing,   by streaming changes of a local key-value database to Kafka</li> <li>In certain cases, this can aid in reducing load on the database server</li> <li>Mature ecosystem around it, including a vast landscape of client libraries for various languages</li> <li>Kafka Streams with its support for     stateful transformations     in particular turned out to be a unique selling point for the Kafka ecosystem</li> <li>Mature cloud offerings for fully managed instances (see Options for running Kafka)</li> </ul> <p>The concept of partitioned topics turned out to be especially useful: We can rely on the fact that events with the same key always end up in the same partition, and are processed by only one consumer (within a consumer group) at a time. In case of vulnerability scanning, by choosing the component's PURL as event key, it can be guaranteed that only the first event triggers an HTTP request to OSS Index, while later events can be handled immediately from cache. There is no race condition anymore between lookup and population of the cache.</p> <p>We also found the first-class support for stateful processing incredibly useful in some cases, e.g.:</p> <ul> <li>Scatter-gather.    As used for scanning one component with multiple analyzers. Waiting for all analyzers to complete is a stateful   operation, that otherwise would require database access.</li> <li>Batching. Some external services allow for submitting multiple component identifiers per request.   With OSS Index, up to 128 package URLs can be sent in a single request. Submitting only one package URL at a   time would drastically increase end-to-end latency. It'd also present a greater risk of getting rate limited.</li> </ul> <p>That being said, Kafka does add a considerable amount of operational complexity. Historically, Kafka has depended on Apache ZooKeeper. Operating both Kafka and ZooKeeper is not something we wanted  to force DT users to do. Luckily, the Apache Kafka project has been working on removing the ZooKeeper dependency,  and replacing it with Kafka's own raft consensus protocol (KRaft).</p> <p>There are other, more light-weight, yet Kafka API-compatible broker implementations available, too.  Redpanda being the most popular. Redpanda is distributed in a single, self-contained binary and is optimal for deployments with limited resources. Having options like Redpanda available makes building a system on Kafka much more viable.</p> <p>For this reason in fact, we primarily develop with, and test against, Redpanda.</p>"},{"location":"about/#considered-alternatives","title":"Considered alternatives","text":"<p>Before choosing for Kafka, we looked at various other messaging systems.</p>"},{"location":"about/#apache-pulsar","title":"Apache Pulsar","text":"<p>Among all options, Pulsar was the most promising besides Kafka. Pulsar prides itself in  being truly cloud native, supporting tiered storage, and multiple messaging paradigms (pub-sub and queueing).  It has native support for negative acknowledgment  of messages, and message retries.  However, the Pulsar architecture consists not  only of Pulsar brokers, but also requires Apache ZooKeeper and Apache BookKeeper  clusters. We had to dismiss Pulsar for its operational complexity.</p>"},{"location":"about/#rabbitmq","title":"RabbitMQ","text":"<p>RabbitMQ is a popular message broker. It exceeds in cases where multiple worker processes need to work on a shared queue of tasks (similar to how Dependency-Track does it today). It can achieve a high level of concurrency, as there's no limit to how many consumers can consume from a queue. This high grade of concurrency comes with the cost of lost ordering, and high potential for race conditions. </p> <p>RabbitMQ supports Kafka-like partitioned streams via its Streams plugin. In the end, we decided against RabbitMQ, because brokers do not support key-based compaction, and its consumer libraries  in turn lack adequate support for fault-tolerant stateful operations.</p>"},{"location":"about/#liftbridge","title":"Liftbridge","text":"<p>Liftbridge is built on top of NATS and provides Kafka-like features. It is however not compatible with the Kafka API, as it uses a custom envelope protocol, and is heavily focused on Go. There are no managed service offerings for Liftbridge, leaving self-hosting as only option to run it.</p>"},{"location":"about/#options-for-running-kafka","title":"Options for running Kafka","text":"<p>When it comes to running Kafka in production, users will have the choice between various self-hosted and fully managed  Infrastructure as a Service (IaaS) solutions. The following table lists a few, but there will be more we don't know of:</p> Solution Type URL Apache Kafka Self-Hosted https://kafka.apache.org/quickstart Redpanda Self-Hosted / IaaS https://redpanda.com/ Strimzi Self-Hosted https://strimzi.io/ Aiven IaaS https://aiven.io/kafka AWS MSK IaaS https://aws.amazon.com/msk/ Azure Event Hubs IaaS https://azure.microsoft.com/en-us/products/event-hubs/ Confluent Cloud IaaS https://www.confluent.io/ Red Hat OpenShift Streams IaaS https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview <p>The wide range of mature IaaS offerings is a very important benefit of Kafka over other messaging systems we evaluated.</p>"},{"location":"about/#why-java","title":"Why Java?","text":"<p>We went with Java for now because it was the path of the least resistance for us. There is no intention to exclusively  use Java though. We are considering to use Go, and generally  are open to any technology that makes sense.</p>"},{"location":"about/#why-not-microservices","title":"Why not microservices?","text":"<p>The proposed architecture is based on the rough idea of domain services for now. This keeps the number of independent services manageable, while still allowing us to distribute the overall system load. If absolutely necessary, it is  possible to break this up even further. For example, instead of having one vulnerability-analyzer service,  the scanners for each vulnerability source (e.g. OSS Index, Snyk) could be separated out into dedicated microservices.</p>"},{"location":"architecture/design/workflow-state-tracking/","title":"Tracking of Workflow State for BOM Processing and Analysis","text":"<p>Note This document was extracted from #664.</p> <p>For CI/CD use-cases, Dependency-Track offers a mechanism that allows clients to poll whether the BOM they just uploaded is still being processed. \"Processing\" in this context refers to:</p> <ul> <li>Consumption (Parsing)</li> <li>Ingestion (Sync parsed data with database)</li> <li>Vulnerability analysis</li> <li>Policy Evaluation</li> </ul> <p>This is an important capability, allowing for implementation of quality gates and reporting in CI/CD pipelines.</p> <p>The mechanism works by assigning identifiers (UUIDs) to events in Alpine's in-memory event system. As long as an event associated to a given identifier can be found in its internal queue, the identifier is considered to be \"processing\":</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/blob/6153d286d1ac806462bc76cfe17d84a57c224671/src/main/java/org/dependencytrack/resources/v1/BomResource.java#L323-L342</li> <li>https://github.com/stevespringett/Alpine/blob/cd6aa7ed05376935ab32bc43819eba0e3a525b7f/alpine-infra/src/main/java/alpine/event/framework/BaseEventService.java#L158-L161</li> </ul> <p>Events can be chained, such that a <code>BomUploadEvent</code> will trigger a <code>VulnerabilityAnalysisEvent</code>, which will trigger a <code>PolicyEvaluationEvent</code>, and so on. The event identifier is inherited by chained events.</p> <p>As everything happens in-memory, this mechanism does not work when multiple instances of the API server are used in an active-active deployment. For the functionality to continue to work, the state of processing steps must be persistent to external storage.</p> <p>Decoupled from #633.</p>"},{"location":"architecture/design/workflow-state-tracking/#design","title":"Design","text":"<p>Note The goal for now is not to build a multi-purpose workflow engine, but to track state of one specific workflow. In a future iteration, we may invest more time into coming up with a generic workflow engine concept.</p>"},{"location":"architecture/design/workflow-state-tracking/#what-to-track","title":"What to track","text":"<ol> <li>BOM consumption (parsing, de-duplication, etc.)</li> <li>BOM processing (ingestion into database)</li> <li>Vulnerability analysis</li> <li>Repository metadata analysis</li> <li>Policy evaluation</li> <li>Metrics update</li> </ol> <p>The order of execution as of today is as follows:</p> <pre><code>flowchart LR\n    A(BOM_CONSUMPTION) --&gt; B(BOM_PROCESSING)\n    B --&gt; C(VULN_ANALYSIS)\n    B --&gt; D(REPO_META_ANALYSIS)\n    C --&gt; E(POLICY_EVALUATION)\n    E --&gt; F(METRICS_UPDATE)\n    B -. BOM contains no &lt;br/&gt;components to analyze .-&gt; F\n</code></pre> <p>Note Completion of repository metadata analysis can currently not be tracked. We'll need something similar to what we introduced in https://github.com/DependencyTrack/hyades-apiserver/pull/40 for vulnerability analysis completion tracking. For the initial implementation, it may be OK to not track it.</p>"},{"location":"architecture/design/workflow-state-tracking/#states","title":"States","text":"<p>There are multiple states a processing step can be in:</p> <ul> <li><code>PENDING</code>: Initial state</li> <li><code>COMPLETED</code>: Completion detected; No failures</li> <li><code>FAILED</code>: Completion detected; Failures</li> <li><code>CANCELLED</code>: Never started because a precondition failed<ul> <li>e.g. vulnerability analysis will be cancelled when BOM ingestion failed</li> </ul> </li> <li><code>NOT_APPLICABLE</code>: The step is not applicable to the subject of the analysis<ul> <li>e.g. the analysis was triggered for an existing project, without BOM upload</li> </ul> </li> <li><code>TIMED_OUT</code>: The deadline for the step completion was exceeded; Step is unlikely to complete</li> </ul> <pre><code>stateDiagram-v2\n    [*] --&gt; PENDING\n    PENDING --&gt; COMPLETED\n    COMPLETED --&gt; [*]\n    PENDING --&gt; FAILED\n    FAILED --&gt; [*]\n    PENDING --&gt; CANCELLED\n    CANCELLED --&gt; [*]\n    [*] --&gt; NOT_APPLICABLE\n    NOT_APPLICABLE --&gt; [*]\n    PENDING --&gt; TIMED_OUT\n    TIMED_OUT --&gt; COMPLETED\n    TIMED_OUT --&gt; FAILED\n</code></pre> <p>Each step is responsible for updating its own state.</p> <p>The overall state can be considered to be complete, if there's no step in <code>PENDING</code> state.</p> <p>When a step failure is detected, a \"failure reason\" message must be persisted. If multiple steps fail, (rough) failure details for each step must be available.</p> <p>There should be a deadline mechanism, which automatically transitions steps from <code>PENDING</code> into <code>TIMED_OUT</code> state. Steps in <code>TIMED_OUT</code> state communicate that it is unlikely that a terminal state can be reached (<code>COMPLETED</code>, <code>FAILED</code>). However, it is still possible (e.g. due to significant consumer lag, events may arrive late).</p>"},{"location":"architecture/design/workflow-state-tracking/#workflow","title":"Workflow","text":"<p>The API server will act as workflow orchestrator, kicking off new steps as needed. This implies that completion of a step must be registered by the API server.</p> <p>To illustrate how the initial stage of the workflow should look like:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ BomResource: Upload BOM\n    BomResource -&gt;&gt; BomResource: Validate BOM\n    BomResource -&gt;&gt; BomResource: Generate Correlation ID (UUID)\n    BomResource -&gt;&gt; Database: Create workflow steps\n    Note over BomResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=BOM_CONSUMPTION&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=BOM_PROCESSING&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Status=PENDING\n    BomResource -&gt;&gt; BomUploadProcessingTask: Dispatch BomUploadEvent\n    BomResource -&gt;&gt;- Client: Correlation ID\n    par\n        loop Continuously\n            Client -&gt;&gt; BomResource: Poll Status\n            Note over Client, BomResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n        end\n    and\n        BomUploadProcessingTask -&gt;&gt; Database: Update step start time\n        activate BomUploadProcessingTask\n        Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;StartedAt=&lt;NOW&gt;\n        BomUploadProcessingTask -&gt;&gt; BomUploadProcessingTask: Consume BOM&lt;br/&gt;(Parse, De-dupe)\n        alt Consumption succeeded\n            BomUploadProcessingTask -&gt;&gt; Database: Update step status\n            Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Process BOM&lt;br/&gt;(Insert, Update, Delete)\n            alt Processing succeeded\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.repo-meta-analysis.component\" topic\n            else Processing failed\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=VULN_ANALYSIS&lt;br/&gt;Status=CANCELLED\n            end\n        else Consumption failed\n            BomUploadProcessingTask -&gt;&gt; Database: Update step Status\n            Note over BomUploadProcessingTask, Database: Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=CANCELLED\n        end\n        deactivate BomUploadProcessingTask\n    end\n</code></pre> <p>When triggering a re-analysis of an already existing project, the workflow may be kicked off as follows:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ FindingResource: Re-Analyze project\n    FindingResource -&gt;&gt; FindingResource: Generate Correlation ID (UUID)\n    FindingResource -&gt;&gt; Database: Create workflow steps\n    Note over FindingResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=POLICY_EVALUATION&lt;br/&gt;Status=PENDING\n    FindingResource -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n    FindingResource -&gt;&gt;- Client: Correlation ID\n    loop Continuously\n        Client -&gt;&gt; FindingResource: Poll Status\n        Note over Client, FindingResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n    end\n</code></pre>"},{"location":"architecture/design/workflow-state-tracking/#proposed-table-schema","title":"Proposed Table Schema","text":"<p>Each step of the workflow will be represented in a dedicated row. This allows us to add or remove steps without altering the database schema (see original version of the schema further down below), or even add steps while the workflow is running. It also plays better with concurrent writes, as no two threads / instances will need to modify the same row.</p> Name Type Nullable Example ID <code>SERIAL</code> \u274c 1 PARENT_STEP_ID <code>SERIAL FK</code> \u2705 0 TOKEN <code>VARCHAR(36)</code> \u274c <code>484d9eaa-7ea4-4476-97d6-f36327b5a626</code> STARTED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> UPDATED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> STEP <code>VARCHAR(64)</code> \u274c <code>METRICS_UPDATE</code> STATUS <code>VARCHAR(64)</code> \u274c <code>PENDING</code> FAILURE_REASON <code>TEXT</code> \u2705 <code>Failed to acquire database connection</code> <p>Potential Future Improvements: * Do we need/want to capture the order in which steps are supposed to be executed? * Do we need/want to capture metadata of the overall workflow (who triggered it, when was it triggered, correlation id, ...)?</p> Original Version  | Name | Type | Nullable | Example | | :--- | :--- | :---: | :--- | | TOKEN | `VARCHAR(36)` | \u274c | `484d9eaa-7ea4-4476-97d6-f36327b5a626` | | STARTED_AT | `TIMESTAMP` | \u274c | `1999-01-08 04:05:06` | | UPDATED_AT | `TIMESTAMP` | \u274c | `1999-01-08 04:05:06` | | BOM_CONSUMPTION | `VARCHAR(64)` | \u274c | `PENDING` | | BOM_PROCESSING | `VARCHAR(64)` | \u274c | `PENDING` | | VULN_ANALYSIS | `VARCHAR(64)` | \u274c | `PENDING` | | REPO_META_ANALYSIS | `VARCHAR(64)` | \u274c | `PENDING` | | POLICY_EVALUATION | `VARCHAR(64)` | \u274c | `PENDING` | | METRICS_UPDATE | `VARCHAR(64)` | \u274c | `PENDING` | | FAILURE_REASON | `TEXT` | \u2705 | - |  `FAILURE_REASON` is a field of unlimited length. It either holds no value (`NULL`), or a JSON object listing failure details per step, e.g.:  <pre><code>{\n  \"BOM_PROCESSING\": \"Failed to acquire database connection\"\n}\n</code></pre> <p>Where applicable, the \"detailed\" status of a step is tracked in a dedicated table.</p>"},{"location":"architecture/design/workflow-state-tracking/#retention","title":"Retention","text":"<p>Rows in the table should be cleaned up on a recurring basis.</p> <p>This could be as simple as scheduling a job that executes this SQL query:</p> <pre><code>DELETE FROM org.dependencytrack.repometaanalyzer.model.WorkflowStep WHERE this.updatedAt &lt; :threshold\n</code></pre> <p>A retention time of 1-3 days since the last update should be reasonable.</p>"},{"location":"architecture/design/workflow-state-tracking/#rest-api-endpoints","title":"REST API endpoints","text":"<p>The existing endpoint to check whether a BOM is still being processed should continue to work as expected:</p> <p><pre><code>GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;\n</code></pre> <pre><code>{\n  \"processing\": true\n}\n</code></pre></p> <p>An additional endpoint may be added, which allows for retrieval of the individual step states:</p> <p><pre><code>GET /api/v1/workflow/token/&lt;CORRELATION_ID&gt;/status\n</code></pre> <pre><code>[\n  {\n    \"step\": \"BOM_CONSUMPTION\",\n    \"status\": \"COMPLETED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\"\n  },\n  {\n    \"step\": \"BOM_PROCESSING\",\n    \"status\": \"FAILED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\",\n    \"failureReason\": \"Failed to acquire database connection\"\n  },\n  {\n    \"step\": \"VULN_ANALYSIS\",\n    \"status\": \"CANCELLED\"\n  }\n]\n</code></pre></p> <p>If all a client cares about are vulnerability analysis results, they could stop polling immediately after <code>vulnerabilityAnalysis</code> transitions into the <code>COMPLETED</code> state.</p>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>All available configuration options used by all applications are listed in their respective <code>application.properties</code>. Options can be provided via environment variables as well, refer to the Quarkus docs for details.</p> <p>Not all options are supposed to be tweaked by users though. This document contains an overview of all options that are expected to be changed by users.</p>"},{"location":"reference/configuration/#kafka-topic-configuration","title":"Kafka Topic Configuration","text":"<p>Kafka topics (including internal topics) can be configured with custom prefix. In order to provide custom prefix, below environment variable can be used.</p> Environment Variable Description Default Required <code>API_TOPIC_PREFIX</code> Prefix for topic names - \u274c"},{"location":"reference/configuration/#notification-publisher","title":"Notification Publisher","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>PARALLEL_CONSUMER_MAX_CONCURRENCY</code> Number of threads to process notifications with <code>6</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_INITIAL_DELAY</code> Initial delay before retrying notification delivery <code>3S</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_MULTIPLIER</code> Multiplier for retry delays <code>0.3</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_RANDOMIZATION_FACTOR</code> Randomization factory for jitter in retry delays <code>0.3</code> \u274c <code>PARALLEL_CONSUMER_RETRY_MAX_DURATION</code> Maximum duration of delays between retry attempts <code>2M</code> \u2705 <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <code>QUARKUS_MAILER_FROM</code> The sender name for email notifications - When email notifications are enabled <code>QUARKUS_MAILER_HOST</code> Address of the mail server for email notifications - When email notifications are enabled <code>QUARKUS_MAILER_PORT</code> Port of the mail server for email notifications - When email notifications are enabled <code>QUARKUS_MAILER_SSL</code> Use SSL / TLS to communicate with the email server <code>false</code> - <code>QUARKUS_MAILER_START_TLS</code> Use StartTLS to communicate with the email server <code>DISABLED</code> When email notifications are enabled <code>QUARKUS_MAILER_USERNAME</code> Username to authenticate with the email server - When email notifications are enabled <code>QUARKUS_MAILER_PASSWORD</code> Password to authenticate with the email server - When email notifications are enabled <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/#repository-meta-analyzer","title":"Repository Meta Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/#vulnerability-analyzer","title":"Vulnerability Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>STATE_STORE_TYPE</code> Whether to use in-memory or persistent (RocksDB) Kafka Streams state stores <code>in_memory</code> \u2705 <code>STATE_STORE_ROCKS_DB_COMPACTION_STYLE</code> Compaction style to use for RocksDB state stores - \u274c <code>STATE_STORE_ROCKS_DB_COMPRESSION_TYPE</code> Compression type to use for RocksDB state stores - \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <code>SCANNER_INTERNAL_ENABLED</code> Enable the internal vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_ENABLED</code> Enable the OSS Index vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by OssIndex analyzer. This depends on standard and custom supported package urls <code>cargo,composer,gem,hex,maven,npm,nuget,pypi,rpm,conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by ossindex. These are not part of the standard package urls list <code>conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_API_USERNAME</code> OSS Index API username - \u274c <code>SCANNER_OSSINDEX_API_TOKEN</code> OSS Index API token - \u274c <code>SCANNER_OSSINDEX_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> \u274c <code>SCANNER_OSSINDEX_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSS Index <code>false</code> \u274c <code>SCANNER_SNYK_ENABLED</code> Enable the Snyk vulnerability scanner <code>false</code> \u274c <code>SCANNER_SNYK_API_ORG_ID</code> Snyk organization ID - When Snyk is enabled <code>SCANNER_SNYK_API_TOKENS</code> Comma-separated list of Snyk API tokens - When Snyk is enabled <code>SCANNER_SNYK_API_VERSION</code> Version of the Snyk API to use <code>2022-12-15</code> When Snyk is enabled <code>SCANNER_SNYK_SEVERITY_SOURCE_PRIORITY</code> Priority of preferred source for vulnerability severities <code>nvd,snyk,redhat,suse</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_SIZE</code> Max size of batch at which it will be submitted <code>100</code> When Snyk is enabled <code>SCANNER_SNYK_ALIAS_SYNC_ENABLED</code> Enable alias syncing for Snyk <code>false</code> \u274c <code>SCANNER_SNYK_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by Snyk analyzer. This depends on supported standard and custom package urls <code>cargo,cocoapods,composer,gem,generic,hex,maven,npm,nuget,pypi,swift,golang</code> \u274c <code>SCANNER_SNYK_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by Snyk. These are not part of the standard package urls list <code>cocoapods,apk,swift</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/#mirror-service","title":"Mirror Service","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>MIRROR_DATASOURCE_GITHUB_ALIAS_SYNC_ENABLED</code> Enable alias syncing for GitHub Advisories <code>false</code> \u274c <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSV <code>false</code> \u274c <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/topics/","title":"Topics","text":"Name Partitions Config <code>dtrack-apiserver-processed-vuln-scan-result-by-scan-token-repartition</code><sup>1A</sup> 3 <code>dtrack.notification.analyzer</code> 3 <code>dtrack.notification.bom</code> 3 <code>dtrack.notification.configuration</code> 3 <code>dtrack.notification.datasource-mirroring</code> 3 <code>dtrack.notification.file-system</code> 3 <code>dtrack.notification.integration</code> 3 <code>dtrack.notification.new-vulnerability</code> 3 <code>dtrack.notification.new-vulnerable-dependency</code> 3 <code>dtrack.notification.policy-violation</code> 3 <code>dtrack.notification.project-audit-change</code> 3 <code>dtrack.notification.project-created</code> 3 <code>dtrack.notification.repository</code> 3 <code>dtrack.notification.vex</code> 3 <code>dtrack.notification.project-vuln-analysis-complete</code> <sup>3</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>dtrack.repo-meta-analysis.component</code><sup>1B</sup> 3 <code>dtrack.repo-meta-analysis.result</code> 3 <code>dtrack.vuln-analysis.component</code><sup>1C</sup> 3 <code>dtrack.vuln-analysis.result</code><sup>1A</sup> 3 <code>dtrack.vuln-analysis.scanner.result</code><sup>1C</sup> 3 <code>dtrack.vulnerability</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.digest</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.mirror.command</code><sup>2</sup> 1 <code>dtrack.vulnerability.mirror.state</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>hyades-repository-meta-analyzer-command-by-purl-coordinates-repartition</code><sup>1B</sup> 3 <code>hyades-vulnerability-analyzer-completed-scans-table-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-last-update-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-table-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-ossindex-batch-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-ossindex-retry-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-scan-task-internal-repartition</code> 3 <code>hyades-vulnerability-analyzer-scan-task-ossindex-repartition</code><sup>1D</sup> 3 <code>hyades-vulnerability-analyzer-scan-task-snyk-repartition</code><sup>1E</sup> 3 <code>hyades-vulnerability-analyzer-snyk-batch-store-changelog</code><sup>1E</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-snyk-retry-store-changelog</code><sup>1E</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <p><sup>1X</sup> The topic is subject to co-partitioning requirements <sup>2</sup> The partition number of this topic should not be changed <sup>3</sup> To use this notification, the PROJECT_VULN_ANALYSIS_COMPLETE group needs to be manually added through api call to the notify on condition The put request to be executed is here</p>"},{"location":"reference/topics/#co-partitioning-requirements","title":"Co-Partitioning Requirements","text":"<p>Some topics must be co-partitioned, meaning they must share the exact same number of partitions. Applications using those topics will not work correctly when this is not the case.</p>"},{"location":"usage/policy-compliance/expressions/","title":"Expressions","text":""},{"location":"usage/policy-compliance/expressions/#introduction","title":"Introduction","text":"<p>Dependency-Track allows policy conditions to be defined using the Common Expression Language (CEL), enabling more flexibility, and more control compared to predefined conditions.</p> <p>To use CEL, simply select the subject <code>Expression</code> when adding a new condition. A code editor will appear in which expressions can be provided.</p> <p></p> <p>In addition to the expression itself, it's necessary to specify a violation type, which may be any of <code>License</code>, <code>Operational</code>, or <code>Security</code>. The violation type aids in communicating what kind of risk is introduced by the condition being matched.</p>"},{"location":"usage/policy-compliance/expressions/#syntax","title":"Syntax","text":"<p>The CEL syntax is similar to other C-style languages like Java and JavaScript. However, CEL is not Turing-complete. As such, it does not support constructs like <code>if</code> statements or loops (i.e. <code>for</code>, <code>while</code>).</p> <p>As a compensation for missing loops, CEL offers macros like <code>all</code>, <code>exists</code>, <code>exists_one</code>, <code>map</code>, and <code>filter</code>. Refer to the macros documentation for more details, or have a look at the examples to see how they may be utilized in practice.</p> <p>CEL syntax is described thoroughly in the official language definition.</p>"},{"location":"usage/policy-compliance/expressions/#evaluation-context","title":"Evaluation Context","text":"<p>Conditions are scoped to individual components. Each condition is evaluated for every single component in a project.</p> <p>The context in which expressions are evaluated in contains the following variables:</p> Variable Type Description <code>component</code> <code>Component</code> The component being evaluated <code>project</code> <code>Project</code> The project the component is part of <code>vulns</code> <code>list(Vulnerability)</code> Vulnerabilities the component is affected by"},{"location":"usage/policy-compliance/expressions/#best-practices","title":"Best Practices","text":"<ol> <li>Keep expressions simple and concise. The more complex an expression becomes, the harder it gets to determine why it did or did not match. Use policy operators (<code>Any</code>, <code>All</code>) to chain multiple expressions if practical.</li> <li>Call functions last. Custom functions involve additional computation that is more expensive than simple field accesses. Performing any checks on fields first, and calling functions last, oftentimes allows evaluation to short-circuit.</li> <li>Remove conditions that are no longer needed. Dependency-Track analyzes the configured expressions to determine what data it has to load from the database in order to evaluate them. The more fields are being accessed, the more data has to be loaded. Removal of outdated conditions thus has a direct positive performance impact.</li> </ol>"},{"location":"usage/policy-compliance/expressions/#examples","title":"Examples","text":""},{"location":"usage/policy-compliance/expressions/#component-age","title":"Component age","text":"<p>Besides out-of-date versions, component age is another indicator of potential risk. Components may be on the latest available version, but still be 20 years old. </p> <p>Component age can be evaluated using the <code>compare_age</code> function. The first function argument  is a numeric comparator (<code>&lt;</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&gt;=</code>), and the second is a duration in ISO8601 notation.</p> <p>The following expression matches Components that are two years old, or even older:</p> <pre><code>component.compare_age(\"&gt;=\", \"P2Y\")\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#component-blacklist","title":"Component blacklist","text":"<p>The following expression matches on the Component's Package URL, using a regular expression in RE2 syntax. Additionally, it checks whether the Component's version falls into a given vers range, consisting of multiple constraints.</p> <pre><code>component.purl.matches(\"^pkg:maven/com.acme/acme-lib\\\\b.*\")\n  &amp;&amp; component.matches_range(\"vers:maven/&gt;0|&lt;1|!=0.2.4\")\n</code></pre> <p>The expression will match:</p> <ul> <li><code>pkg:maven/com.acme/acme-lib@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.9.9</code></li> </ul> <p>but not:</p> <ul> <li><code>pkg:maven/com.acme/acme-library@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.2.4</code></li> </ul> <p><code>matches_range</code> currently supports the following versioning schemes:</p> Versioning Scheme Ecosystem <code>deb</code> Debian / Ubuntu <code>generic</code> Generic / Any <code>golang</code> Go <code>maven</code> Java / Maven <code>npm</code> JavaScript / NodeJS <code>rpm</code> CentOS / Fedora / Red Hat / SUSE <p>Note</p> <p>If the ecosystem of the component(s) to match against is known upfront, it's good practice to use the according versioning scheme in <code>matches_range</code>. This helps with accuracy, as versioning schemes have different nuances across ecosystems, which makes comparisons error-prone.</p>"},{"location":"usage/policy-compliance/expressions/#dependency-graph-traversal","title":"Dependency graph traversal","text":"<p>The following expression matches Components that are a (possibly transitive) dependency of a Component with name <code>foo</code>, but only if a Component with name <code>bar</code> is also present in the Project.</p> <pre><code>component.is_dependency_of(v1.Component{name: \"foo\"})\n  &amp;&amp; project.depends_on(v1.Component{name: \"bar\"})\n</code></pre> <p><code>is_dependency_of</code> and <code>depends_on</code> lookups currently support the following Component fields:</p> <ul> <li><code>uuid</code></li> <li><code>group</code></li> <li><code>name</code></li> <li><code>version</code></li> <li><code>classifier</code></li> <li><code>cpe</code></li> <li><code>purl</code></li> <li><code>swid_tag_id</code></li> <li><code>internal</code></li> </ul> <p>Initially, only exact matches on those fields are supported. In the future, more sophisticated matching options will be added.</p> <p>Note</p> <p>When constructing objects like Component on-the-fly, it is necessary to use their version namespace, i.e. <code>v1</code>. This is required in order to perform type checking, as well as ensuring backward compatibility.</p>"},{"location":"usage/policy-compliance/expressions/#license-blacklist","title":"License blacklist","text":"<p>The following expression matches Components that are not internal to the organization, and have either:</p> <ul> <li>No resolved License at all</li> <li>A resolved License that is not part of the <code>Permissive</code> license group</li> </ul> <pre><code>!component.is_internal &amp;&amp; (\n  !has(component.resolved_license)\n    || component.resolved_license.groups.exisits(licenseGroup, \n         licenseGroup.name == \"Permissive\")\n)\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerability-blacklist","title":"Vulnerability blacklist","text":"<p>The following expression matches Components in Projects tagged as <code>3rd-party</code>, with at least one Vulnerability being any of the given blacklisted IDs.</p> <pre><code>\"3rd-party\" in project.tags\n  &amp;&amp; vulns.exists(vuln, vuln.id in [\n       \"CVE-2017-5638\",  // struts RCE\n       \"CVE-2021-44228\", // log4shell\n       \"CVE-2022-22965\", // spring4shell\n     ])\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerabilities-with-high-severity-in-public-facing-projects","title":"Vulnerabilities with high severity in public facing projects","text":"<p>The following expression matches Components in Projects tagged as <code>public-facing</code>, with at least one <code>HIGH</code> or <code>CRITICAL</code> Vulnerability, where the CVSSv3 attack vector is <code>Network</code>.</p> <pre><code>\"public-facing\" in project.tags\n  &amp;&amp; vulns.exists(vuln,\n    vuln.severity in [\"HIGH\", \"CRITICAL\"]\n      &amp;&amp; vuln.cvssv3_vector.matches(\".*/AV:N/.*\")\n  )\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#reference","title":"Reference","text":""},{"location":"usage/policy-compliance/expressions/#types","title":"Types","text":""},{"location":"usage/policy-compliance/expressions/#component","title":"<code>Component</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>is_internal</code> <code>bool</code> Is internal? <code>md5</code> <code>string</code> MD5 hash <code>sha1</code> <code>string</code> SHA1 hash <code>sha256</code> <code>string</code> SHA256 hash <code>sha384</code> <code>string</code> SHA384 hash <code>sha512</code> <code>string</code> SHA512 hash <code>sha3_256</code> <code>string</code> SHA3-256 hash <code>sha3_384</code> <code>string</code> SHA3-384 hash <code>sha3_512</code> <code>string</code> SHA3-512 hash <code>blake2b_256</code> <code>string</code> BLAKE2b-256 hash <code>blake2b_384</code> <code>string</code> BLAKE2b-384 hash <code>blake2b_512</code> <code>string</code> BLAKE2b-512 hash <code>blake3</code> <code>string</code> BLAKE3 hash <code>license_name</code> <code>string</code> License name (if unresolved) <code>license_expression</code> <code>string</code> SPDX license expression <code>resolved_license</code> <code>License</code> Resolved license <code>published_at</code> <code>google.protobuf.Timestamp</code> When the component was published <code>latest_version</code> <code>string</code> Latest known version"},{"location":"usage/policy-compliance/expressions/#license","title":"<code>License</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> SPDX license ID <code>name</code> <code>string</code> License name <code>groups</code> <code>list(License.Group)</code> Groups this license is included in <code>is_osi_approved</code> <code>bool</code> Is OSI-approved? <code>is_fsf_libre</code> <code>bool</code> Is included in FSF license list? <code>is_deprecated_id</code> <code>bool</code> Uses a deprecated SPDX license ID? <code>is_custom</code> <code>bool</code> Is custom / not included in SPDX license list?"},{"location":"usage/policy-compliance/expressions/#licensegroup","title":"<code>License.Group</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>name</code> <code>string</code> Group name"},{"location":"usage/policy-compliance/expressions/#project","title":"<code>Project</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>is_active</code> <code>bool</code> Is active? <code>tags</code> <code>list(string)</code> Tags <code>properties</code> <code>list(Project.Property)</code> Properties <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>last_bom_import</code> <code>google.protobuf.Timestamp</code>"},{"location":"usage/policy-compliance/expressions/#projectproperty","title":"<code>Project.Property</code>","text":"Field Type Description <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>value</code> <code>string</code> <code>type</code> <code>string</code>"},{"location":"usage/policy-compliance/expressions/#vulnerability","title":"<code>Vulnerability</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>CVE-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>NVD</code>) <code>aliases</code> <code>list(Vulnerability.Alias)</code> Known aliases <code>cwes</code> <code>list(int)</code> CWE IDs <code>created</code> <code>google.protobuf.Timestamp</code> When the vulnerability was created <code>published</code> <code>google.protobuf.Timestamp</code> When the vulnerability was published <code>updated</code> <code>google.protobuf.Timestamp</code> Then the vulnerability was updated <code>severity</code> <code>string</code> <code>cvssv2_base_score</code> <code>double</code> CVSSv2 base score <code>cvssv2_impact_subscore</code> <code>double</code> CVSSv2 impact sub score <code>cvssv2_exploitability_subscore</code> <code>double</code> CVSSv2 exploitability sub score <code>cvssv2_vector</code> <code>string</code> CVSSv2 vector <code>cvssv3_base_score</code> <code>double</code> CVSSv3 base score <code>cvssv3_impact_subscore</code> <code>double</code> CVSSv3 impact sub score <code>cvssv3_exploitability_subscore</code> <code>double</code> CVSSv3 exploitability sub score <code>cvssv3_vector</code> <code>string</code> CVSSv3 vector <code>owasp_rr_likelihood_score</code> <code>double</code> OWASP Risk Rating likelihood score <code>owasp_rr_technical_impact_score</code> <code>double</code> OWASP Risk Rating technical impact score <code>owasp_rr_business_impact_score</code> <code>double</code> OWASP Risk Rating business impact score <code>owasp_rr_vector</code> <code>string</code> OWASP Risk Rating vector <code>epss_score</code> <code>double</code> EPSS score <code>epss_percentile</code> <code>double</code> EPSS percentile"},{"location":"usage/policy-compliance/expressions/#vulnerabilityalias","title":"<code>Vulnerability.Alias</code>","text":"Field Type Description <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>GHSA-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>GITHUB</code>)"},{"location":"usage/policy-compliance/expressions/#function-definitions","title":"Function Definitions","text":"<p>In addition to the standard definitions of the CEL specification, Dependency-Track offers additional functions to unlock even more use cases:</p> Symbol Type Description <code>depends_on</code> <code>(Project, Component)</code> -&gt; <code>bool</code> Check if <code>Project</code> depends on <code>Component</code> <code>compare_age</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s age matches a given duration <code>is_dependency_of</code> <code>(Component, Component)</code> -&gt; <code>bool</code> Check if a <code>Component</code> is a dependency of another <code>Component</code> <code>matches_range</code> <code>(Project, string)</code> -&gt; <code>bool</code><code>(Component, string)</code> -&gt; <code>bool</code> Check if a <code>Project</code> or <code>Component</code> matches a vers range <code>matches_version_distance</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s version matches a given distance"},{"location":"usage/policy-compliance/overview/","title":"Overview","text":"<p>Organizations can create policies and measure policy violations across the portfolio, and against individual projects and components. Policies are configurable and can be enforced for the portfolio, or can be limited to specific projects. Policies are evaluated when an SBOM is uploaded.</p> <p>There are three types of policy violations:</p> <ul> <li>License</li> <li>Security</li> <li>Operational</li> </ul>"},{"location":"usage/policy-compliance/overview/#license-violation","title":"License Violation","text":"<p>Policy conditions can specify zero or more SPDX license IDs as well as license groups. Dependency-Track comes with pre-configured groups of related licenses (e.g. Copyleft) that provide a starting point for organizations to create custom license policies.</p>"},{"location":"usage/policy-compliance/overview/#security-violation","title":"Security Violation","text":"<p>Policy conditions can specify the severity of vulnerabilities. A vulnerability affecting a component can result in a policy violation if the policy condition matches the severity of the vulnerability. Vulnerabilities that are suppressed will not result in a policy violation.</p>"},{"location":"usage/policy-compliance/overview/#operational-violation","title":"Operational Violation","text":"<p>Policy conditions can specify zero or more:</p> <ul> <li>Coordinates (group, name, version)</li> <li>Package URL</li> <li>CPE</li> <li>SWID Tag ID</li> <li>Hash (MD5, SHA, SHA3, Blake2b, Blake3)</li> </ul> <p>This allows organizations to create lists of allowable and/or prohibited components. Future versions of Dependency-Track will incorporate additional operational parameters into the policy framework.</p>"}]}