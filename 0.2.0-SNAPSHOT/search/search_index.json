{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hyades","text":""},{"location":"#what-is-this","title":"What is this? \ud83e\udd14","text":"<p>Hyades, named after the star cluster closest to earth,  decouples responsibilities from Dependency-Track's monolithic API server into separate,  scalable\u2122 services. We're using Kafka (or Kafka-compatible brokers like Redpanda) for communicating between API  server and Hyades services.</p> <p>If you're interested in the technical background of this project, please refer to \ud83d\udc49 <code>WTF.md</code> \ud83d\udc48.</p> <p>As of now, Hyades is capable of:</p> <ul> <li>Performing vulnerability analysis using scanners that leverage:</li> <li>Dependency-Track's internal vulnerability database</li> <li>OSS Index</li> <li>Snyk</li> <li>Gathering component metadata (e.g. latest available version) from remote repositories</li> <li>Sending notifications via all channels supported by the original API server (E-Mail, Webhook, etc.)</li> </ul> <p>Here's a rough overview of the architecture:</p> <p></p> <p>To read more about the individual services, refer to their respective <code>REAMDE.md</code>:</p> <ul> <li>Repository Metadata Analyzer</li> <li>Vulnerability Analyzer</li> </ul>"},{"location":"#great-can-i-try-it","title":"Great, can I try it? \ud83d\ude4c","text":"<p>Yes! We prepared demo setup that you can use to play around with Hyades. Check out \ud83d\udc49 <code>DEMO.md</code> \ud83d\udc48 for details!</p>"},{"location":"#technical-documentation","title":"Technical Documentation \ud83d\udcbb","text":""},{"location":"#configuration","title":"Configuration \ud83d\udcdd","text":"<p>See <code>CONFIGURATION.md</code>.</p>"},{"location":"#development","title":"Development","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>JDK 17+</li> <li>Docker</li> </ul>"},{"location":"#building","title":"Building","text":"<pre><code>mvn clean install -DskipTests\n</code></pre>"},{"location":"#running-locally","title":"Running locally","text":"<p>Running the Hyades services locally requires both a Kafka broker and a database server to be present. Containers for Redpanda and PostgreSQL can be launched using Docker Compose:</p> <pre><code>docker compose up -d\n</code></pre> <p>To launch individual services execute the <code>quarkus:dev</code> Maven goal for the respective module:</p> <pre><code>mvn -pl vulnerability-analyzer quarkus:dev\n</code></pre> <p>Make sure you've built the project at least once, otherwise the above command will fail.</p> <p>Note If you're unfamiliar with Quarkus' Dev Mode, you can read more about it  here</p>"},{"location":"#testing","title":"Testing \ud83e\udd1e","text":""},{"location":"#unit-testing","title":"Unit Testing \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"<p>To execute the unit tests for all Hyades modules:</p> <pre><code>mvn clean verify\n</code></pre>"},{"location":"#end-to-end-testing","title":"End-To-End Testing \ud83e\udddf","text":"<p>Note End-to-end tests are based on container images. The tags of those images are currently hardcoded. For the Hyades services, the tags are set to <code>latest</code>. If you want to test local changes, you'll have to first: * Build container images locally * Update the tags in <code>AbstractE2ET</code></p> <p>To execute end-to-end tests as part of the build:</p> <pre><code>mvn clean verify -Pe2e-all\n</code></pre> <p>To execute only the end-to-end tests:</p> <pre><code>mvn -pl e2e clean verify -Pe2e-all\n</code></pre>"},{"location":"#load-testing","title":"Load Testing \ud83d\ude80","text":"<p>See <code>load-tests</code>.</p>"},{"location":"#deployment","title":"Deployment \ud83d\udea2","text":"<p>The recommended way to deploy Hyades is via Helm. Our chart is not officially published to any repository yet, so for now you'll have to clone this repository to access it.</p> <p>The chart does not include:</p> <ul> <li>a database</li> <li>a Kafka-compatible broker</li> <li>the API server</li> <li>the frontend</li> </ul> <p>While API server and frontend will eventually be included, database and Kafka broker will not.</p> <p>Helm charts to deploy Kafka brokers to Kubernetes are provided by both Strimzi  and Redpanda. </p>"},{"location":"#minikube","title":"Minikube","text":"<p>Deploying to a local Minikube cluster is a great way to get started.</p> <p>Note For now, services not included in the Helm chart are deployed using Docker Compose.</p> <ol> <li>Start PostgreSQL and Redpanda via Docker Compose <pre><code>docker compose up -d\n</code></pre></li> <li>Start the API server and frontend <pre><code>docker compose up -d apiserver frontend\n</code></pre></li> <li>Start a local Minikube cluster <pre><code>minikube start\n</code></pre></li> <li>Deploy Hyades <pre><code>helm install hyades ./helm-charts/ \\\n-n hyades --create-namespace \\\n-f ./helm-charts/hyades/values.yaml \\\n-f ./helm-charts/hyades/values-minikube.yaml\n</code></pre></li> </ol>"},{"location":"#monitoring","title":"Monitoring \ud83d\udcca","text":""},{"location":"#metrics","title":"Metrics","text":"<p>A basic metrics monitoring stack is provided, consisting of Prometheus and Grafana. To start both services, run:</p> <pre><code>docker compose --profile monitoring up -d\n</code></pre> <p>The services will be available locally at the following locations:</p> <ul> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000</li> </ul> <p>Prometheus is configured to scrape metrics from the following services in a 5s intervals:</p> <ul> <li>Redpanda Broker</li> <li>API Server</li> <li>Notification Publisher</li> <li>Repository Meta Analyzer</li> <li>Vulnerability Analyzer</li> </ul> <p>The Grafana instance will be automatically provisioned to use Prometheus as data source. Additionally, dashboards for the following services are automatically set up:</p> <ul> <li>Redpanda Broker</li> <li>API Server</li> <li>Vulnerability Analyzer</li> </ul>"},{"location":"#redpanda-console","title":"Redpanda Console \ud83d\udc3c","text":"<p>The provided <code>docker-compose.yml</code> includes an instance of Redpanda Console to aid with gaining insight into what's happening in the message broker. Among many other things, it can be used to inspect messages inside any given topic.</p> <p></p> <p>The console is exposed at <code>http://127.0.0.1:28080</code> and does not require authentication. It's intended for local use only.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at steve.springett@owasp.org or niklas.duester@owasp.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"CONFIGURATION/","title":"Configuration","text":"<p>All available configuration options used by all applications are listed in their respective <code>application.properties</code>. Options can be provided via environment variables as well, refer to the Quarkus docs for details.</p> <p>Not all options are supposed to be tweaked by users though. This document contains an overview of all options that are expected to be changed by users.</p>"},{"location":"CONFIGURATION/#kafka-topic-configuration","title":"Kafka Topic Configuration","text":"<p>Kafka topics (including internal topics) can be configured with custom prefix. In order to provide custom prefix, below environment variable can be used.</p> Environment Variable Description Default Required <code>API_TOPIC_PREFIX</code> Prefix for topic names - \u274c"},{"location":"CONFIGURATION/#notification-publisher","title":"Notification Publisher","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>PARALLEL_CONSUMER_MAX_CONCURRENCY</code> Number of threads to process notifications with <code>6</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_INITIAL_DELAY</code> Initial delay before retrying notification delivery <code>3S</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_MULTIPLIER</code> Multiplier for retry delays <code>0.3</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_RANDOMIZATION_FACTOR</code> Randomization factory for jitter in retry delays <code>0.3</code> \u274c <code>PARALLEL_CONSUMER_RETRY_MAX_DURATION</code> Maximum duration of delays between retry attempts <code>2M</code> \u2705 <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <code>QUARKUS_MAILER_FROM</code> The sender name for email notifications - When email notifications are enabled <code>QUARKUS_MAILER_HOST</code> Address of the mail server for email notifications - When email notifications are enabled <code>QUARKUS_MAILER_PORT</code> Port of the mail server for email notifications - When email notifications are enabled <code>QUARKUS_MAILER_SSL</code> Use SSL / TLS to communicate with the email server <code>false</code> - <code>QUARKUS_MAILER_START_TLS</code> Use StartTLS to communicate with the email server <code>DISABLED</code> When email notifications are enabled <code>QUARKUS_MAILER_USERNAME</code> Username to authenticate with the email server - When email notifications are enabled <code>QUARKUS_MAILER_PASSWORD</code> Password to authenticate with the email server - When email notifications are enabled <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"CONFIGURATION/#repository-meta-analyzer","title":"Repository Meta Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"CONFIGURATION/#vulnerability-analyzer","title":"Vulnerability Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>STATE_STORE_TYPE</code> Whether to use in-memory or persistent (RocksDB) Kafka Streams state stores <code>in_memory</code> \u2705 <code>STATE_STORE_ROCKS_DB_COMPACTION_STYLE</code> Compaction style to use for RocksDB state stores - \u274c <code>STATE_STORE_ROCKS_DB_COMPRESSION_TYPE</code> Compression type to use for RocksDB state stores - \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <code>SCANNER_INTERNAL_ENABLED</code> Enable the internal vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_ENABLED</code> Enable the OSS Index vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by OssIndex analyzer. This depends on standard and custom supported package urls <code>cargo,composer,gem,hex,maven,npm,nuget,pypi,rpm,conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by ossindex. These are not part of the standard package urls list <code>conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_API_USERNAME</code> OSS Index API username - \u274c <code>SCANNER_OSSINDEX_API_TOKEN</code> OSS Index API token - \u274c <code>SCANNER_OSSINDEX_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> \u274c <code>SCANNER_OSSINDEX_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSS Index <code>false</code> \u274c <code>SCANNER_SNYK_ENABLED</code> Enable the Snyk vulnerability scanner <code>false</code> \u274c <code>SCANNER_SNYK_API_ORG_ID</code> Snyk organization ID - When Snyk is enabled <code>SCANNER_SNYK_API_TOKENS</code> Comma-separated list of Snyk API tokens - When Snyk is enabled <code>SCANNER_SNYK_API_VERSION</code> Version of the Snyk API to use <code>2022-12-15</code> When Snyk is enabled <code>SCANNER_SNYK_SEVERITY_SOURCE_PRIORITY</code> Priority of preferred source for vulnerability severities <code>nvd,snyk,redhat,suse</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_SIZE</code> Max size of batch at which it will be submitted <code>100</code> When Snyk is enabled <code>SCANNER_SNYK_ALIAS_SYNC_ENABLED</code> Enable alias syncing for Snyk <code>false</code> \u274c <code>SCANNER_SNYK_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by Snyk analyzer. This depends on supported standard and custom package urls <code>cargo,cocoapods,composer,gem,generic,hex,maven,npm,nuget,pypi,swift,golang</code> \u274c <code>SCANNER_SNYK_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by Snyk. These are not part of the standard package urls list <code>cocoapods,apk,swift</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"CONFIGURATION/#mirror-service","title":"Mirror Service","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>MIRROR_DATASOURCE_GITHUB_ALIAS_SYNC_ENABLED</code> Enable alias syncing for GitHub Advisories <code>false</code> \u274c <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSV <code>false</code> \u274c <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to OWASP Dependency-Track","text":"<p>Thank you for contributing to Dependency-Track! We appreciate and are thankful for contributions in all shapes and sizes.</p>"},{"location":"CONTRIBUTING/#asking-questions","title":"Asking Questions","text":"<ul> <li>Use either Slack or GitHub Discussions to ask questions.</li> <li>Do not open issues for questions. Questions submitted through GitHub issues will be closed.</li> <li>Consult the community-maintained FAQ to see whether your question has already been answered.</li> <li>Avoid contacting individual contributors directly; ask questions in public instead. A well-worded question will help serve as a resource to others searching for help.</li> </ul> <p>The <code>#proj-dependency-track</code> channel in the OWASP Slack space is the best place to ask questions and get in touch with other users and contributors. We provide an invitation to the Slack space here. Please do create an issue if either channel link or invitation do not work anymore.</p>"},{"location":"CONTRIBUTING/#filing-issues","title":"Filing Issues","text":""},{"location":"CONTRIBUTING/#looking-for-existing-issues","title":"Looking for existing Issues","text":"<p>Before you create a new issue, please do a search in open issues to see if the defect or enhancement request has already been filed.</p> <p>If you find your issue already exists, add relevant comments to the existing issue.</p> <p>Optionally indicate your interest by reacting with thumbs up (\ud83d\udc4d). Issues with higher community attention are more likely to be addressed sooner.</p> <p>If you cannot find an existing issue for your bug or feature request, create a new issue using the guidelines below.</p>"},{"location":"CONTRIBUTING/#requesting-enhancements","title":"Requesting Enhancements","text":"<p>File a single issue per enhancement request. Do not list multiple enhancement requests in the same issue.</p> <p>Describe your use case and the value you will get from the requested enhancement.</p>"},{"location":"CONTRIBUTING/#reporting-defects","title":"Reporting Defects","text":"<p>File a single issue per defect. Do not list multiple defects in the same issue.</p> <p>The more information you can provide, the more likely we will be successful at reproducing the bug and finding a fix.</p> <p>API server logs (including errors) are logged to the following locations per default:</p> <ul> <li><code>~/.dependency-track/dependency-track.log</code> (<code>/data/.dependency-track/dependency-track.log</code> within Docker containers)</li> <li>Standard output (use <code>docker logs -f &lt;CONTAINER_NAME&gt;</code> when using Docker)</li> </ul> <p>Errors in the frontend are logged to your browser's developer console (see here for Google Chrome). Issues in the communication between frontend and API server will be visible in the Network tab of your browser's developer tools (see here for Google Chrome).</p> <p>Depending on the defect, we may ask you for a sample BOM that triggers your issue.</p> <p>Before sharing BOMs, logs, screenshots or any other resources with us, please ensure that you blank, pseudonymise or remove all references that may leak internals of your organization.</p>"},{"location":"CONTRIBUTING/#reporting-vulnerabilities","title":"Reporting Vulnerabilities","text":"<p>Please refer to our security policy in <code>SECURITY.md</code> for how to responsibly disclose vulnerabilities to us.</p>"},{"location":"CONTRIBUTING/#improving-documentation","title":"Improving Documentation","text":"<p>Beside extending existing documentation or correcting any errors it may contain, another great way to contribute is to update the FAQ with questions and answers that benefit the community.</p>"},{"location":"CONTRIBUTING/#testing-snapshot-versions","title":"Testing Snapshot Versions","text":"<p>Every time a commit is pushed to the <code>master</code> branch, container images are built and published to the <code>snapshot</code> tag. This is true for both API server and frontend:</p> <ul> <li><code>dependencytrack/apiserver:snapshot</code></li> <li><code>dependencytrack/bundled:snapshot</code></li> <li><code>dependencytrack/frontend:snapshot</code></li> </ul> <p>Testing <code>snapshot</code> versions and providing early feedback helps in improving the quality of new releases. The private <code>#proj-dependency-track-beta</code> Slack channel is intended for discussions about and feedback for <code>snapshot</code> versions. If you are interested in participating, please let us know in <code>#proj-dependency-track</code>.</p>"},{"location":"CONTRIBUTING/#contributing-code","title":"Contributing Code","text":"<p>Before raising pull requests, please file a defect or enhancement request first.</p> <ul> <li>We use GitHub milestones to plan what will be included in the next release.<ul> <li>Depending on the size of your PR and whether it introduces breaking changes, we may schedule your change for a later release.</li> </ul> </li> <li>Issues for which community contributions are explicitly requested are labeled with <code>help wanted</code>.</li> <li>Issues suitable for first-time contributors are labeled with <code>good first issue</code>.</li> </ul>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<ul> <li>Pull requests that do not merge easily with the tip of the <code>master</code> branch will be declined.<ul> <li>The author will be asked to merge with tip and submit a new pull request.</li> </ul> </li> <li>Code should follow standard code style conventions for whitespace, indentation and naming.<ul> <li>In the case of style differences between existing code and language standards, consistency with existing code is preferred.</li> </ul> </li> <li>New functionality should have corresponding tests added to the existing test suite if possible.</li> <li>Avoid new dependencies if the functionality that is being used is trivial to implement directly or is available in standard libraries.</li> <li>Avoid checking in unrelated whitespace changes with code changes.</li> <li>Commits must be signed off to indicate agreement with Developer Certificate of Origin (DCO).</li> <li>Optionally include visualizations like screenshots, videos or diagrams in the pull request description.</li> </ul>"},{"location":"CONTRIBUTING/#commit-messages","title":"Commit Messages","text":"<p>Please follow these rules when writing a commit message:</p> <ul> <li>Separate subject from body with a blank line</li> <li>Limit the subject line to 50 characters</li> <li>Capitalize the subject line</li> <li>Do not end the subject line with a period</li> <li>Use the imperative mood in the subject line</li> <li>Wrap the body at 72 characters</li> <li>Use the body to explain what and why vs. how</li> </ul>"},{"location":"DEMO/","title":"Demo","text":""},{"location":"DEMO/#setup","title":"Setup \ud83d\udcbb","text":"<p>All you need is Docker, Docker Compose and a somewhat capable machine. A UNIX-based system is strongly recommended. In case you're bound to Windows, please use WSL.</p> <p>Note A &gt;4 core CPU and &gt;=16GB RAM are recommended for a smooth experience.</p> <ol> <li>In a terminal, clone this repository and navigate to it: <pre><code>git clone https://github.com/DependencyTrack/hyades.git\ncd hyades\n</code></pre></li> <li>Generate a secret key for encryption and decryption of credentials in the database: <pre><code>openssl rand 32 &gt; secret.key\n</code></pre></li> <li>Pull and start all containers: <pre><code>docker compose --profile demo up -d --pull always\n</code></pre></li> <li>Make sure you include the <code>--profile demo</code> flag!</li> </ol> <p>Once completed, the following services will be available:</p> Service URL API Server http://localhost:8080 Frontend http://localhost:8081 Redpanda Console http://localhost:28080 PostgreSQL <code>localhost:5432</code> Redpanda Kafka API <code>localhost:9092</code> <p>Note You'll not need to interact with PostgreSQL or the Kafka API directly to try out the project, but if you're curious \ud83d\udd75\ufe0f of course you can!</p> <p>Finally, to remove everything again, including persistent volumes:</p> <pre><code>docker compose --profile demo down --volumes\n</code></pre>"},{"location":"DEMO/#common-issues","title":"Common Issues","text":""},{"location":"DEMO/#postgres-container-fails-to-start","title":"Postgres container fails to start","text":"<p>If the <code>dt-postgres</code> container fails to start with messages like:</p> <pre><code>ls: can't open '/docker-entrypoint-initdb.d/': Permission denied\n</code></pre> <p>It's likely that the local directory mounted into <code>/docker-entrypoint-initdb.d</code> is not accessible by the postgres process. To fix, make the local directory readable by everyone, and restart the <code>dt-postgres</code> container:</p> <pre><code>chmod -R o+r ./commons/src/main/resources/migrations/postgres\ndocker restart dt-postgres\n</code></pre>"},{"location":"DEMO/#testing","title":"Testing \ud83e\udd1e","text":"<ol> <li>In a web browser, navigate to http://localhost:8081 and login (username: <code>admin</code>, password: <code>admin</code>)</li> <li>Navigate to the Notifications section in the administration panel</li> <li>Create a new alert with publisher Outbound Webhook </li> <li>Select a few notification groups and enter a destination URL (Pipedream is convenient for testing Webhooks)    </li> <li>Navigate to the projects view and click Create Project</li> <li>Provide an arbitrary project name and click Create</li> <li>Select the project you just created from the project list</li> <li>Navigate to the Components tab and click Upload BOM</li> <li>Upload any (S)BOM you like. If you don't have one handy, here are some to try:<ul> <li>Dependency-Track API Server 4.6.2</li> <li>Dependency-Track Frontend 4.6.1</li> <li>CycloneDX SBOM examples</li> </ul> </li> <li>Now navigate to the Audit Vulnerabilities tab and hit the \ud83d\udd04 button to the top right of the table a few times<ul> <li>You should see the table being populated with vulnerability data</li> </ul> </li> <li>Going back to the service you used as Webhook destination, you should see that a few alerts have been delivered     </li> </ol> <p>Overall, this should behave just like what you're used to from Dependency-Track. However in this case, the publishing of notifications and vulnerability analysis was performed by external, individually scalable services.</p>"},{"location":"DEMO/#scaling-up","title":"Scaling up \ud83d\udcc8","text":"<p>Warning This section is still a work in progress and does not necessarily show the current state of the setup.</p> <p>One of the goals of this project is to achieve scalability, remember? Well, we're delighted to report that there are multiple ways to scale! If you're interested, you can find out more about the parallelism model at play here.</p> <p>Per default, when opening the Consumer Groups view in Redpanda Console, you'll see a total of two groups:</p> <p></p> <p>The Members column shows the number of stream threads in each group. Clicking on the dtrack-vuln-analyzer group will reveal a more detailed view:</p> <p></p> <p>Each stream thread got assigned 20 partitions. 20 partitions are a lot to take care of, so being limited to just three stream threads will not yield the best performance.</p>"},{"location":"DEMO/#scaling-a-single-instance","title":"Scaling a single instance \ud83d\ude80","text":"<p>Arguably the easiest option is to simply increase the number of stream threads used by a service instance. By modifying the <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> environment variable in <code>docker-compose.yml</code>, the number of worker threads can be tweaked.</p> <p>Let's change it to <code>3</code> and see what happens! To do this, remove the comment (<code>#</code>) from the <code># KAFKA_STREAMS_NUM_STREAM_THREADS: \"3\"</code> line in <code>docker-compose.yml</code>, and recreate the container with <code>docker compose up -d vulnerability-analyzer</code>.</p>"},{"location":"DEMO/#scaling-to-multiple-instances","title":"Scaling to multiple instances \ud83d\ude80\ud83d\ude80\ud83d\ude80","text":"<p>Putting more load on a single service instance is not always desirable, so oftentimes simply increasing the replica count is the preferable route. In reality this may be done via Kubernetes manifests, but we can do it in Docker Compose, too. Let's scale up to three instances:</p> <pre><code>docker compose --profile demo up -d --scale vulnerability-analyzer=3\n</code></pre>"},{"location":"SECURITY/","title":"Reporting Security Issues","text":"<p>The Dependency-Track team and community take security bugs seriously. We appreciate your efforts to responsibly disclose your findings, and will make every effort to acknowledge your contributions.</p> <p>To report a security issue, email security@dependencytrack.org, steve.springett@owasp.org, and niklas.duester@owasp.org and include the word \"SECURITY\" in the subject line.</p> <p>The Dependency-Track team will send a response indicating the next steps in handling your report. After the initial reply to your report, the security team will keep you informed of the progress towards a fix and full announcement, and may ask for additional information or guidance.</p> <p>Report security bugs in third-party modules to the person or team maintaining the module.</p>"},{"location":"TOPICS/","title":"Topics","text":"Name Partitions Config <code>dtrack-apiserver-processed-vuln-scan-result-by-scan-token-repartition</code><sup>1A</sup> 3 <code>dtrack.notification.analyzer</code> 3 <code>dtrack.notification.bom</code> 3 <code>dtrack.notification.configuration</code> 3 <code>dtrack.notification.datasource-mirroring</code> 3 <code>dtrack.notification.file-system</code> 3 <code>dtrack.notification.integration</code> 3 <code>dtrack.notification.new-vulnerability</code> 3 <code>dtrack.notification.new-vulnerable-dependency</code> 3 <code>dtrack.notification.policy-violation</code> 3 <code>dtrack.notification.project-audit-change</code> 3 <code>dtrack.notification.project-created</code> 3 <code>dtrack.notification.repository</code> 3 <code>dtrack.notification.vex</code> 3 <code>dtrack.notification.project-vuln-analysis-complete</code> <sup>3</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>dtrack.repo-meta-analysis.component</code><sup>1B</sup> 3 <code>dtrack.repo-meta-analysis.result</code> 3 <code>dtrack.vuln-analysis.component</code><sup>1C</sup> 3 <code>dtrack.vuln-analysis.result</code><sup>1A</sup> 3 <code>dtrack.vuln-analysis.scanner.result</code><sup>1C</sup> 3 <code>dtrack.vulnerability</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.digest</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.mirror.command</code><sup>2</sup> 1 <code>dtrack.vulnerability.mirror.state</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>hyades-repository-meta-analyzer-command-by-purl-coordinates-repartition</code><sup>1B</sup> 3 <code>hyades-vulnerability-analyzer-completed-scans-table-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-last-update-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-table-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-ossindex-batch-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-ossindex-retry-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-scan-task-internal-repartition</code> 3 <code>hyades-vulnerability-analyzer-scan-task-ossindex-repartition</code><sup>1D</sup> 3 <code>hyades-vulnerability-analyzer-scan-task-snyk-repartition</code><sup>1E</sup> 3 <code>hyades-vulnerability-analyzer-snyk-batch-store-changelog</code><sup>1E</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <code>hyades-vulnerability-analyzer-snyk-retry-store-changelog</code><sup>1E</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=0</code> <p><sup>1X</sup> The topic is subject to co-partitioning requirements <sup>2</sup> The partition number of this topic should not be changed <sup>3</sup> To use this notification, the PROJECT_VULN_ANALYSIS_COMPLETE group needs to be manually added through api call to the notify on condition The put request to be executed is here</p>"},{"location":"TOPICS/#co-partitioning-requirements","title":"Co-Partitioning Requirements","text":"<p>Some topics must be co-partitioned, meaning they must share the exact same number of partitions. Applications using those topics will not work correctly when this is not the case.</p>"},{"location":"WTF/","title":"Q&amp;A","text":""},{"location":"WTF/#why","title":"Why?","text":"<p>tl;dr: Dependency-Track's architecture prevents it from scaling past a certain workload.</p> <p>Dependency-Track, for the most part, is an event-based system. As a platform for ingesting data (in the form of BOMs), listening for and itself emitting signals on it, an event-based architecture makes sense conceptually. The majority of operations Dependency-Track performs happen asynchronously, without client interaction.</p> <p>On a technical level, Dependency-Track uses an in-memory publish-subscribe architecture, implemented using Java <code>ExecutorService</code>s. An <code>ExecutorService</code> can be thought of as a pool of worker threads, consuming from an internal task queue. Tasks can be submitted to an <code>ExecutorService</code>, which will then execute them one-by-one. As multiple threads work on the queue in  parallel, the order in which tasks are being processed is not guaranteed. Thread pool sizes can vary from one,  up to unbounded numbers of threads.</p> <p>In Dependency-Track, when an event is published, subscribers to the event are looked up.  Per API contract,  event subscribers must implement an <code>inform</code> method, which takes the published event as argument. For any given event, 0-N tasks will be enqueued to the <code>ExecutorService</code>'s task queue - one for each subscriber.</p> <p></p> <p>There are three <code>ExecutorService</code> instances in Dependency-Track:</p> <ul> <li><code>EventService</code></li> <li><code>SingleThreadedEventService</code></li> <li><code>NotificationService</code></li> </ul> <p><code>EventService</code> forms the primary worker pool. Its thread pool size defaults to <code>&lt;NUM_CPU&gt; * 4</code>.  A machine with a 4-core CPU will thus have a thread pool size of <code>16</code>. The size is configurable. Common tasks handled by this worker pool include:</p> <ul> <li>Processing of uploaded BOMs and VEXs</li> <li>Performing vulnerability analysis of components, projects, or the entire portfolio</li> <li>Performing repository meta analysis of components, projects, or the entire portfolio</li> <li>Calculation of metrics for components, projects, or the entire portfolio</li> </ul> <p><code>SingleThreadedEventService</code> is a worker pool with only a single thread. The purpose of this worker pool is execute tasks that must not be run in parallel. As such, it serves as a means to serialize task execution.  Common tasks handled by this worker pool include:</p> <ul> <li>Mirroring of the NVD vulnerability database</li> <li>Updating Lucene indexes on disk</li> </ul> <p><code>NotificationService</code> is a dedicated worker pool for dispatching notifications. Its thread pool size defaults to <code>4</code> and is not configurable.</p>"},{"location":"WTF/#limitations","title":"Limitations","text":"<p>While this architecture works great for small to medium workloads, it presents various challenges for larger ones:</p> <ol> <li>Not horizontally scalable. As pub-sub is happening entirely in-memory, it is not possible to distribute the work to multiple application instances. The only way to handle more load using this architecture is to scale vertically, e.g.</li> <li>Increasing <code>ExecutorService</code> thread pool sizes (<code>alpine.worker.threads</code>, <code>alpine.worker.thread.multiplier</code>)</li> <li>Increasing database connection pool sizes</li> <li>Increasing resource allocations for CPU, RAM, and potentially disk / network</li> <li>No ordering guarantees of events. As multiple threads work on a shared queue of tasks in parallel, there is no way of enforcing a reliable ordering of events. </li> <li>Limited fault-tolerance. If an instance of Dependency-Track goes down, planned or unplanned, all queued tasks are  gone. Not only does this impact business-as-usual operation, but also limits the times when upgrades can be applied.</li> <li>Shared, multipurpose task queue. A single task queue is used to process all kinds of events. This means that lots of events of a certain type can \"clog\" the task queue, preventing other types of events from being processed. This is further amplified if processing of events \"clogging\" the queue relies on external services, introducing further latency. Ideally, there should be a dedicated queue per event type, so that one busy queue doesn't block others.</li> <li>Prone to race conditions. As a consequence of (2), it is possible that multiple events addressing the same thing are processed in parallel, leading to race conditions in cache lookups or database operations. Race conditions would be an even bigger problem if the work was shared across multiple application instances, and would require distributed locking as a countermeasure, which is inherently hard to get right.</li> </ol> <p>In order to scale Dependency-Track beyond its current capabilities, a distributed messaging service is required.</p>"},{"location":"WTF/#related-issues","title":"Related Issues","text":"<p>On multiple occasions in the past, the Dependency-Track community raised questions about high availability (HA) deployments, and / or how to better scale the platform:</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/issues/218</li> <li>https://github.com/DependencyTrack/dependency-track/issues/903</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1210</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1856</li> </ul> <p>Note The work we've done so far does not make the API server highly available. However, it does address a substantial chunk of work that is required to make that happen.</p>"},{"location":"WTF/#why-kafka","title":"Why Kafka?","text":"<p>Kafka was chosen because it employs various concepts  that are advantageous for Dependency-Track:</p> <ul> <li>It supports publish-subscribe use cases based on topics and partitions</li> <li>Events with the same key are guaranteed to be sent to the same partition</li> <li>Order of events is guaranteed on the partition level</li> <li>Consumers can share the load of consuming from a topic by forming consumer groups<ul> <li>Minor drawback: maximum concurrency is bound to the number of partitions</li> </ul> </li> <li>It is distributed and fault-tolerant by design, replication is built-in</li> <li>Events are stored durably on the brokers, with various options to control retention</li> <li>Log compaction allows for fault-tolerant, stateful processing,   by streaming changes of a local key-value database to Kafka</li> <li>In certain cases, this can aid in reducing load on the database server</li> <li>Mature ecosystem around it, including a vast landscape of client libraries for various languages</li> <li>Kafka Streams with its support for     stateful transformations     in particular turned out to be a unique selling point for the Kafka ecosystem</li> <li>Mature cloud offerings for fully managed instances (see Options for running Kafka)</li> </ul> <p>The concept of partitioned topics turned out to be especially useful: We can rely on the fact that events with the same key always end up in the same partition, and are processed by only one consumer (within a consumer group) at a time. In case of vulnerability scanning, by choosing the component's PURL as event key, it can be guaranteed that only the first event triggers an HTTP request to OSS Index, while later events can be handled immediately from cache. There is no race condition anymore between lookup and population of the cache.</p> <p>We also found the first-class support for stateful processing incredibly useful in some cases, e.g.:</p> <ul> <li>Scatter-gather.    As used for scanning one component with multiple analyzers. Waiting for all analyzers to complete is a stateful   operation, that otherwise would require database access.</li> <li>Batching. Some external services allow for submitting multiple component identifiers per request.   With OSS Index, up to 128 package URLs can be sent in a single request. Submitting only one package URL at a   time would drastically increase end-to-end latency. It'd also present a greater risk of getting rate limited.</li> </ul> <p>That being said, Kafka does add a considerable amount of operational complexity. Historically, Kafka has depended on Apache ZooKeeper. Operating both Kafka and ZooKeeper is not something we wanted  to force DT users to do. Luckily, the Apache Kafka project has been working on removing the ZooKeeper dependency,  and replacing it with Kafka's own raft consensus protocol (KRaft).</p> <p>There are other, more light-weight, yet Kafka API-compatible broker implementations available, too.  Redpanda being the most popular. Redpanda is distributed in a single, self-contained binary and is optimal for deployments with limited resources. Having options like Redpanda available makes building a system on Kafka much more viable.</p> <p>For this reason in fact, we primarily develop with, and test against, Redpanda.</p>"},{"location":"WTF/#considered-alternatives","title":"Considered alternatives","text":"<p>Before choosing for Kafka, we looked at various other messaging systems.</p>"},{"location":"WTF/#apache-pulsar","title":"Apache Pulsar","text":"<p>Among all options, Pulsar was the most promising besides Kafka. Pulsar prides itself in  being truly cloud native, supporting tiered storage, and multiple messaging paradigms (pub-sub and queueing).  It has native support for negative acknowledgment  of messages, and message retries.  However, the Pulsar architecture consists not  only of Pulsar brokers, but also requires Apache ZooKeeper and Apache BookKeeper  clusters. We had to dismiss Pulsar for its operational complexity.</p>"},{"location":"WTF/#rabbitmq","title":"RabbitMQ","text":"<p>RabbitMQ is a popular message broker. It exceeds in cases where multiple worker processes need to work on a shared queue of tasks (similar to how Dependency-Track does it today). It can achieve a high level of concurrency, as there's no limit to how many consumers can consume from a queue. This high grade of concurrency comes with the cost of lost ordering, and high potential for race conditions. </p> <p>RabbitMQ supports Kafka-like partitioned streams via its Streams plugin. In the end, we decided against RabbitMQ, because brokers do not support key-based compaction, and its consumer libraries  in turn lack adequate support for fault-tolerant stateful operations.</p>"},{"location":"WTF/#liftbridge","title":"Liftbridge","text":"<p>Liftbridge is built on top of NATS and provides Kafka-like features. It is however not compatible with the Kafka API, as it uses a custom envelope protocol, and is heavily focused on Go. There are no managed service offerings for Liftbridge, leaving self-hosting as only option to run it.</p>"},{"location":"WTF/#options-for-running-kafka","title":"Options for running Kafka","text":"<p>When it comes to running Kafka in production, users will have the choice between various self-hosted and fully managed  Infrastructure as a Service (IaaS) solutions. The following table lists a few, but there will be more we don't know of:</p> Solution Type URL Apache Kafka Self-Hosted https://kafka.apache.org/quickstart Redpanda Self-Hosted / IaaS https://redpanda.com/ Strimzi Self-Hosted https://strimzi.io/ Aiven IaaS https://aiven.io/kafka AWS MSK IaaS https://aws.amazon.com/msk/ Azure Event Hubs IaaS https://azure.microsoft.com/en-us/products/event-hubs/ Confluent Cloud IaaS https://www.confluent.io/ Red Hat OpenShift Streams IaaS https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview <p>The wide range of mature IaaS offerings is a very important benefit of Kafka over other messaging systems we evaluated.</p>"},{"location":"WTF/#why-java","title":"Why Java?","text":"<p>We went with Java for now because it was the path of the least resistance for us. There is no intention to exclusively  use Java though. We are considering to use Go, and generally  are open to any technology that makes sense.</p>"},{"location":"WTF/#why-not-microservices","title":"Why not microservices?","text":"<p>The proposed architecture is based on the rough idea of domain services for now. This keeps the number of independent services manageable, while still allowing us to distribute the overall system load. If absolutely necessary, it is  possible to break this up even further. For example, instead of having one vulnerability-analyzer service,  the scanners for each vulnerability source (e.g. OSS Index, Snyk) could be separated out into dedicated microservices.</p>"},{"location":"design/01-workflow-state-tracking/","title":"Tracking of Workflow State for BOM Processing and Analysis","text":"<p>Note This document was extracted from #664.</p> <p>For CI/CD use-cases, Dependency-Track offers a mechanism that allows clients to poll whether the BOM they just uploaded is still being processed. \"Processing\" in this context refers to:</p> <ul> <li>Consumption (Parsing)</li> <li>Ingestion (Sync parsed data with database)</li> <li>Vulnerability analysis</li> <li>Policy Evaluation</li> </ul> <p>This is an important capability, allowing for implementation of quality gates and reporting in CI/CD pipelines.</p> <p>The mechanism works by assigning identifiers (UUIDs) to events in Alpine's in-memory event system. As long as an event associated to a given identifier can be found in its internal queue, the identifier is considered to be \"processing\":</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/blob/6153d286d1ac806462bc76cfe17d84a57c224671/src/main/java/org/dependencytrack/resources/v1/BomResource.java#L323-L342</li> <li>https://github.com/stevespringett/Alpine/blob/cd6aa7ed05376935ab32bc43819eba0e3a525b7f/alpine-infra/src/main/java/alpine/event/framework/BaseEventService.java#L158-L161</li> </ul> <p>Events can be chained, such that a <code>BomUploadEvent</code> will trigger a <code>VulnerabilityAnalysisEvent</code>, which will trigger a <code>PolicyEvaluationEvent</code>, and so on. The event identifier is inherited by chained events.</p> <p>As everything happens in-memory, this mechanism does not work when multiple instances of the API server are used in an active-active deployment. For the functionality to continue to work, the state of processing steps must be persistent to external storage.</p> <p>Decoupled from #633.</p>"},{"location":"design/01-workflow-state-tracking/#design","title":"Design","text":"<p>Note The goal for now is not to build a multi-purpose workflow engine, but to track state of one specific workflow. In a future iteration, we may invest more time into coming up with a generic workflow engine concept.</p>"},{"location":"design/01-workflow-state-tracking/#what-to-track","title":"What to track","text":"<ol> <li>BOM consumption (parsing, de-duplication, etc.)</li> <li>BOM processing (ingestion into database)</li> <li>Vulnerability analysis</li> <li>Repository metadata analysis</li> <li>Policy evaluation</li> <li>Metrics update</li> </ol> <p>The order of execution as of today is as follows:</p> <pre><code>flowchart LR\n    A(BOM_CONSUMPTION) --&gt; B(BOM_PROCESSING)\n    B --&gt; C(VULN_ANALYSIS)\n    B --&gt; D(REPO_META_ANALYSIS)\n    C --&gt; E(POLICY_EVALUATION)\n    E --&gt; F(METRICS_UPDATE)\n    B -. BOM contains no &lt;br/&gt;components to analyze .-&gt; F\n</code></pre> <p>Note Completion of repository metadata analysis can currently not be tracked. We'll need something similar to what we introduced in https://github.com/DependencyTrack/hyades-apiserver/pull/40 for vulnerability analysis completion tracking. For the initial implementation, it may be OK to not track it.</p>"},{"location":"design/01-workflow-state-tracking/#states","title":"States","text":"<p>There are multiple states a processing step can be in:</p> <ul> <li><code>PENDING</code>: Initial state</li> <li><code>COMPLETED</code>: Completion detected; No failures</li> <li><code>FAILED</code>: Completion detected; Failures</li> <li><code>CANCELLED</code>: Never started because a precondition failed<ul> <li>e.g. vulnerability analysis will be cancelled when BOM ingestion failed</li> </ul> </li> <li><code>NOT_APPLICABLE</code>: The step is not applicable to the subject of the analysis<ul> <li>e.g. the analysis was triggered for an existing project, without BOM upload</li> </ul> </li> <li><code>TIMED_OUT</code>: The deadline for the step completion was exceeded; Step is unlikely to complete</li> </ul> <pre><code>stateDiagram-v2\n    [*] --&gt; PENDING\n    PENDING --&gt; COMPLETED\n    COMPLETED --&gt; [*]\n    PENDING --&gt; FAILED\n    FAILED --&gt; [*]\n    PENDING --&gt; CANCELLED\n    CANCELLED --&gt; [*]\n    [*] --&gt; NOT_APPLICABLE\n    NOT_APPLICABLE --&gt; [*]\n    PENDING --&gt; TIMED_OUT\n    TIMED_OUT --&gt; COMPLETED\n    TIMED_OUT --&gt; FAILED\n</code></pre> <p>Each step is responsible for updating its own state.</p> <p>The overall state can be considered to be complete, if there's no step in <code>PENDING</code> state.</p> <p>When a step failure is detected, a \"failure reason\" message must be persisted. If multiple steps fail, (rough) failure details for each step must be available.</p> <p>There should be a deadline mechanism, which automatically transitions steps from <code>PENDING</code> into <code>TIMED_OUT</code> state. Steps in <code>TIMED_OUT</code> state communicate that it is unlikely that a terminal state can be reached (<code>COMPLETED</code>, <code>FAILED</code>). However, it is still possible (e.g. due to significant consumer lag, events may arrive late).</p>"},{"location":"design/01-workflow-state-tracking/#workflow","title":"Workflow","text":"<p>The API server will act as workflow orchestrator, kicking off new steps as needed. This implies that completion of a step must be registered by the API server.</p> <p>To illustrate how the initial stage of the workflow should look like:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ BomResource: Upload BOM\n    BomResource -&gt;&gt; BomResource: Validate BOM\n    BomResource -&gt;&gt; BomResource: Generate Correlation ID (UUID)\n    BomResource -&gt;&gt; Database: Create workflow steps\n    Note over BomResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=BOM_CONSUMPTION&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=BOM_PROCESSING&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Status=PENDING\n    BomResource -&gt;&gt; BomUploadProcessingTask: Dispatch BomUploadEvent\n    BomResource -&gt;&gt;- Client: Correlation ID\n    par\n        loop Continuously\n            Client -&gt;&gt; BomResource: Poll Status\n            Note over Client, BomResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n        end\n    and\n        BomUploadProcessingTask -&gt;&gt; Database: Update step start time\n        activate BomUploadProcessingTask\n        Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;StartedAt=&lt;NOW&gt;\n        BomUploadProcessingTask -&gt;&gt; BomUploadProcessingTask: Consume BOM&lt;br/&gt;(Parse, De-dupe)\n        alt Consumption succeeded\n            BomUploadProcessingTask -&gt;&gt; Database: Update step status\n            Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Process BOM&lt;br/&gt;(Insert, Update, Delete)\n            alt Processing succeeded\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.repo-meta-analysis.component\" topic\n            else Processing failed\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=VULN_ANALYSIS&lt;br/&gt;Status=CANCELLED\n            end\n        else Consumption failed\n            BomUploadProcessingTask -&gt;&gt; Database: Update step Status\n            Note over BomUploadProcessingTask, Database: Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=CANCELLED\n        end\n        deactivate BomUploadProcessingTask\n    end\n</code></pre> <p>When triggering a re-analysis of an already existing project, the workflow may be kicked off as follows:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ FindingResource: Re-Analyze project\n    FindingResource -&gt;&gt; FindingResource: Generate Correlation ID (UUID)\n    FindingResource -&gt;&gt; Database: Create workflow steps\n    Note over FindingResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=POLICY_EVALUATION&lt;br/&gt;Status=PENDING\n    FindingResource -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n    FindingResource -&gt;&gt;- Client: Correlation ID\n    loop Continuously\n        Client -&gt;&gt; FindingResource: Poll Status\n        Note over Client, FindingResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n    end\n</code></pre>"},{"location":"design/01-workflow-state-tracking/#proposed-table-schema","title":"Proposed Table Schema","text":"<p>Each step of the workflow will be represented in a dedicated row. This allows us to add or remove steps without altering the database schema (see original version of the schema further down below), or even add steps while the workflow is running. It also plays better with concurrent writes, as no two threads / instances will need to modify the same row.</p> Name Type Nullable Example ID <code>SERIAL</code> \u274c 1 PARENT_STEP_ID <code>SERIAL FK</code> \u2705 0 TOKEN <code>VARCHAR(36)</code> \u274c <code>484d9eaa-7ea4-4476-97d6-f36327b5a626</code> STARTED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> UPDATED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> STEP <code>VARCHAR(64)</code> \u274c <code>METRICS_UPDATE</code> STATUS <code>VARCHAR(64)</code> \u274c <code>PENDING</code> FAILURE_REASON <code>TEXT</code> \u2705 <code>Failed to acquire database connection</code> <p>Potential Future Improvements: * Do we need/want to capture the order in which steps are supposed to be executed? * Do we need/want to capture metadata of the overall workflow (who triggered it, when was it triggered, correlation id, ...)?</p> Original Version  | Name | Type | Nullable | Example | | :--- | :--- | :---: | :--- | | TOKEN | `VARCHAR(36)` | \u274c | `484d9eaa-7ea4-4476-97d6-f36327b5a626` | | STARTED_AT | `TIMESTAMP` | \u274c | `1999-01-08 04:05:06` | | UPDATED_AT | `TIMESTAMP` | \u274c | `1999-01-08 04:05:06` | | BOM_CONSUMPTION | `VARCHAR(64)` | \u274c | `PENDING` | | BOM_PROCESSING | `VARCHAR(64)` | \u274c | `PENDING` | | VULN_ANALYSIS | `VARCHAR(64)` | \u274c | `PENDING` | | REPO_META_ANALYSIS | `VARCHAR(64)` | \u274c | `PENDING` | | POLICY_EVALUATION | `VARCHAR(64)` | \u274c | `PENDING` | | METRICS_UPDATE | `VARCHAR(64)` | \u274c | `PENDING` | | FAILURE_REASON | `TEXT` | \u2705 | - |  `FAILURE_REASON` is a field of unlimited length. It either holds no value (`NULL`), or a JSON object listing failure details per step, e.g.:  <pre><code>{\n\"BOM_PROCESSING\": \"Failed to acquire database connection\"\n}\n</code></pre> <p>Where applicable, the \"detailed\" status of a step is tracked in a dedicated table. For example, for vulnerability analysis:</p> <p>https://github.com/DependencyTrack/hyades/blob/e70c47fff359df1fb8150805d5b0c2acebe85ed3/commons-persistence/src/main/resources/migrations/postgres/V0.0.1__API-Server-4.8.2.sql#L158-L171</p>"},{"location":"design/01-workflow-state-tracking/#retention","title":"Retention","text":"<p>Rows in the table should be cleaned up on a recurring basis.</p> <p>This could be as simple as scheduling a job that executes this SQL query:</p> <pre><code>DELETE FROM org.dependencytrack.model.WorkflowStep WHERE this.updatedAt &lt; :threshold\n</code></pre> <p>A retention time of 1-3 days since the last update should be reasonable.</p>"},{"location":"design/01-workflow-state-tracking/#rest-api-endpoints","title":"REST API endpoints","text":"<p>The existing endpoint to check whether a BOM is still being processed should continue to work as expected:</p> <p><pre><code>GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;\n</code></pre> <pre><code>{\n\"processing\": true\n}\n</code></pre></p> <p>An additional endpoint may be added, which allows for retrieval of the individual step states:</p> <p><pre><code>GET /api/v1/workflow/token/&lt;CORRELATION_ID&gt;/status\n</code></pre> <pre><code>[\n{\n\"step\": \"BOM_CONSUMPTION\",\n\"status\": \"COMPLETED\",\n\"startedAt\": \"1999-01-08 04:05:06\",\n\"updatedAt\": \"1999-01-08 04:05:06\"\n},\n{\n\"step\": \"BOM_PROCESSING\",\n\"status\": \"FAILED\",\n\"startedAt\": \"1999-01-08 04:05:06\",\n\"updatedAt\": \"1999-01-08 04:05:06\",\n\"failureReason\": \"Failed to acquire database connection\"\n},\n{\n\"step\": \"VULN_ANALYSIS\",\n\"status\": \"CANCELLED\"\n}\n]\n</code></pre></p> <p>If all a client cares about are vulnerability analysis results, they could stop polling immediately after <code>vulnerabilityAnalysis</code> transitions into the <code>COMPLETED</code> state.</p>"},{"location":"repository-meta-analyzer/","title":"Repository Metadata Analyzer","text":"<p>The repository metadata analyzer is responsible for fetching metadata about packages from remote repositories.</p> <p>In contrast to the vulnerability analyzer, it exclusively supports the package URL component identifier. CPE and SWID tag IDs are not supported.</p>"},{"location":"repository-meta-analyzer/#how-it-works","title":"How it works","text":"<p>Note The repository metadata analyzer's API is defined using Protocol Buffers. The respective protocol definitions can be found here: * <code>repo-meta-analysis_v1.proto</code></p> <p>An analysis can be triggered by emitting an <code>AnalysisCommand</code> event to the  <code>dtrack.repo-meta-analysis.component</code> topic. The event key can be of arbitrary format; it doesn't matter. </p> <p>The event value (<code>AnalysisCommand</code>) must contain a component with a valid package URL. Events without valid PURL will be silently dropped. For components internal to the organization,  the <code>internal</code> field may be set to indicate the same. Internal components will not be looked up in public repositories. Refer to https://docs.dependencytrack.org/datasources/internal-components/ for details.</p> <p>In practice (and translated to JSON for readability), a valid <code>AnalysisCommand</code> may end up looking like this:</p> <pre><code>{\n\"component\": {\n\"purl\": \"pkg:maven/foo/bar@1.2.3\",\n\"internal\": true\n}\n}\n</code></pre> <p><code>AnalysisCommand</code>s are re-keyed to package URLs, excluding the version. For example, and event with PURL <code>pkg:maven/foo/bar@1.2.3</code> will be re-keyed to <code>pkg:maven/foo/bar</code>. This is done to ensure that all events referring to the same package end up in the same topic partition.</p> <p>Each <code>AnalysisCommand</code> is checked for whether an applicable repository analyzer is available. There are repository analyzers for each package ecosystem, e.g. Ruby Gems, Maven, and Go Modules. In case no applicable analyzer is available, an \"empty\" <code>AnalysisResult</code> event is published to the <code>dtrack.repo-meta-analysis.result</code> topic. Such \"empty\" results only contain the component of the respective <code>AnalysisCommand</code>.</p> <p><pre><code>pkg:maven/foo/bar\n</code></pre> <pre><code>{\n\"component\": {\n\"purl\": \"pkg:maven/foo/bar@1.2.3\",\n\"internal\": true\n}\n}\n</code></pre></p> <p>In case an applicable analyzer is found, the component is analyzed with it. If the component was found in one of the configured repositories, an <code>AnalysisResult</code> like the following is published to the <code>dtrack.repo-meta-analysis.result</code> topic.</p> <p><pre><code>pkg:maven/foo/bar\n</code></pre> <pre><code>{\n\"component\": {\n\"purl\": \"pkg:maven/foo/bar@1.2.3\",\n\"internal\": true\n},\n\"repository\": \"acme-nexus-rm\",\n\"latestVersion\": \"2.1.0\",\n\"published\": 1664395172\n}\n</code></pre></p> <p>If none of the configured repositories contain the requested component, an \"empty\" event is published instead.</p> <p>The key of events in the <code>dtrack.repo-meta-analysis.result</code> remains the package URL without version. Consumers can thus safely rely on Kafka's ordering guarantees without having to worry about race conditions when processing results.</p>"},{"location":"repository-meta-analyzer/#streams-topology","title":"Streams topology","text":"<p>The repository metadata analyzer is implemented as Kafka Streams application. As such,  it is possible to generate a diagram of the topology that every single event processed by  application will be funnelled through.</p> <p></p>"},{"location":"vulnerability-analyzer/","title":"Vulnerability Analyzer","text":"<p>The vulnerability analyzer is responsible for scanning components for known vulnerabilities.</p>"},{"location":"vulnerability-analyzer/#how-it-works","title":"How it works","text":"<p>Note The vulnerability analyzer's API is defined using Protocol Buffers. The respective protocol definitions can be found here: * <code>vuln-analysis_v1.proto</code> * <code>vuln_v1.proto</code></p> <p>A scan can be triggered by emitting a <code>ScanCommand</code> event to the <code>dtrack.vuln-analysis.component</code> topic. The event key (<code>ScanKey</code>) is a composite key, consisting of a scan token and a component UUID, where:</p> <ul> <li>Scan token is an arbitrary string used to correlate one or more scans with each other</li> <li>Component UUID is the UUID of the to-be-scanned component in the API server's database</li> </ul> <p>In practice (and translated to JSON for readability), a valid <code>ScanKey</code> may end up looking like this:</p> <pre><code>{\n\"scan_token\": \"6cb18e5f-518b-44bc-a042-ba3794ba0e6e\",\n\"component_uuid\": \"848f1dba-08bb-40dc-8bf8-354d9fe8019c\"\n}\n</code></pre> <p>The event value (<code>ScanCommand</code>) must contain all necessary information for identifying the component that shall be scanned. At the very least, it should include:</p> <ul> <li>The component's UUID</li> <li>The component's CPE and / or PURL</li> </ul> <p>A minimal <code>ScanCommand</code> would be:</p> <pre><code>{\n\"component\": {\n\"uuid\": \"848f1dba-08bb-40dc-8bf8-354d9fe8019c\",\n\"purl\": \"pkg:maven/foo/bar@1.2.3\"\n}\n}\n</code></pre> <p>Internally, a <code>ScanTask</code> (see <code>vuln-analysis-internal_v1.proto</code>)  will be generated for each scanner that is both:</p> <ol> <li>Enabled (see [<code>CONFIGURATION.md</code>])</li> <li>Capable of scanning the component</li> </ol> <p>Note Whether a scanner is capable scanning a given component primarily depends on the component's identifiers.  While most scanners are capable of dealing with PURLs, only the internal analyzer is capable of handling CPEs.</p> <p><code>ScanTask</code>s are re-keyed to the \"primary\" identifier of the component. If a PURL is available, the coordinates (type, namespace, name, and version, but excluding qualifiers and subpaths) of it will be used. Alternatively,  CPE or UUID will be used. This re-key operation is performed to ensure that tasks for the same component identity are published to the same topic partition.</p> <p>Each scan task is then forwarded to the topic of the respective scanner. As the number of partitions is the means of achieving parallelism in Kafka consumers, it is expected that the partition count will differ from scanner to scanner.</p> <p>OSS Index allows for batching of up to 128 PURLs per request, while Snyk requires individual PURLs to be submitted. In (local) testing, requests to OSS Index take about 600-900ms to complete, whereas requests to Snyk take about 200-400ms. In order to achieve a throughput with Snyk that is similar to what is possible with OSS Index, the Snyk topic requires a lot more partitions. The number of partitions are configurable for each scanner, see [<code>CONFIGURATION.md</code>].</p> <p>Scanner results (<code>ScannerResult</code>) are re-keyed back to the <code>ScanKey</code> again, and published to the <code>dtrack.vuln-analysis.scanner.result</code> topic. A <code>ScannerResult</code> is an object composed of the following fields:</p> <ul> <li><code>scanner</code>: The scanner that produced this result (e.g. <code>SCANNER_OSSINDEX</code>)</li> <li><code>status</code>: Status of the scan (e.g. <code>SCAN_STATUS_SUCCESSFUL</code>)</li> <li><code>vulnerabilities</code>: Any vulnerabilities that have been identified</li> <li>When <code>status</code> is <code>SCAN_STATUS_SUCCESSFUL</code></li> <li><code>failureReason</code>: Reason for the failure</li> <li>When <code>status</code> is <code>SCAN_STATUS_FAILED</code></li> </ul> <p>Warning <code>dtrack.vuln-analysis.scanner.result</code> is considered to be an internal topic. Third party applications should not directly consume from it, as there will be no indication of when all applicable scanners have completed for a given <code>ScanCommand</code>.</p> <p>By observing the <code>ScanTask</code>s created, and the <code>ScannerResult</code>s received, the vulnerability analyzer is able  to deduce when the initial <code>ScanCommand</code> has been completed for all capable scanners. Once completion is detected,  a <code>ScanResult</code> event is published to <code>dtrack.vuln-analysis.result</code>. A <code>ScanResult</code> simply is an aggregate of all <code>ScannerResult</code>s.</p> <p>Applications consuming from <code>dtrack.vuln-analysis.result</code> can correlate results with their initial <code>ScanCommand</code> based on the <code>ScanKey</code>.</p> <p>Note Reported vulnerabilities are not de-duplicated. It is the responsibility of the consumer to decide what data source to prefer in case multiple scanners report the same vulnerability. </p>"},{"location":"vulnerability-analyzer/#streams-topology","title":"Streams topology","text":"<p>The vulnerability analyzer is implemented as Kafka Streams application. As such, it is possible to generate a diagram of the topology that every single event processed by application will be funnelled through.</p> <p></p>"}]}