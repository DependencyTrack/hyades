package org.dependencytrack.processor.misc;

import com.google.protobuf.util.Timestamps;
import org.apache.commons.lang3.tuple.Pair;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.processor.Cancellable;
import org.apache.kafka.streams.processor.PunctuationType;
import org.apache.kafka.streams.processor.api.ContextualFixedKeyProcessor;
import org.apache.kafka.streams.processor.api.FixedKeyProcessorContext;
import org.apache.kafka.streams.processor.api.FixedKeyRecord;
import org.apache.kafka.streams.state.KeyValueIterator;
import org.apache.kafka.streams.state.KeyValueStore;
import org.dependencytrack.proto.vulnanalysis.internal.v1beta1.ScannerResultAggregate;
import org.dependencytrack.proto.vulnanalysis.v1.ScanKey;
import org.dependencytrack.proto.vulnanalysis.v1.ScanStatus;
import org.dependencytrack.proto.vulnanalysis.v1.ScannerResult;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.Duration;
import java.time.Instant;
import java.util.HashMap;
import java.util.Map;
import java.util.Optional;
import java.util.stream.Collectors;

import static org.dependencytrack.proto.vulnanalysis.v1.ScanStatus.SCAN_STATUS_PENDING;

/**
 * A {@link ContextualFixedKeyProcessor} that aggregates {@link ScannerResultAggregate}s
 * and automatically removes them from the underlying state store.
 * <p>
 * Removal is triggered by any of the following situations:
 * <ol>
 *     <li>The aggregate is considered to be complete</li>
 *     <li>The aggregate has not been updated for a certain amount of (stream) time</li>
 * </ol>
 * <p>
 * In the former case, the complete aggregate is forwarded to the next processor in the topology.
 * <p>
 * Removal and expiration mechanisms are necessary as aggregates are based on unbounded
 * sets of keys, thus have the potential to grow indefinitely.
 */
public class ScannerResultAggregator extends ContextualFixedKeyProcessor<ScanKey, ScannerResultAggregate, ScannerResultAggregate> {

    private static final Logger LOGGER = LoggerFactory.getLogger(ScannerResultAggregator.class);

    private final String storeName;
    private final Duration checkInterval;
    private final Duration maxLifetime;
    private KeyValueStore<ScanKey, ScannerResultAggregate> store;
    private Cancellable punctuator;

    ScannerResultAggregator(final String storeName, final Duration checkInterval, final Duration maxLifetime) {
        this.storeName = storeName;
        this.checkInterval = checkInterval;
        this.maxLifetime = maxLifetime;
    }

    @Override
    public void init(final FixedKeyProcessorContext<ScanKey, ScannerResultAggregate> context) {
        super.init(context);

        store = context().getStateStore(storeName);
        punctuator = context().schedule(checkInterval, PunctuationType.STREAM_TIME, this::punctuate);
    }

    @Override
    public void process(final FixedKeyRecord<ScanKey, ScannerResultAggregate> record) {
        final ScannerResultAggregate aggregate = doAggregate(store.get(record.key()), record.value());

        if (isComplete(aggregate)) {
            LOGGER.debug("Forwarding completed aggregate for {}", record.key());
            store.delete(record.key());
            context().forward(record.withValue(aggregate));
        } else {
            LOGGER.debug("Updating aggregate for {}", record.key());
            store.put(record.key(), aggregate);
        }
    }

    @Override
    public void close() {
        Optional.ofNullable(punctuator).ifPresent(Cancellable::cancel);
    }

    private void punctuate(final long timestamp) {
        final Instant cutoffTimestamp = Instant.ofEpochMilli(timestamp).minus(maxLifetime);

        try (final KeyValueIterator<ScanKey, ScannerResultAggregate> all = store.all()) {
            while (all.hasNext()) {
                final KeyValue<ScanKey, ScannerResultAggregate> record = all.next();
                final Instant lastUpdated = Instant.ofEpochSecond(record.value.getLastUpdated().getSeconds());

                if (isComplete(record.value)) {
                    LOGGER.debug("Deleting completed aggregate; This is a normal housekeeping activity (token={}, component={})",
                            record.key.getScanToken(), record.key.getComponentUuid());
                    store.delete(record.key);
                } else if (lastUpdated.isBefore(cutoffTimestamp)) {
                    // To give more useful context in the log message, figure out for which scanner we received results,
                    // and for which we did not. This map will have the form of:
                    //   {
                    //     SCANNER_OSSINDEX: SCAN_STATUS_SUCCESSFUL,
                    //     SCANNER_SNYK: SCAN_STATUS_PENDING
                    //   }
                    //
                    // Normally, scanners should always send results, no matter if they succeed or fail.
                    // If this scenario happens, then likely an unhandled error occurred in a scanner, causing
                    // it to drop records, OR the batching logic is faulty.
                    final Map<String, ScanStatus> scanStatusByScanner = record.value.getResultsMap().entrySet().stream()
                            .map(entry -> Pair.of(entry.getKey(), entry.getValue().getStatus()))
                            .collect(Collectors.toMap(Pair::getLeft, Pair::getRight));

                    LOGGER.warn("""
                            Deleting incomplete aggregate from store; At least one applicable scanner failed to send \
                            results in time (token={}, component={}, statuses={}, lastUpdated={}, cutoff={})
                            """, record.key.getScanToken(), record.key.getComponentUuid(), scanStatusByScanner, lastUpdated, cutoffTimestamp);
                    store.delete(record.key);
                }
            }
        }
    }

    private ScannerResultAggregate doAggregate(final ScannerResultAggregate existingAggregate,
                                               final ScannerResultAggregate incomingAggregate) {
        final ScannerResultAggregate.Builder resultAggregateBuilder;
        if (existingAggregate != null) {
            resultAggregateBuilder = ScannerResultAggregate.newBuilder(existingAggregate);
        } else {
            resultAggregateBuilder = ScannerResultAggregate.newBuilder();
        }

        final Map<String, ScannerResult> scannerResults;
        if (resultAggregateBuilder.getResultsCount() > 0) {
            scannerResults = new HashMap<>(resultAggregateBuilder.getResultsMap());
            incomingAggregate.getResultsMap().entrySet().stream()
                    .filter(entry -> entry.getValue().getStatus() != SCAN_STATUS_PENDING)
                    .forEach(entry -> scannerResults.put(entry.getKey(), entry.getValue()));
        } else {
            scannerResults = incomingAggregate.getResultsMap();
        }

        return resultAggregateBuilder
                .setLastUpdated(Timestamps.fromMillis(context().currentStreamTimeMs()))
                .putAllResults(scannerResults)
                .build();
    }

    private static boolean isComplete(final ScannerResultAggregate aggregate) {
        return aggregate.getResultsMap().values().stream()
                .map(ScannerResult::getStatus)
                .noneMatch(SCAN_STATUS_PENDING::equals);
    }

}
