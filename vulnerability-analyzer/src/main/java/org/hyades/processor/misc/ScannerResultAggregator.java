package org.hyades.processor.misc;

import com.google.protobuf.Timestamp;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.processor.Cancellable;
import org.apache.kafka.streams.processor.PunctuationType;
import org.apache.kafka.streams.processor.api.ContextualFixedKeyProcessor;
import org.apache.kafka.streams.processor.api.FixedKeyProcessorContext;
import org.apache.kafka.streams.processor.api.FixedKeyRecord;
import org.apache.kafka.streams.state.KeyValueIterator;
import org.apache.kafka.streams.state.KeyValueStore;
import org.hyades.proto.vulnanalysis.internal.v1beta1.ScannerResultAggregate;
import org.hyades.proto.vulnanalysis.v1.ScanKey;
import org.hyades.proto.vulnanalysis.v1.ScannerResult;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.Duration;
import java.time.Instant;
import java.util.HashMap;
import java.util.Map;
import java.util.Optional;

import static org.hyades.proto.vulnanalysis.v1.ScanStatus.SCAN_STATUS_PENDING;

/**
 * A {@link ContextualFixedKeyProcessor} that aggregates {@link ScannerResultAggregate}s
 * and automatically removes them from the underlying state store.
 * <p>
 * Removal is triggered by any of the following situations:
 * <ol>
 *     <li>The aggregate is considered to be complete</li>
 *     <li>The aggregate has not been updated for a certain amount of (stream) time</li>
 * </ol>
 * <p>
 * In the former case, the complete aggregate is forwarded to the next processor in the topology.
 * <p>
 * Removal and expiration mechanisms are necessary as aggregates are based on unbounded
 * sets of keys, thus have the potential to grow indefinitely.
 */
public class ScannerResultAggregator extends ContextualFixedKeyProcessor<ScanKey, ScannerResultAggregate, ScannerResultAggregate> {

    private static final Logger LOGGER = LoggerFactory.getLogger(ScannerResultAggregator.class);

    private final String storeName;
    private final Duration checkInterval;
    private final Duration maxLifetime;
    private KeyValueStore<ScanKey, ScannerResultAggregate> store;
    private Cancellable punctuator;

    ScannerResultAggregator(final String storeName, final Duration checkInterval, final Duration maxLifetime) {
        this.storeName = storeName;
        this.checkInterval = checkInterval;
        this.maxLifetime = maxLifetime;
    }

    @Override
    public void init(final FixedKeyProcessorContext<ScanKey, ScannerResultAggregate> context) {
        super.init(context);

        store = context().getStateStore(storeName);
        punctuator = context().schedule(checkInterval, PunctuationType.STREAM_TIME, this::punctuate);
    }

    @Override
    public void process(final FixedKeyRecord<ScanKey, ScannerResultAggregate> record) {
        final ScannerResultAggregate aggregate = doAggregate(store.get(record.key()), record.value());

        if (isComplete(aggregate)) {
            LOGGER.debug("Forwarding completed aggregate for {}", record.key());
            store.delete(record.key());
            context().forward(record.withValue(aggregate));
        } else {
            LOGGER.debug("Updating aggregate for {}", record.key());
            store.put(record.key(), aggregate);
        }
    }

    @Override
    public void close() {
        Optional.ofNullable(punctuator).ifPresent(Cancellable::cancel);
    }

    private void punctuate(final long timestamp) {
        final Instant cutoffTimestamp = Instant.ofEpochMilli(timestamp).minus(maxLifetime);

        try (final KeyValueIterator<ScanKey, ScannerResultAggregate> all = store.all()) {
            while (all.hasNext()) {
                final KeyValue<ScanKey, ScannerResultAggregate> record = all.next();
                final Instant lastUpdated = Instant.ofEpochSecond(record.value.getLastUpdated().getSeconds());

                if (isComplete(record.value) || lastUpdated.isBefore(cutoffTimestamp)) {
                    LOGGER.debug("Deleting expired aggregate for {}", record.key);
                    store.delete(record.key);
                }
            }
        }
    }

    private ScannerResultAggregate doAggregate(final ScannerResultAggregate existingAggregate,
                                               final ScannerResultAggregate incomingAggregate) {
        final ScannerResultAggregate.Builder resultAggregateBuilder;
        if (existingAggregate != null) {
            resultAggregateBuilder = ScannerResultAggregate.newBuilder(existingAggregate);
        } else {
            resultAggregateBuilder = ScannerResultAggregate.newBuilder();
        }

        final Map<String, ScannerResult> scannerResults;
        if (resultAggregateBuilder.getResultsCount() > 0) {
            scannerResults = new HashMap<>(resultAggregateBuilder.getResultsMap());
            incomingAggregate.getResultsMap().entrySet().stream()
                    .filter(entry -> entry.getValue().getStatus() != SCAN_STATUS_PENDING)
                    .forEach(entry -> scannerResults.put(entry.getKey(), entry.getValue()));
        } else {
            scannerResults = incomingAggregate.getResultsMap();
        }

        return resultAggregateBuilder
                .setLastUpdated(currentStreamTimestamp())
                .putAllResults(scannerResults)
                .build();
    }

    private Timestamp currentStreamTimestamp() {
        return Timestamp.newBuilder()
                .setSeconds(context().currentStreamTimeMs() / 1000)
                .build();
    }

    private static boolean isComplete(final ScannerResultAggregate aggregate) {
        return aggregate.getResultsMap().values().stream()
                .map(ScannerResult::getStatus)
                .noneMatch(SCAN_STATUS_PENDING::equals);
    }

}
