package org.hyades.processor.retry;

import io.github.resilience4j.core.IntervalFunction;
import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.DistributionSummary;
import io.micrometer.core.instrument.Gauge;
import io.micrometer.core.instrument.Meter;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Tag;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.processor.Cancellable;
import org.apache.kafka.streams.processor.PunctuationType;
import org.apache.kafka.streams.processor.api.ProcessorContext;
import org.apache.kafka.streams.state.KeyValueIterator;
import org.apache.kafka.streams.state.KeyValueStore;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.concurrent.atomic.AtomicInteger;

public abstract class RetryingBatchProcessor<KI, VI, KO, VO, KR> extends RetryingProcessor<KI, VI, KO, VO, KR> {

    protected KeyValueStore<KR, RetryableRecord<KI, VI>> batchStore;
    private final String batchStoreName;
    private final Duration batchInterval;
    private final MeterRegistry meterRegistry;
    private Cancellable batchPunctuator;
    private Gauge batchStoreEntriesGauge;
    private DistributionSummary batchSizeDistribution;
    private Counter.Builder componentsScannedCounterBuilder;
    private Instant lastBatchAnalysis;
    private final int batchSize;
    private final Logger logger;

    protected RetryingBatchProcessor(final String storeName, final IntervalFunction intervalFunction,
                                     final int maxAttempts, final MeterRegistry meterRegistry,
                                     final String batchStoreName, final Duration batchInterval,
                                     final int batchSize) {
        super(storeName, intervalFunction, maxAttempts, meterRegistry);
        this.meterRegistry = meterRegistry;
        this.batchStoreName = batchStoreName;
        this.batchInterval = batchInterval;
        this.batchSize = batchSize;
        this.logger = LoggerFactory.getLogger(getClass());
    }

    @Override
    public void init(final ProcessorContext<KO, VO> context) {
        super.init(context);

        batchStore = context().getStateStore(batchStoreName);
        batchPunctuator = context().schedule(batchInterval, PunctuationType.WALL_CLOCK_TIME, this::punctuateBatch);

        final Tag threadIdMeterTag = Tag.of("thread_id", Thread.currentThread().getName());
        final Tag taskIdMeterTag = Tag.of("task_id", context().taskId().toString());
        final Tag processorMeterTag = Tag.of("processor", getClass().getSimpleName());
        final Tag scannerMeterTag = Tag.of("scanner", getClass().getSimpleName());

        batchStoreEntriesGauge = Gauge.builder("kafka.stream.store.entries", batchStore::approximateNumEntries)
                .description("Total number of entries in the batch state store")
                .tags(Set.of(threadIdMeterTag, taskIdMeterTag, processorMeterTag, Tag.of("store", batchStoreName)))
                .register(meterRegistry);
        batchSizeDistribution = DistributionSummary.builder("scanner.components.batch.size")
                .tags(Set.of(threadIdMeterTag, taskIdMeterTag, scannerMeterTag))
                .register(meterRegistry);
        componentsScannedCounterBuilder = Counter.builder("scanner.components.scanned")
                .description("Total number of scanned components")
                .tags(Set.of(threadIdMeterTag, taskIdMeterTag, scannerMeterTag));

        lastBatchAnalysis = Instant.now();
    }

    protected List<RetryableRecord<KI, VI>> currentBatch() {
        AtomicInteger counter = new AtomicInteger(0);
        final var batch = new ArrayList<RetryableRecord<KI, VI>>();

        try (final KeyValueIterator<KR, RetryableRecord<KI, VI>> valueIterator = batchStore.all()) {
            while (valueIterator.hasNext() && counter.incrementAndGet() <= batchSize) {
                final KeyValue<KR, RetryableRecord<KI, VI>> keyValue = valueIterator.next();
                if (keyValue != null) {
                    final RetryableRecord<KI, VI> record = keyValue.value;
                    batch.add(record);
                    batchStore.delete(keyValue.key);
                }
            }
        }
        return batch;
    }

    @Override
    public void close() {
        super.close();

        Optional.ofNullable(batchPunctuator).ifPresent(Cancellable::cancel);
        Optional.ofNullable(batchStoreEntriesGauge).ifPresent(Meter::close);
    }

    private void punctuateBatch(final long timestamp) {
        if (Instant.ofEpochMilli(timestamp).isBefore(lastBatchAnalysis.plusSeconds(5))) {
            logger.debug("Current batch is not yet due for analysis (lastBatchAnalysis={})", lastBatchAnalysis);
            return;
        }

        final var batch = currentBatch();
        if (batch.isEmpty()) {
            // Note: DO NOT use batchStore#approximateNumEntries to check the batch store size,
            // it will report 0 even though there are entries in the store when RocksDB is used.
            // It is NOT reliable, the only reliable way to determine the store size is to actually
            // read from it.
            logger.debug("Current batch is empty");
            return;
        }

        analyzeBatch(batch);
    }

    protected void analyzeBatch(final List<RetryableRecord<KI, VI>> batch) {
        batchSizeDistribution.record(batch.size());
        lastBatchAnalysis = Instant.ofEpochMilli(context().currentSystemTimeMs());
    }

    protected abstract void addToBatch(final RetryableRecord<KI, VI> record);

    protected void reportScanResult(final String result) {
        componentsScannedCounterBuilder
                .tag("result", result)
                .register(meterRegistry)
                .increment();
    }
}
