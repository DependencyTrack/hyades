package org.hyades.processor.scanner.ossindex;

import com.github.packageurl.PackageURL;
import io.github.resilience4j.circuitbreaker.CircuitBreaker;
import io.github.resilience4j.core.IntervalFunction;
import io.micrometer.core.instrument.MeterRegistry;
import io.quarkus.cache.Cache;
import jakarta.ws.rs.core.MultivaluedHashMap;
import jakarta.ws.rs.core.MultivaluedMap;
import org.apache.kafka.streams.processor.api.Processor;
import org.cyclonedx.proto.v1_4.Bom;
import org.hyades.client.ossindex.ComponentReport;
import org.hyades.client.ossindex.ComponentReportRequest;
import org.hyades.client.ossindex.ModelConverterToCdx;
import org.hyades.client.ossindex.OssIndexClient;
import org.hyades.config.OssIndexConfig;
import org.hyades.processor.retry.RetryStatus;
import org.hyades.processor.retry.RetryableRecord;
import org.hyades.processor.retry.RetryingBatchProcessor;
import org.hyades.proto.vulnanalysis.internal.v1beta1.ScanTask;
import org.hyades.proto.vulnanalysis.v1.Component;
import org.hyades.proto.vulnanalysis.v1.ScanKey;
import org.hyades.proto.vulnanalysis.v1.ScanStatus;
import org.hyades.proto.vulnanalysis.v1.Scanner;
import org.hyades.proto.vulnanalysis.v1.ScannerResult;
import org.hyades.util.ProcessorUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.Duration;
import java.util.List;
import java.util.Map;
import java.util.NoSuchElementException;
import java.util.Optional;

/**
 * A {@link Processor} for analyzing {@link Component}s with Sonatype OSS Index.
 */
public class OssIndexProcessor extends RetryingBatchProcessor<String, ScanTask, ScanKey, ScannerResult, ScanKey> {

    private static final Logger LOGGER = LoggerFactory.getLogger(OssIndexProcessor.class);

    private final OssIndexClient client;
    private final Cache cache;
    private final CircuitBreaker circuitBreaker;

    private final OssIndexConfig config;

    OssIndexProcessor(final OssIndexClient client, final Cache cache, final CircuitBreaker circuitBreaker,
                      final String batchStoreName, final Duration batchInterval,
                      final String retryStoreName, final IntervalFunction retryIntervalFunction,
                      final int retryMaxAttempts, final MeterRegistry meterRegistry, OssIndexConfig config) {
        super(retryStoreName, retryIntervalFunction, retryMaxAttempts, meterRegistry, batchStoreName, batchInterval, config.batchSize());
        this.client = client;
        this.cache = cache;
        this.circuitBreaker = circuitBreaker;
        this.config = config;
    }

    @Override
    public void process(final RetryableRecord<String, ScanTask> record) {
        addToBatch(record);
    }

    @Override
    protected ScanKey retryKey(final String key, final ScanTask value) {
        return value.getKey();
    }

    @Override
    protected void onRetry(final RetryableRecord<String, ScanTask> record) {
        addToBatch(record);
    }

    @Override
    protected void onMaxRetriesExceeded(final RetryableRecord<String, ScanTask> record) {
        reportFailure(record, new RuntimeException("Max retry attempts exceeded"));
    }

    @Override
    public void addToBatch(final RetryableRecord<String, ScanTask> record) {
        final Optional<ComponentReport> cachedReport = getCachedReport(record.value().getComponent());
        if (cachedReport.isPresent()) {
            LOGGER.debug("Processing cached report for {}", record);
            processReport(cachedReport.get(), List.of(record));
            return;
        }

        if (batchStore.get(record.value().getKey()) == null) {
            batchStore.put(record.value().getKey(), record);
        } else {
            LOGGER.warn("Record {} is already in the current batch; Dropping", record);
            return;
        }

        // Note: approximateNumEntries may over-report or under-report when RocksDB is used.
        // It is possible that this condition will be true, even though the store contains
        // more or less entries than config.batchSize.
        // This is an acceptable trade-off here, because it's still a better option than reading
        // entries from the store too often (as it involves de-serializing all of them).
        if (batchStore.approximateNumEntries() >= config.batchSize()) {
            final List<RetryableRecord<String, ScanTask>> batch = currentBatch();
            analyzeBatch(batch);
        }
    }

    @Override
    public void analyzeBatch(final List<RetryableRecord<String, ScanTask>> batch) {
        super.analyzeBatch(batch);

        final MultivaluedMap<String, RetryableRecord<String, ScanTask>> purlRecords = new MultivaluedHashMap<>();
        for (final RetryableRecord<String, ScanTask> record : batch) {
            purlRecords.add(record.key(), record);
        }

        final List<ComponentReport> reports;
        try {
            reports = circuitBreaker.executeSupplier(() ->
                    client.getComponentReports(new ComponentReportRequest(purlRecords.keySet())));
            batch.forEach(record -> reportRetryStatus(record, RetryStatus.SUCCEEDED));
        } catch (Throwable e) {
            if (ProcessorUtils.isRetryable(e)) {
                batch.forEach(record -> {
                    LOGGER.debug("Encountered retryable exception while analyzing {}: {}", record, e.getMessage());
                    scheduleForRetry(record);
                });
            } else {
                batch.forEach(record -> {
                    LOGGER.error("Encountered non-retryable exception while analyzing {}", record, e);
                    reportFailure(record, e);
                });
            }
            return;
        }

        for (final ComponentReport report : reports) {
            final List<RetryableRecord<String, ScanTask>> affectedRecords = purlRecords.get(report.coordinates());
            if (affectedRecords == null) {
                LOGGER.warn("Reported coordinates do not match any records: " + report.coordinates());
                continue;
            } else {
                purlRecords.remove(report.coordinates());
            }

            processReport(report, affectedRecords);
            cacheReport(report);
        }

        for (final Map.Entry<String, List<RetryableRecord<String, ScanTask>>> unmatchedRecords : purlRecords.entrySet()) {
            LOGGER.warn("No results were returned for coordinates {} (affecting {}/{} records in this batch)",
                    unmatchedRecords.getKey(), unmatchedRecords.getValue().size(), batch.size());
            // TODO: Should we cache "no vulnerabilities" for this case, too?
            for (final RetryableRecord<String, ScanTask> record : unmatchedRecords.getValue()) {
                final ScannerResult result = ScannerResult.newBuilder()
                        .setScanner(Scanner.SCANNER_OSSINDEX)
                        .setStatus(ScanStatus.SCAN_STATUS_SUCCESSFUL)
                        .build();
                context().forward(record.withKey(record.value().getKey()).withValue(result).withTimestamp(context().currentSystemTimeMs()));
                reportScanResult("not_vulnerable");
            }
        }
    }

    private Optional<ComponentReport> getCachedReport(final Component component) {
        try {
            final ComponentReport cachedReport = cache.<String, ComponentReport>get(new PackageURL(component.getPurl()).getCoordinates(),
                    key -> {
                        // null values would be cached, so throw an exception instead.
                        // See https://quarkus.io/guides/cache#let-exceptions-bubble-up
                        throw new NoSuchElementException();
                    }).await().indefinitely();
            return Optional.of(cachedReport);
        } catch (Exception e) {
            return Optional.empty();
        }
    }

    private void cacheReport(final ComponentReport report) {
        cache.get(report.coordinates(), key -> report).await().indefinitely();
    }

    private void processReport(final ComponentReport report, List<RetryableRecord<String, ScanTask>> affectedRecords) {

        final Bom bov = ModelConverterToCdx.convert(report.vulnerabilities(), config.aliasSyncEnabled());
        for (final RetryableRecord<String, ScanTask> record : affectedRecords) {
            final var result = ScannerResult.newBuilder()
                    .setScanner(Scanner.SCANNER_OSSINDEX)
                    .setStatus(ScanStatus.SCAN_STATUS_SUCCESSFUL)
                    .setBom(bov)
                    .build();
            context().forward(record.withKey(record.value().getKey()).withValue(result).withTimestamp(context().currentSystemTimeMs()));
            reportScanResult(bov.getVulnerabilitiesList().isEmpty() ? "not_vulnerable" : "vulnerable");
        }
    }

    private void reportFailure(final RetryableRecord<String, ScanTask> record, final Throwable failureCause) {
        final var result = ScannerResult.newBuilder()
                .setScanner(Scanner.SCANNER_OSSINDEX)
                .setStatus(ScanStatus.SCAN_STATUS_FAILED)
                .setFailureReason(failureCause.getMessage())
                .build();
        context().forward(record.withKey(record.value().getKey()).withValue(result).withTimestamp(context().currentSystemTimeMs()));
        reportRetryStatus(record, RetryStatus.FAILED);
        reportScanResult("failed");
    }
}
