package org.acme.processor;

import io.github.resilience4j.circuitbreaker.CallNotPermittedException;
import io.github.resilience4j.circuitbreaker.CircuitBreaker;
import io.github.resilience4j.core.IntervalFunction;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Tag;
import org.acme.client.ossindex.ComponentReport;
import org.acme.client.ossindex.ComponentReportRequest;
import org.acme.client.ossindex.ComponentReportVulnerability;
import org.acme.client.ossindex.ModelConverter;
import org.acme.client.ossindex.OssIndexClient;
import org.acme.model.AnalyzerIdentity;
import org.acme.model.Component;
import org.acme.model.Vulnerability;
import org.acme.model.VulnerabilityResult;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.processor.Cancellable;
import org.apache.kafka.streams.processor.PunctuationType;
import org.apache.kafka.streams.processor.api.ContextualProcessor;
import org.apache.kafka.streams.processor.api.ProcessorContext;
import org.apache.kafka.streams.processor.api.Record;
import org.apache.kafka.streams.processor.api.RecordMetadata;
import org.apache.kafka.streams.state.KeyValueIterator;
import org.apache.kafka.streams.state.KeyValueStore;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MultivaluedHashMap;
import javax.ws.rs.core.MultivaluedMap;
import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.UUID;

public class OssIndexProcessor extends ContextualProcessor<String, Component, String, VulnerabilityResult> {

    private static final Logger LOGGER = LoggerFactory.getLogger(OssIndexProcessor.class);
    private static final String PARTITION_ID_PROPERTY = "partitionId";

    private final String batchStoreName;
    private final String retryStoreName;
    private final OssIndexClient client;
    private final CircuitBreaker circuitBreaker;
    private final IntervalFunction retryIntervalFunction;
    private final int retryMaxAttempts;
    private final MeterRegistry meterRegistry;
    private final Map<Integer, Long> lastBatchAnalysis;
    private KeyValueStore<Integer, List<RetryableRecord<String, Component>>> batchStore;
    private KeyValueStore<UUID, RetryableRecord<String, Component>> retryStore;
    private Cancellable batchPunctuator;
    private Cancellable retryPunctuator;

    OssIndexProcessor(final String batchStoreName, final String retryStoreName,
                      final OssIndexClient client, final CircuitBreaker circuitBreaker,
                      final IntervalFunction retryIntervalFunction, final int retryMaxAttempts,
                      final MeterRegistry meterRegistry) {
        this.batchStoreName = batchStoreName;
        this.retryStoreName = retryStoreName;
        this.lastBatchAnalysis = new HashMap<>();
        this.client = client;
        this.circuitBreaker = circuitBreaker;
        this.retryIntervalFunction = retryIntervalFunction;
        this.retryMaxAttempts = retryMaxAttempts;
        this.meterRegistry = meterRegistry;
    }

    @Override
    public void init(final ProcessorContext<String, VulnerabilityResult> context) {
        super.init(context);

        batchStore = context().getStateStore(batchStoreName);
        retryStore = context().getStateStore(retryStoreName);

        batchPunctuator = context().schedule(Duration.ofSeconds(5), PunctuationType.WALL_CLOCK_TIME, this::punctuateBatch);
        retryPunctuator = context().schedule(Duration.ofSeconds(1), PunctuationType.WALL_CLOCK_TIME, this::punctuateRetry);
    }

    @Override
    public void process(final Record<String, Component> record) {
        RetryableRecord<String, Component> retryableRecord = retryStore.get(record.value().getUuid());
        if (retryableRecord != null) {
            LOGGER.warn("Component {} is already scheduled for retry at {}; Dropping", record.value(), retryableRecord.nextRetryAt());
            meterRegistry.counter("ossindex.records.dropped", List.of(Tag.of("reason", "alreadyRetrying"))).increment();
            // Do not emit an event here in order to avoid duplicate events
            return;
        } else {
            retryableRecord = RetryableRecord.fromRecord(record);
        }

        // Records are batched based on the ID of the partition they're in.
        // It's unlikely that we can't access the partition ID, but better be on the safe side.
        final Optional<Integer> partitionId = context().recordMetadata().map(RecordMetadata::partition);
        if (partitionId.isEmpty()) {
            LOGGER.error("Unable to get partition ID for record; Dropping");
            meterRegistry.counter("ossindex.records.dropped", List.of(Tag.of("reason", "partitionIdMissing"))).increment();
            reportFailure(retryableRecord, new RuntimeException("Record is missing the partitionId property"));
            return;
        }

        meterRegistry.counter("ossindex.records.consumed").increment();

        addToBatch(retryableRecord.withProperties(Map.of(PARTITION_ID_PROPERTY, partitionId.get())));
    }

    @Override
    public void close() {
        Optional.ofNullable(batchPunctuator).ifPresent(Cancellable::cancel);
        Optional.ofNullable(retryPunctuator).ifPresent(Cancellable::cancel);
    }

    private void punctuateBatch(final long timestamp) {
        try (final KeyValueIterator<Integer, List<RetryableRecord<String, Component>>> valueIterator = batchStore.all()) {
            while (valueIterator.hasNext()) {
                final KeyValue<Integer, List<RetryableRecord<String, Component>>> keyValue = valueIterator.next();
                if (keyValue != null && !keyValue.value.isEmpty()) {
                    final Long lastAnalysis = lastBatchAnalysis.get(keyValue.key);
                    if (lastAnalysis != null && (context().currentSystemTimeMs() - lastAnalysis) < Duration.ofSeconds(5).toMillis()) {
                        LOGGER.warn("Batch for partition {} is not yet due for submission", keyValue.key);
                        continue;
                    }

                    LOGGER.info("Forwarding batch (key: {}, records: {})", keyValue.key, keyValue.value.size());
                    batchStore.delete(keyValue.key);
                    lastBatchAnalysis.put(keyValue.key, context().currentSystemTimeMs());
                    analyzeBatch(keyValue.value);
                }
            }
        }
    }

    private void punctuateRetry(final long timestamp) {
        try (final KeyValueIterator<UUID, RetryableRecord<String, Component>> valueIterator = retryStore.all()) {
            while (valueIterator.hasNext()) {
                final KeyValue<UUID, RetryableRecord<String, Component>> keyValue = valueIterator.next();
                if (keyValue != null) {
                    final RetryableRecord<String, Component> record = keyValue.value;
                    if (record.nextRetryAt() <= timestamp) {
                        LOGGER.info("Retrying component {}", record.value());
                        retryStore.delete(keyValue.key);
                        addToBatch(record);
                    }
                }
            }
        }
    }

    private void addToBatch(final RetryableRecord<String, Component> record) {
        final Optional<Integer> partitionId = record.getPropertyAsInteger(PARTITION_ID_PROPERTY);
        if (partitionId.isEmpty()) {
            LOGGER.error("Record is missing the partitionId property; Dropping");
            meterRegistry.counter("ossindex.records.dropped", List.of(Tag.of("reason", "partitionIdMissing"))).increment();
            reportFailure(record, new RuntimeException("Record is missing the partitionId property"));
            return;
        }

        List<RetryableRecord<String, Component>> batch = batchStore.get(partitionId.get());
        if (batch == null) {
            batch = new ArrayList<>(List.of(record));
        } else {
            batch.add(record);
        }

        if (batch.size() >= 128) {
            batchStore.delete(partitionId.get());
            lastBatchAnalysis.put(partitionId.get(), context().currentSystemTimeMs());
            analyzeBatch(batch);
        } else {
            batchStore.put(partitionId.get(), batch);
        }
    }

    private void analyzeBatch(final List<RetryableRecord<String, Component>> batch) {
        LOGGER.info("Analyzing batch of {} records", batch.size());

        final MultivaluedMap<String, RetryableRecord<String, Component>> purlRecords = new MultivaluedHashMap<>();
        for (final RetryableRecord<String, Component> record : batch) {
            purlRecords.add(record.key(), record);
        }

        final List<ComponentReport> reports;
        try {
            reports = circuitBreaker.executeSupplier(() -> client.getComponentReports(new ComponentReportRequest(purlRecords.keySet())));
        } catch (WebApplicationException e) {
            switch (e.getResponse().getStatus()) {
                case 429, 502, 503, 504 -> {
                    LOGGER.warn("Encountered a retryable error with status {} while analyzing batch of {} records",
                            e.getResponse().getStatus(), batch.size());
                    batch.forEach(this::scheduleForRetry);
                }
                default -> {
                    LOGGER.error("Analysis of batch with {} records failed: {}", batch.size(), e);
                    meterRegistry.counter("ossindex.retry", List.of(Tag.of("status", "failedWithoutRetry"))).increment();
                    meterRegistry.counter("ossindex.records.dropped", List.of(Tag.of("reason", "failed"))).increment();
                    batch.forEach(record -> reportFailure(record, e));
                }
            }
            return;
        } catch (CallNotPermittedException e) {
            LOGGER.warn("Couldn't analyze batch of {} records because circuit breaker is open", batch.size());
            batch.forEach(this::scheduleForRetry);
            return;
        } catch (Exception e) {
            LOGGER.error("Analysis of batch with {} records failed due to an unexpected error", batch.size(), e);
            meterRegistry.counter("ossindex.records.dropped", List.of(Tag.of("reason", "unexpectedError"))).increment(batch.size());
            batch.forEach(record -> reportFailure(record, e));
            return;
        }

        for (final ComponentReport report : reports) {
            final List<RetryableRecord<String, Component>> affectedRecords = purlRecords.get(report.coordinates());
            if (affectedRecords == null) {
                LOGGER.warn("Reported coordinates do not match any records: " + report.coordinates());
                continue;
            }

            for (final RetryableRecord<String, Component> record : affectedRecords) {
                if (record.retryAttempts() == 0) {
                    meterRegistry.counter("ossindex.retry", List.of(Tag.of("status", "succeededWithoutRetry"))).increment();
                } else {
                    meterRegistry.counter("ossindex.retry", List.of(Tag.of("status", "succeededWithRetry"))).increment();
                }
            }

            if (report.vulnerabilities().isEmpty()) {
                for (final RetryableRecord<String, Component> record : affectedRecords) {
                    final var result = new VulnerabilityResult();
                    result.setComponent(record.value());
                    result.setIdentity(AnalyzerIdentity.OSSINDEX_ANALYZER);
                    result.setVulnerability(null);
                    context().forward(new Record<>(record.key(), result, context().currentSystemTimeMs()));
                }
            } else {
                for (final ComponentReportVulnerability reportedVuln : report.vulnerabilities()) {
                    final Vulnerability vuln = ModelConverter.convert(reportedVuln);
                    for (final RetryableRecord<String, Component> record : affectedRecords) {
                        final var result = new VulnerabilityResult();
                        result.setComponent(record.value());
                        result.setIdentity(AnalyzerIdentity.OSSINDEX_ANALYZER);
                        result.setVulnerability(vuln);
                        context().forward(new Record<>(record.key(), result, context().currentSystemTimeMs()));
                    }
                }
            }
        }
    }

    private void scheduleForRetry(final RetryableRecord<String, Component> record) {
        if (record.retryAttempts() < retryMaxAttempts) {
            retryStore.put(record.value().getUuid(), record.withNextRetryAt(calculateNextRetryTimestamp(record)));
            meterRegistry.counter("ossindex.retry", List.of(Tag.of("status", "scheduledForRetry"))).increment();
        } else {
            LOGGER.warn("Max retry attempts ({}) exceeded for record {}", retryMaxAttempts, record.value());
            meterRegistry.counter("ossindex.retry", List.of(Tag.of("status", "failedWithMaxAttemptsExceeded"))).increment();
            reportFailure(record, new RuntimeException("Max retry attempts exceeded"));
        }
    }

    private void reportFailure(final RetryableRecord<String, Component> record, final Throwable failureCause) {
        final var result = VulnerabilityResult.forFailure(failureCause, record.value(), AnalyzerIdentity.OSSINDEX_ANALYZER);
        context().forward(new Record<>(record.key(), result, context().currentSystemTimeMs()));
    }

    private long calculateNextRetryTimestamp(final RetryableRecord<?, ?> record) {
        final long delay = retryIntervalFunction.apply(record.retryAttempts() + 1);
        final long nextRetry = context().currentSystemTimeMs() + delay;
        LOGGER.info("Next retry at {} (attempt: {}, delay: {})", Instant.ofEpochMilli(nextRetry), record.retryAttempts() + 1, Duration.ofMillis(delay));
        return nextRetry;
    }

}
