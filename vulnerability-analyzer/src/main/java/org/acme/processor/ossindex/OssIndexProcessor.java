package org.acme.processor.ossindex;

import io.github.resilience4j.circuitbreaker.CallNotPermittedException;
import io.github.resilience4j.circuitbreaker.CircuitBreaker;
import io.github.resilience4j.core.IntervalFunction;
import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.DistributionSummary;
import io.micrometer.core.instrument.Gauge;
import io.micrometer.core.instrument.Meter;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Tag;
import io.quarkus.cache.Cache;
import org.acme.client.ossindex.ComponentReport;
import org.acme.client.ossindex.ComponentReportRequest;
import org.acme.client.ossindex.ModelConverter;
import org.acme.client.ossindex.OssIndexClient;
import org.acme.modelx.Component;
import org.acme.modelx.ScanKey;
import org.acme.modelx.ScanResult;
import org.acme.modelx.ScanStatus;
import org.acme.modelx.ScanTask;
import org.acme.modelx.ScannerIdentity;
import org.acme.model.Vulnerability;
import org.acme.processor.retry.RetryStatus;
import org.acme.processor.retry.RetryableRecord;
import org.acme.processor.retry.RetryingProcessor;
import org.apache.commons.lang3.exception.ExceptionUtils;
import org.apache.http.HttpStatus;
import org.apache.http.conn.ConnectTimeoutException;
import org.apache.kafka.streams.processor.Cancellable;
import org.apache.kafka.streams.processor.PunctuationType;
import org.apache.kafka.streams.processor.api.Processor;
import org.apache.kafka.streams.processor.api.ProcessorContext;
import org.apache.kafka.streams.state.KeyValueIterator;
import org.apache.kafka.streams.state.KeyValueStore;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MultivaluedHashMap;
import javax.ws.rs.core.MultivaluedMap;
import java.net.SocketTimeoutException;
import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.NoSuchElementException;
import java.util.Optional;
import java.util.Set;

/**
 * A {@link Processor} for analyzing {@link Component}s with Sonatype OSS Index.
 */
public class OssIndexProcessor extends RetryingProcessor<String, ScanTask, String, ScanResult, ScanKey> {

    private static final Logger LOGGER = LoggerFactory.getLogger(OssIndexProcessor.class);

    private final OssIndexClient client;
    private final Cache cache;
    private final CircuitBreaker circuitBreaker;
    private final String batchStoreName;
    private final Duration batchInterval;
    private final MeterRegistry meterRegistry;
    private KeyValueStore<ScanKey, RetryableRecord<String, ScanTask>> batchStore;
    private Cancellable batchPunctuator;
    private Gauge batchStoreEntriesGauge;
    private DistributionSummary batchSizeDistribution;
    private Counter.Builder componentsScannedCounterBuilder;
    private Instant lastBatchAnalysis;

    OssIndexProcessor(final OssIndexClient client, final Cache cache, final CircuitBreaker circuitBreaker,
                      final String batchStoreName, final Duration batchInterval,
                      final String retryStoreName, final IntervalFunction retryIntervalFunction,
                      final int retryMaxAttempts, final MeterRegistry meterRegistry) {
        super(retryStoreName, retryIntervalFunction, retryMaxAttempts, meterRegistry);
        this.client = client;
        this.cache = cache;
        this.circuitBreaker = circuitBreaker;
        this.batchStoreName = batchStoreName;
        this.batchInterval = batchInterval;
        this.meterRegistry = meterRegistry;
    }

    @Override
    public void init(final ProcessorContext<String, ScanResult> context) {
        super.init(context);

        batchStore = context().getStateStore(batchStoreName);
        batchPunctuator = context().schedule(batchInterval, PunctuationType.WALL_CLOCK_TIME, this::punctuateBatch);

        final Tag threadIdMeterTag = Tag.of("thread_id", Thread.currentThread().getName());
        final Tag taskIdMeterTag = Tag.of("task_id", context().taskId().toString());
        final Tag processorMeterTag = Tag.of("processor", getClass().getSimpleName());
        final Tag scannerMeterTag = Tag.of("scanner", "ossindex");

        batchStoreEntriesGauge = Gauge.builder("kafka.stream.store.entries", batchStore::approximateNumEntries)
                .description("Total number of entries in the OSS Index batch state store")
                .tags(Set.of(threadIdMeterTag, taskIdMeterTag, processorMeterTag, Tag.of("store", batchStoreName)))
                .register(meterRegistry);
        batchSizeDistribution = DistributionSummary.builder("scanner.components.batch.size")
                .tags(Set.of(threadIdMeterTag, taskIdMeterTag, scannerMeterTag))
                .register(meterRegistry);
        componentsScannedCounterBuilder = Counter.builder("scanner.components.scanned")
                .description("Total number of scanned components")
                .tags(Set.of(threadIdMeterTag, taskIdMeterTag, scannerMeterTag));

        lastBatchAnalysis = Instant.now();
    }

    @Override
    public void process(final RetryableRecord<String, ScanTask> record) {
        addToBatch(record);
    }

    @Override
    public void close() {
        super.close();

        Optional.ofNullable(batchPunctuator).ifPresent(Cancellable::cancel);
        Optional.ofNullable(batchStoreEntriesGauge).ifPresent(Meter::close);
    }

    @Override
    protected ScanKey retryKey(final String key, final ScanTask value) {
        return value.scanKey();
    }

    @Override
    protected void onRetry(final RetryableRecord<String, ScanTask> record) {
        addToBatch(record);
    }

    @Override
    protected void onMaxRetriesExceeded(final RetryableRecord<String, ScanTask> record) {
        reportFailure(record, new RuntimeException("Max retry attempts exceeded"));
    }

    private void punctuateBatch(final long timestamp) {
        if (batchStore.approximateNumEntries() == 0) {
            LOGGER.debug("Current batch is empty");
            return;
        }

        if (Instant.ofEpochMilli(timestamp).isBefore(lastBatchAnalysis.plusSeconds(5))) {
            LOGGER.debug("Current batch is not yet due for submission");
            return;
        }

        final List<RetryableRecord<String, ScanTask>> batch = currentBatch();
        batch.forEach(batchRecord -> batchStore.delete(batchRecord.value().scanKey()));
        analyzeBatch(batch);
    }

    private void addToBatch(final RetryableRecord<String, ScanTask> record) {
        final Optional<ComponentReport> cachedReport = getCachedReport(record.value().component());
        if (cachedReport.isPresent()) {
            LOGGER.debug("Processing cached report for {}", record);
            processReport(cachedReport.get(), List.of(record));
            return;
        }

        if (batchStore.get(record.value().scanKey()) == null) {
            batchStore.put(record.value().scanKey(), record);
        } else {
            LOGGER.warn("Record {} is already in the current batch; Dropping", record);
            return;
        }

        if (batchStore.approximateNumEntries() >= 128) {
            final List<RetryableRecord<String, ScanTask>> batch = currentBatch();
            batch.forEach(batchRecord -> batchStore.delete(batchRecord.value().scanKey()));
            analyzeBatch(batch);
        }
    }

    private void analyzeBatch(final List<RetryableRecord<String, ScanTask>> batch) {
        LOGGER.info("Analyzing batch of {} records", batch.size());
        batchSizeDistribution.record(batch.size());
        lastBatchAnalysis = Instant.ofEpochMilli(context().currentSystemTimeMs());

        final MultivaluedMap<String, RetryableRecord<String, ScanTask>> purlRecords = new MultivaluedHashMap<>();
        for (final RetryableRecord<String, ScanTask> record : batch) {
            purlRecords.add(record.key(), record);
        }

        final List<ComponentReport> reports;
        try {
            reports = circuitBreaker.executeSupplier(() ->
                    client.getComponentReports(new ComponentReportRequest(purlRecords.keySet())));
            batch.forEach(record -> reportRetryStatus(record, RetryStatus.SUCCEEDED));
        } catch (Throwable e) {
            if (isRetryable(e)) {
                batch.forEach(record -> {
                    LOGGER.warn("Encountered retryable exception while analyzing {}: {}", record, e.getMessage());
                    scheduleForRetry(record);
                });
            } else {
                batch.forEach(record -> {
                    LOGGER.error("Encountered non-retryable exception while analyzing {}", record, e);
                    reportFailure(record, e);
                });
            }

            return;
        }

        for (final ComponentReport report : reports) {
            final List<RetryableRecord<String, ScanTask>> affectedRecords = purlRecords.get(report.coordinates());
            if (affectedRecords == null) {
                LOGGER.warn("Reported coordinates do not match any records: " + report.coordinates());
                continue;
            } else {
                purlRecords.remove(report.coordinates());
            }

            processReport(report, affectedRecords);
            cacheReport(report);
        }

        for (final Map.Entry<String, List<RetryableRecord<String, ScanTask>>> unmatchedRecords : purlRecords.entrySet()) {
            LOGGER.warn("No results were returned for coordinates {} (affecting {}/{} records in this batch)",
                    unmatchedRecords.getKey(), unmatchedRecords.getValue().size(), batch.size());
            // TODO: Should we cache "no vulnerabilities" for this case, too?
            for (final RetryableRecord<String, ScanTask> record : unmatchedRecords.getValue()) {
                final ScanResult result = ScanResult.builder(record.value().scanKey(), ScannerIdentity.OSSINDEX)
                        .withStatus(ScanStatus.SUCCESSFUL)
                        .withVulnerabilities(Collections.emptyList())
                        .build();
                context().forward(record.withValue(result).withTimestamp(context().currentSystemTimeMs()));
                reportScanResult("not_vulnerable");
            }
        }
    }

    private List<RetryableRecord<String, ScanTask>> currentBatch() {
        final var batch = new ArrayList<RetryableRecord<String, ScanTask>>();
        try (final KeyValueIterator<ScanKey, RetryableRecord<String, ScanTask>> valueIterator = batchStore.all()) {
            while (valueIterator.hasNext()) {
                batch.add(valueIterator.next().value);
            }
        }
        return batch;
    }

    private Optional<ComponentReport> getCachedReport(final Component component) {
        try {
            final ComponentReport cachedReport = cache.<String, ComponentReport>get(component.purl().getCoordinates(),
                    key -> {
                        // null values would be cached, so throw an exception instead.
                        // See https://quarkus.io/guides/cache#let-exceptions-bubble-up
                        throw new NoSuchElementException();
                    }).await().indefinitely();
            return Optional.of(cachedReport);
        } catch (Exception e) {
            return Optional.empty();
        }
    }

    private void cacheReport(final ComponentReport report) {
        cache.get(report.coordinates(), key -> report).await().indefinitely();
    }

    private void processReport(final ComponentReport report, List<RetryableRecord<String, ScanTask>> affectedRecords) {
        final List<Vulnerability> vulnerabilities = report.vulnerabilities().stream()
                .map(ModelConverter::convert)
                .toList();

        for (final RetryableRecord<String, ScanTask> record : affectedRecords) {
            final var result = ScanResult.builder(record.value().scanKey(), ScannerIdentity.OSSINDEX)
                    .withStatus(ScanStatus.SUCCESSFUL)
                    .withVulnerabilities(vulnerabilities)
                    .build();
            context().forward(record.withValue(result).withTimestamp(context().currentSystemTimeMs()));
            reportScanResult(vulnerabilities.size() > 0 ? "vulnerable" : "not_vulnerable");
        }
    }

    private void reportFailure(final RetryableRecord<String, ScanTask> record, final Throwable failureCause) {
        final var result = ScanResult.builder(record.value().scanKey(), ScannerIdentity.OSSINDEX)
                .withFailureReason(failureCause.getMessage())
                .build();
        context().forward(record.withValue(result).withTimestamp(context().currentSystemTimeMs()));
        reportRetryStatus(record, RetryStatus.FAILED);
        reportScanResult("failed");
    }

    private void reportScanResult(final String scanResult) {
        componentsScannedCounterBuilder
                .tag("result", scanResult)
                .register(meterRegistry)
                .increment();
    }

    private boolean isRetryable(final Throwable throwable) {
        final Throwable rootCause = ExceptionUtils.getRootCause(throwable);

        if (rootCause instanceof final WebApplicationException wae) {
            return List.of(
                    HttpStatus.SC_TOO_MANY_REQUESTS,
                    HttpStatus.SC_INTERNAL_SERVER_ERROR,
                    HttpStatus.SC_BAD_GATEWAY,
                    HttpStatus.SC_SERVICE_UNAVAILABLE,
                    HttpStatus.SC_GATEWAY_TIMEOUT
            ).contains(wae.getResponse().getStatus());
        }

        return rootCause instanceof CallNotPermittedException
                || rootCause instanceof ConnectTimeoutException
                || rootCause instanceof SocketTimeoutException;
    }

}
