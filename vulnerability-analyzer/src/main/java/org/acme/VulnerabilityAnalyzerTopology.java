package org.acme;

import com.fasterxml.jackson.core.type.TypeReference;
import com.github.packageurl.MalformedPackageURLException;
import com.github.packageurl.PackageURL;
import io.quarkus.kafka.client.serialization.ObjectMapperDeserializer;
import io.quarkus.kafka.client.serialization.ObjectMapperSerde;
import io.quarkus.kafka.client.serialization.ObjectMapperSerializer;
import io.quarkus.runtime.annotations.RegisterForReflection;
import org.acme.analyzer.InternalAnalyzer;
import org.acme.analyzer.OssIndexAnalyzer;
import org.acme.analyzer.SnykAnalyzer;
import org.acme.common.KafkaTopic;
import org.acme.model.Component;
import org.acme.model.VulnerabilityResult;
import org.acme.processor.BatchProcessor;
import org.acme.processor.PartitionIdReKeyProcessor;
import org.apache.commons.lang3.StringUtils;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.Branched;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Named;
import org.apache.kafka.streams.kstream.Produced;
import org.apache.kafka.streams.state.Stores;
import org.eclipse.microprofile.config.inject.ConfigProperty;
import org.jboss.logging.Logger;

import javax.enterprise.context.ApplicationScoped;
import javax.enterprise.inject.Produces;
import javax.inject.Inject;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

import static org.acme.commonutil.KafkaStreamsUtil.processorNameConsume;
import static org.acme.commonutil.KafkaStreamsUtil.processorNameProduce;

@ApplicationScoped
@RegisterForReflection(targets = {org.apache.commons.logging.impl.LogFactoryImpl.class, org.apache.commons.logging.impl.SimpleLog.class,org.apache.commons.logging.LogFactory.class})
public class VulnerabilityAnalyzerTopology {

    private static final Logger LOGGER = Logger.getLogger(VulnerabilityAnalyzerTopology.class);

    private final OssIndexAnalyzer ossIndexAnalyzer;
    private final SnykAnalyzer snykAnalyzer;
    private final InternalAnalyzer internalAnalyzer;
    private final int componentsBatchMaxSize;
    private final Duration componentsBatchInterval;


    @Inject
    public VulnerabilityAnalyzerTopology(final OssIndexAnalyzer ossIndexAnalyzer,
                                         final SnykAnalyzer snykAnalyzer, final InternalAnalyzer internalAnalyzer,
                                         @ConfigProperty(name = "scanner.batch.max.size") final int componentsBatchMaxSize,
                                         @ConfigProperty(name = "scanner.batch.interval") final Duration componentsBatchInterval) {
        this.ossIndexAnalyzer = ossIndexAnalyzer;
        this.snykAnalyzer = snykAnalyzer;
        this.internalAnalyzer = internalAnalyzer;
        this.componentsBatchMaxSize = componentsBatchMaxSize;
        this.componentsBatchInterval = componentsBatchInterval;
    }

    @Produces
    public Topology topology() {
        final var streamsBuilder = new StreamsBuilder();

        final var componentSerde = new ObjectMapperSerde<>(Component.class);
        final var componentsSerde = Serdes.serdeFrom(new ObjectMapperSerializer<>(),
                new ObjectMapperDeserializer<List<Component>>(new TypeReference<>() {
                }));
        final var vulnResultSerde = new ObjectMapperSerde<>(VulnerabilityResult.class);

        // Flat-Map incoming components from the API server, and re-key the stream from UUIDs to CPEs, PURLs, and SWID Tag IDs.
        // Every component from component-analysis can thus produce up to three new events.
        final KStream<String, Component> componentStream = streamsBuilder
                .stream(KafkaTopic.VULN_ANALYSIS_COMPONENT.getName(), Consumed
                        .with(Serdes.UUID(), componentSerde)
                        .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_COMPONENT)))
                .peek((uuid, component) -> LOGGER.info("Received component: " + component),
                        Named.as("log_component"))
                .flatMap((uuid, component) -> {
                    final var components = new ArrayList<KeyValue<String, Component>>();
                    if (component.getCpe() != null) {
                        // TODO: Canonicalize the CPE used as key, so that CPEs describing the same component end up in the same partition.
                        components.add(KeyValue.pair(component.getCpe(), component));
                    }
                    if (component.getPurl() != null) {
                        components.add(KeyValue.pair(component.getPurl().getCoordinates(), component));
                    }
                    if (component.getSwidTagId() != null) {
                        // NOTE: Barely any components have a SWID Tag ID yet, and no scanner supports it
                        components.add(KeyValue.pair(component.getSwidTagId(), component));
                    }
                    if (component.getCpe() == null && component.getPurl() == null && component.getSwidTagId() == null) {
                        components.add(KeyValue.pair("no-identifier", component));
                    }
                    return components;
                }, Named.as("re-key_component_from_uuid_to_identifier"))
                .peek((identifier, component) -> LOGGER.info("Re-keyed component: " + component.getUuid() + " -> " + identifier),
                        Named.as("log_re-keyed_component"));

        final Map<String, KStream<String, Component>> branches = componentStream
                .split(Named.as("component-with-identifier-type"))
                .branch((identifier, component) -> isCpe(identifier), Branched.as("-cpe"))
                .branch((identifier, component) -> isPurl(identifier), Branched.as("-purl"))
                .branch((identifier, component) -> isSwidTagId(identifier), Branched.as("-swid"))
                .defaultBranch(Branched.as("-unknown"));
        branches.get("component-with-identifier-type-cpe").to(KafkaTopic.VULN_ANALYSIS_COMPONENT_CPE.getName(), Produced
                .with(Serdes.String(), componentSerde)
                .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_COMPONENT_CPE)));
        branches.get("component-with-identifier-type-purl").to(KafkaTopic.VULN_ANALYSIS_COMPONENT_PURL.getName(), Produced
                .with(Serdes.String(), componentSerde)
                .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_COMPONENT_PURL)));
        branches.get("component-with-identifier-type-swid").to(KafkaTopic.VULN_ANALYSIS_COMPONENT_SWID.getName(), Produced
                .with(Serdes.String(), componentSerde)
                .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_COMPONENT_SWID)));
        branches.get("component-with-identifier-type-unknown")
                // The component does not have an identifier that we can work with,
                // but we still want to produce a result.
                // TODO: Instead of reporting "no vulnerability", report "not applicable" or so
                .map((identifier, component) -> {
                    final var result = new VulnerabilityResult();
                    result.setComponent(component);
                    result.setIdentity(null);
                    result.setVulnerability(null);
                    return KeyValue.pair(component.getUuid(), result);
                }, Named.as("map_to_empty_result"))
                .to(KafkaTopic.VULN_ANALYSIS_RESULT.getName(), Produced
                        .with(Serdes.UUID(), vulnResultSerde)
                        .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_RESULT, "empty_result")));

        streamsBuilder.addStateStore(Stores.keyValueStoreBuilder(
                Stores.persistentKeyValueStore("purl_component_batches"), Serdes.Integer(), componentsSerde));

        streamsBuilder.addStateStore(Stores.keyValueStoreBuilder(
                Stores.persistentKeyValueStore("cpe_component_batches"), Serdes.Integer(), componentsSerde));

        // TODO: Repeat this for CPE and SWID Tag ID
        final KStream<Integer, List<Component>> purlAggregateStream = streamsBuilder
                .stream(KafkaTopic.VULN_ANALYSIS_COMPONENT_PURL.getName(), Consumed
                        .with(Serdes.String(), componentSerde)
                        .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_COMPONENT_PURL)))
                // Batching requires records to have the same key.
                // Because most records will have different identifiers as key, we re-key
                // the stream to the partition ID the records are in.
                // This allows us to aggregate all records within the partition(s) we're consuming from.
                .process(() -> new PartitionIdReKeyProcessor<>(), Named.as("re-key_component_from_purl_to_partition_id"))
                .process(() -> new BatchProcessor<>("purl_component_batches", componentsBatchInterval, componentsBatchMaxSize),
                        Named.as("batch_purl_components"), "purl_component_batches");

        final KStream<Integer, List<Component>> cpeAggregateStream = streamsBuilder
                .stream(KafkaTopic.VULN_ANALYSIS_COMPONENT_CPE.getName(), Consumed
                        .with(Serdes.String(), componentSerde)
                        .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_COMPONENT_CPE)))
                // Batching requires records to have the same key.
                // Because most records will have different identifiers as key, we re-key
                // the stream to the partition ID the records are in.
                // This allows us to aggregate all records within the partition(s) we're consuming from.
                .process(() -> new PartitionIdReKeyProcessor<>(), Named.as("re-key_component_from_cpe_to_partition_id"))
                .process(() -> new BatchProcessor<>("cpe_component_batches", componentsBatchInterval, componentsBatchMaxSize),
                        Named.as("batch_cpe_components"), "cpe_component_batches");


        if (ossIndexAnalyzer.isEnabled()) {
            purlAggregateStream
                    .flatMap((window, components) -> analyzeOssIndex(components),
                            Named.as("analyze_purl_component_batch_with_ossindex"))
                    .to(KafkaTopic.VULN_ANALYSIS_VULNERABILITY.getName(), Produced
                            .with(Serdes.String(), vulnResultSerde)
                            .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_VULNERABILITY, "ossindex_results")));
        }
        if (snykAnalyzer.isEnabled()) {
            purlAggregateStream
                    .flatMap((window, components) -> analyzeSnyk(components),
                            Named.as("analyze_purl_component_batch_with_snyk"))
                    .to(KafkaTopic.VULN_ANALYSIS_VULNERABILITY.getName(), Produced
                            .with(Serdes.String(), vulnResultSerde)
                            .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_VULNERABILITY, "snyk_results")));
        }

        if (internalAnalyzer.isEnabled()) {
            purlAggregateStream
                    .flatMap((window, components) -> analyzeInternalWithPurl(components),
                            Named.as("analyze_purl_component_batch_with_internal"))
                    .to(KafkaTopic.VULN_ANALYSIS_VULNERABILITY.getName(), Produced
                            .with(Serdes.String(), vulnResultSerde)
                            .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_VULNERABILITY, "internal_results_purl")));

            cpeAggregateStream
                    .flatMap((window, components) -> analyzeInternalWithCPE(components),
                            Named.as("analyze_cpe_component_batch_with_internal"))
                    .to(KafkaTopic.VULN_ANALYSIS_VULNERABILITY.getName(), Produced
                            .with(Serdes.String(), vulnResultSerde)
                            .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_VULNERABILITY, "internal_results_cpe")));
        }


        // Consume from the topic where analyzers write their results to,
        // and re-key them from CPE/PURL/SWID back to component UUIDs.
        streamsBuilder
                .stream(KafkaTopic.VULN_ANALYSIS_VULNERABILITY.getName(), Consumed
                        .with(Serdes.String(), vulnResultSerde)
                        .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_VULNERABILITY)))
                .peek((identifier, vulnResult) -> LOGGER.info("Re-keying result: " + identifier + " -> " + vulnResult.getComponent().getUuid()),
                        Named.as("log_vuln_result_re-keying"))
                .map((identifier, vulnResult) -> KeyValue.pair(vulnResult.getComponent().getUuid(), vulnResult),
                        Named.as("re-key_vuln_result_from_identifier_to_component_uuid"))
                .to(KafkaTopic.VULN_ANALYSIS_RESULT.getName(), Produced
                        .with(Serdes.UUID(), vulnResultSerde)
                        .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_RESULT)));

        return streamsBuilder.build();
    }

    private boolean isPurl(final String purl) {
        try {
            new PackageURL(purl);
            return true;
        } catch (MalformedPackageURLException e) {
            return false;
        }
    }

    private boolean isCpe(final String cpe) {
        return StringUtils.startsWith(cpe, "cpe:");
    }

    private boolean isSwidTagId(final String swidTagId) {
        return false;
    }

    public List<KeyValue<String, VulnerabilityResult>> analyzeOssIndex(final List<Component> components) {
        LOGGER.info("Performing OSS Index analysis for " + components.size() + " components: " + components);
        return ossIndexAnalyzer.analyze(components).stream()
                // TODO: Handle exceptions
                .map(vulnResult -> KeyValue.pair(vulnResult.getComponent().getPurl().getCoordinates(), vulnResult))
                .toList();
    }

    List<KeyValue<String, VulnerabilityResult>> analyzeSnyk(final List<Component> components) {
        LOGGER.info("Performing Snyk analysis for components: " + components);
        return snykAnalyzer.analyze(components).stream()
                // TODO: Handle exceptions
                .map(vulnResult -> KeyValue.pair(vulnResult.getComponent().getPurl().getCoordinates(), vulnResult))
                .toList();
    }

    public List<KeyValue<String, VulnerabilityResult>> analyzeInternalWithPurl(final List<Component> component) {
        LOGGER.info("Performing Internal analysis for components: " + component);
        return internalAnalyzer.analyze(component).stream()
                // TODO: Handle exceptions
                .map(vulnResult -> KeyValue.pair(vulnResult.getComponent().getPurl().getCoordinates(), vulnResult))
                .toList();
    }

    public List<KeyValue<String, VulnerabilityResult>> analyzeInternalWithCPE(final List<Component> component) {
        LOGGER.info("Performing Internal analysis for components: " + component);
        return internalAnalyzer.analyze(component).stream()
                // TODO: Handle exceptions
                .map(vulnResult -> KeyValue.pair(vulnResult.getComponent().getCpe(), vulnResult))
                .toList();
    }


}
