package org.acme;

import io.quarkus.kafka.client.serialization.ObjectMapperSerde;
import org.acme.common.KafkaTopic;
import org.acme.config.InternalScannerConfig;
import org.acme.config.OssIndexConfig;
import org.acme.config.SnykConfig;
import org.acme.modelx.CompletedScans;
import org.acme.modelx.Component;
import org.acme.modelx.ExpectedScanResults;
import org.acme.modelx.ScanKey;
import org.acme.modelx.ScanResult;
import org.acme.modelx.ScanStatus;
import org.acme.modelx.ScanTask;
import org.acme.modelx.ScannerIdentity;
import org.acme.processor.internal.InternalScannerProcessorSupplier;
import org.acme.processor.misc.TombstoneEmittingProcessorSupplier;
import org.acme.processor.ossindex.OssIndexProcessorSupplier;
import org.acme.processor.snyk.SnykProcessorSupplier;
import org.acme.serializers.ScanKeySerde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.utils.Bytes;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.Branched;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.Grouped;
import org.apache.kafka.streams.kstream.Joined;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.KTable;
import org.apache.kafka.streams.kstream.Materialized;
import org.apache.kafka.streams.kstream.Named;
import org.apache.kafka.streams.kstream.Produced;
import org.apache.kafka.streams.state.KeyValueStore;

import javax.enterprise.context.ApplicationScoped;
import javax.enterprise.inject.Produces;
import javax.inject.Inject;
import java.time.Duration;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;

import static org.acme.commonutil.KafkaStreamsUtil.processorNameConsume;
import static org.acme.commonutil.KafkaStreamsUtil.processorNameProduce;

@ApplicationScoped
public class VulnerabilityAnalyzerTopology {

    private final InternalScannerConfig internalScannerConfig;
    private final OssIndexConfig ossIndexConfig;
    private final SnykConfig snykConfig;
    private final InternalScannerProcessorSupplier internalScannerProcessorSupplier;
    private final OssIndexProcessorSupplier ossIndexProcessorSupplier;
    private final SnykProcessorSupplier snykProcessorSupplier;

    @Inject
    public VulnerabilityAnalyzerTopology(final InternalScannerConfig internalScannerConfig,
                                         final OssIndexConfig ossIndexConfig,
                                         final SnykConfig snykConfig,
                                         final InternalScannerProcessorSupplier internalScannerProcessorSupplier,
                                         final OssIndexProcessorSupplier ossIndexProcessorSupplier,
                                         final SnykProcessorSupplier snykProcessorSupplier) {
        this.internalScannerConfig = internalScannerConfig;
        this.ossIndexConfig = ossIndexConfig;
        this.snykConfig = snykConfig;
        this.internalScannerProcessorSupplier = internalScannerProcessorSupplier;
        this.ossIndexProcessorSupplier = ossIndexProcessorSupplier;
        this.snykProcessorSupplier = snykProcessorSupplier;
    }

    @Produces
    public Topology topology() {
        final var streamsBuilder = new StreamsBuilder();

        final var componentSerde = new ObjectMapperSerde<>(Component.class);
        final var completedScansSerde = new ObjectMapperSerde<>(CompletedScans.class);
        final var expectedScanResultsSerde = new ObjectMapperSerde<>(ExpectedScanResults.class);
        final var scanKeySerde = new ScanKeySerde();
        final var scanResultSerde = new ObjectMapperSerde<>(ScanResult.class);
        final var scanTaskSerde = new ObjectMapperSerde<>(ScanTask.class);

        final KStream<ScanKey, Component> componentStream = streamsBuilder
                .stream(KafkaTopic.VULN_ANALYSIS_COMPONENT.getName(), Consumed
                        .with(scanKeySerde, componentSerde)
                        .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_COMPONENT)));

        final KStream<String, ScanResult> scanResultStream = streamsBuilder
                .stream(KafkaTopic.VULN_ANALYSIS_SCANNER_RESULT.getName(), Consumed
                        .with(Serdes.String(), scanResultSerde)
                        .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_SCANNER_RESULT)));

        final KStream<ScanKey, ScanResult> resultStream = streamsBuilder
                .stream(KafkaTopic.VULN_ANALYSIS_RESULT.getName(), Consumed
                        .with(scanKeySerde, scanResultSerde)
                        .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_RESULT)));

        // For every incoming component, determine which scanners are enabled, and capable
        // of scanning it. Generate a ScanTask for each applicable scanner.
        final KStream<ScanKey, ScanTask> scanTaskStream = componentStream
                .flatMapValues((scanKey, component) -> {
                    final var tasks = new ArrayList<ScanTask>();
                    if (shouldScanWithInternalScanner(component)) {
                        tasks.add(new ScanTask(scanKey, ScannerIdentity.INTERNAL, component, false));
                    }
                    if (shouldScanWithOssIndex(component)) {
                        tasks.add(new ScanTask(scanKey, ScannerIdentity.OSSINDEX, component, false));
                    }
                    if (shouldScanWithSnyk(component)) {
                        tasks.add(new ScanTask(scanKey, ScannerIdentity.SNYK, component, false));
                    }
                    if (tasks.isEmpty()) {
                        tasks.add(new ScanTask(scanKey, ScannerIdentity.NONE, null, false));
                    }
                    return tasks;
                }, Named.as("generate_scan_tasks"));

        // Based on the generated scan tasks, determine for which scanners we're expecting results.
        // For each ScanKey, store a collection of scanner names in a KTable.
        final KTable<ScanKey, ExpectedScanResults> expectedScanResultsTable = scanTaskStream
                // Emit tombstone events for ScanKeys for which no events have been received
                // for >= 1h (stream time). Check every 5min for eligible keys.
                // This keeps the KTable from growing indefinitely.
                .process(new TombstoneEmittingProcessorSupplier<>(
                        "expected-scan-results-last-update-store",
                        scanKeySerde, Duration.ofMinutes(5), Duration.ofHours(1),
                        ScanTask::asTombstone
                ), Named.as("emit_expected_scan_results_table_tombstones"))
                .groupByKey(Grouped
                        .with(scanKeySerde, scanTaskSerde)
                        .withName("group_scan_tasks_by_scan_key"))
                .aggregate(() -> new ExpectedScanResults(null),
                        (scanKey, scanTask, aggregate) -> {
                            if (scanTask.tombstone()) {
                                return null;
                            }
                            final Set<ScannerIdentity> scanners;
                            if (aggregate.scanners() != null) {
                                scanners = aggregate.scanners();
                                scanners.add(scanTask.scanner());
                            } else {
                                scanners = Set.of(scanTask.scanner());
                            }
                            return new ExpectedScanResults(scanners);
                        }, Named.as("aggregate_expected_scan_results"),
                        Materialized
                                .<ScanKey, ExpectedScanResults, KeyValueStore<Bytes, byte[]>>as("expected-scan-results-table")
                                .withKeySerde(scanKeySerde)
                                .withValueSerde(expectedScanResultsSerde)
                                .withStoreType(Materialized.StoreType.IN_MEMORY));

        // Route the generated scan tasks to the topics of the respective scanners.
        // Events are re-keyed from ScanKey to component identifier (CPE, PURL, etc.).
        // Keying by identifier will ensure that the same identifier will always
        // be scanned by the same Stream Task, allowing for efficient and reliable cache lookups.
        //
        // The priority of identifiers to use as key is as follows:
        //   1. PURL
        //   2. CPE
        //   3. (Others)
        //
        // The assumption is that PURL will cover most cases, and having one or the other outlier
        // where PURL and CPE refer to different components is acceptable in practice.
        scanTaskStream
                .selectKey((scanKey, scanTask) -> {
                    if (scanTask.component() == null) {
                        return scanKey.component().toString();
                    }
                    if (scanTask.component().purl() != null) {
                        return scanTask.component().purl().getCoordinates();
                    } else if (scanTask.component().cpe() != null) {
                        return scanTask.component().cpe();
                    }
                    return scanKey.component().toString();
                }, Named.as("re-key_to_component_identifier"))
                .split(Named.as("applicable_scanner"))
                .branch((identifier, task) -> internalScannerConfig.enabled() && ScannerIdentity.INTERNAL == task.scanner(), Branched
                        .<String, ScanTask>withConsumer(stream -> stream
                                .to(KafkaTopic.VULN_ANALYSIS_SCANNER_INTERNAL.getName(), Produced
                                        .with(Serdes.String(), scanTaskSerde)
                                        .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_SCANNER_INTERNAL))))
                        .withName("-internal"))
                .branch((identifier, task) -> ossIndexConfig.enabled() && ScannerIdentity.OSSINDEX == task.scanner(), Branched
                        .<String, ScanTask>withConsumer(stream -> stream
                                .to(KafkaTopic.VULN_ANALYSIS_SCANNER_OSSINDEX.getName(), Produced
                                        .with(Serdes.String(), scanTaskSerde)
                                        .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_SCANNER_OSSINDEX))))
                        .withName("-ossindex"))
                .branch((identifier, task) -> snykConfig.enabled() && ScannerIdentity.SNYK == task.scanner(), Branched
                        .<String, ScanTask>withConsumer(stream -> stream
                                .to(KafkaTopic.VULN_ANALYSIS_SCANNER_SNYK.getName(), Produced
                                        .with(Serdes.String(), scanTaskSerde)
                                        .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_SCANNER_SNYK))))
                        .withName("-snyk"))
                .branch((identifier, task) -> ScannerIdentity.NONE == task.scanner(), Branched
                        .<String, ScanTask>withConsumer(stream -> stream
                                .map((identifier, task) -> KeyValue.pair(task.scanKey(),
                                        ScanResult.builder(task.scanKey(), ScannerIdentity.NONE)
                                                .withStatus(ScanStatus.COMPLETE)
                                                .build()),
                                        Named.as("map_to_completion_event2")) // TODO: Find better name
                                .to(KafkaTopic.VULN_ANALYSIS_RESULT.getName(), Produced
                                        .with(scanKeySerde, scanResultSerde)
                                        .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_RESULT,
                                                "completion_event_for_unscannable_component"))))
                        .withName("-none"))
                .noDefaultBranch();

        resultStream
                // Filter out completion events, otherwise we'll be running into an infinite loop.
                .filter((scanKey, result) -> ScanStatus.COMPLETE != result.status(),
                        Named.as("filter_out_completion_events"))
                .join(expectedScanResultsTable, (result, expected) -> {
                    final var completed = new HashMap<ScannerIdentity, ScanStatus>();
                    expected.scanners().forEach(scanner -> completed.put(scanner, ScanStatus.PENDING));
                    completed.put(result.identity(), result.status());
                    return new CompletedScans(completed, false);
                }, Joined
                        .with(scanKeySerde, scanResultSerde, expectedScanResultsSerde)
                        .withName("join_results_with_expected_scan_results"))
                // Emit tombstone events for ScanKeys for which no events have been received
                // for >= 1h (stream time). Check every 5min for eligible keys.
                // This keeps the KTable from growing indefinitely.
                .process(new TombstoneEmittingProcessorSupplier<>(
                        "completed-scans-table-last-update-store",
                        scanKeySerde, Duration.ofMinutes(5), Duration.ofHours(1),
                        CompletedScans::asTombstone
                ), Named.as("emit_completed_scans_table_tombstones"))
                .groupByKey(Grouped
                        .with(scanKeySerde, completedScansSerde)
                        .withName("group_completed_scans_by_scan_key"))
                .aggregate(
                        () -> new CompletedScans(null, false),
                        (scanKey, completedScans, aggregate) -> {
                            if (completedScans.tombstone()) {
                                return null;
                            }
                            final Map<ScannerIdentity, ScanStatus> completed;
                            if (aggregate.completed() != null) {
                                completed = aggregate.completed();
                                completedScans.completed().entrySet().stream()
                                        .filter(entry -> ScanStatus.PENDING != entry.getValue())
                                        .forEach(entry -> completed.put(entry.getKey(), entry.getValue()));
                            } else {
                                completed = completedScans.completed();
                            }
                            return new CompletedScans(completed, false);
                        }, Named.as("aggregate_completed_scans"),
                        Materialized
                                .<ScanKey, CompletedScans, KeyValueStore<Bytes, byte[]>>as("completed-scans-table")
                                .withKeySerde(scanKeySerde)
                                .withValueSerde(completedScansSerde)
                                .withStoreType(Materialized.StoreType.IN_MEMORY))
                .toStream(Named.as("stream_completed_scans"))
                // We receive an event for every change of the aggregate here,
                // so suppress events that do not represent a completed scan.
                .filter((scanKey, completedScans) -> completedScans.completed().values().stream().noneMatch(ScanStatus.PENDING::equals),
                        Named.as("filter_completed_scans"))
                .mapValues((scanKey, completedScans) -> ScanResult.builder(scanKey, ScannerIdentity.NONE)
                                .withStatus(ScanStatus.COMPLETE).build(),
                        Named.as("map_to_completion_event"))
                .to(KafkaTopic.VULN_ANALYSIS_RESULT.getName(), Produced
                        .with(scanKeySerde, scanResultSerde)
                        .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_RESULT, "completion_event")));

        if (internalScannerConfig.enabled()) {
            streamsBuilder
                    .stream(KafkaTopic.VULN_ANALYSIS_SCANNER_INTERNAL.getName(), Consumed
                            .with(Serdes.String(), scanTaskSerde)
                            .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_SCANNER_INTERNAL)))
                    .process(internalScannerProcessorSupplier, Named.as("scan_with_internal-scanner"))
                    .to(KafkaTopic.VULN_ANALYSIS_SCANNER_RESULT.getName(), Produced
                            .with(Serdes.String(), scanResultSerde)
                            .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_SCANNER_RESULT, "internal-scanner_results")));
        }
        if (ossIndexConfig.enabled()) {
            streamsBuilder
                    .stream(KafkaTopic.VULN_ANALYSIS_SCANNER_OSSINDEX.getName(), Consumed
                            .with(Serdes.String(), scanTaskSerde)
                            .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_SCANNER_OSSINDEX)))
                    .process(ossIndexProcessorSupplier, Named.as("scan_with_ossindex"))
                    .to(KafkaTopic.VULN_ANALYSIS_SCANNER_RESULT.getName(), Produced
                            .with(Serdes.String(), scanResultSerde)
                            .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_SCANNER_RESULT, "ossindex_results")));
        }
        if (snykConfig.enabled()) {
            streamsBuilder
                    .stream(KafkaTopic.VULN_ANALYSIS_SCANNER_SNYK.getName(), Consumed
                            .with(Serdes.String(), scanTaskSerde)
                            .withName(processorNameConsume(KafkaTopic.VULN_ANALYSIS_SCANNER_SNYK)))
                    .process(snykProcessorSupplier, Named.as("scan_with_snyk"))
                    .to(KafkaTopic.VULN_ANALYSIS_SCANNER_RESULT.getName(), Produced
                            .with(Serdes.String(), scanResultSerde)
                            .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_SCANNER_RESULT, "snyk_results")));
        }

        scanResultStream
                .selectKey((identifier, result) -> result.key(),
                        Named.as("re-key_scan_result_from_identifier_to_scan_key"))
                .to(KafkaTopic.VULN_ANALYSIS_RESULT.getName(), Produced
                        .with(scanKeySerde, scanResultSerde)
                        .withName(processorNameProduce(KafkaTopic.VULN_ANALYSIS_RESULT)));

        return streamsBuilder.build();
    }

    private boolean shouldScanWithInternalScanner(final Component component) {
        return internalScannerConfig.enabled()
                && (component.cpe() != null || component.purl() != null);
    }

    private boolean shouldScanWithOssIndex(final Component component) {
        return ossIndexConfig.enabled() && component.purl() != null;
    }

    private boolean shouldScanWithSnyk(final Component component) {
        return snykConfig.enabled() && component.purl() != null;
    }

}
