{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"\"Hyades\" by Todd Vance (CC BY-SA 2.5)"},{"location":"#introduction","title":"Introduction","text":"<p>Project Hyades, named after the star cluster closest to earth, is an incubating project for decoupling responsibilities from Dependency-Track's monolithic API server into separate, scalable\u2122 services.</p> <p>The main objectives of Hyades are:</p> <ul> <li>Enable Dependency-Track to handle portfolios spanning hundreds of thousands of projects</li> <li>Improve resilience of Dependency-Track, providing more confidence when relying on it in critical workflows</li> <li>Improve deployment and configuration management experience for containerized / cloud native tech stacks</li> </ul> <p>Other than separating responsibilities, the API server has been modified to allow for high availability (active-active) deployments. Various \"hot paths\", like processing of uploaded BOMs, have been optimized in the existing code. Further optimization is an ongoing effort.</p> <p>Hyades already is a superset of Dependency-Track, as changes up to Dependency-Track v4.11.4 were ported, and features made possible by the new architecture have been implemented on top.</p> <p>Warning</p> <p>Hyades is not yet fully production ready, please refer to the GA roadmap for the current status.</p> <p>Already using Dependency-Track v4?</p> <p>Check out Changes over v4 for a complete overview of changes, and have a look at Migrating from v4 for migration instructions.</p>"},{"location":"#background","title":"Background","text":"tl;dr <p>The architecture of Dependency-Track v4 prevents it from scaling past a certain workload.</p> <p>Dependency-Track, for the most part, is an event-based system. As a platform for ingesting data (in the form of BOMs), listening for and itself emitting signals on it, an event-based architecture makes sense conceptually. The majority of operations Dependency-Track performs happen asynchronously, without client interaction.</p> <p>On a technical level, Dependency-Track uses an in-memory publish-subscribe architecture, implemented using Java <code>ExecutorService</code>s. An <code>ExecutorService</code> can be thought of as a pool of worker threads, consuming from an internal task queue. Tasks can be submitted to an <code>ExecutorService</code>, which will then execute them one-by-one. As multiple threads work on the queue in parallel, the order in which tasks are being processed is not guaranteed. Thread pool sizes can vary from one, up to unbounded numbers of threads.</p> <p>In Dependency-Track, when an event is published, subscribers to the event are looked up. Per API contract, event subscribers must implement an <code>inform</code> method, which takes the published event as argument. For any given event, 0-N tasks will be enqueued to the <code>ExecutorService</code>'s task queue - one for each subscriber.</p> <p></p> <p>There are three <code>ExecutorService</code> instances in Dependency-Track:</p> <ul> <li><code>EventService</code></li> <li><code>SingleThreadedEventService</code></li> <li><code>NotificationService</code></li> </ul> <p><code>EventService</code> forms the primary worker pool. Its thread pool size defaults to <code>&lt;NUM_CPU&gt; * 4</code>. A machine with a 4-core CPU will thus have a thread pool size of <code>16</code>. The size is configurable. Common tasks handled by this worker pool include:</p> <ul> <li>Processing of uploaded BOMs and VEXs</li> <li>Performing vulnerability analysis of components, projects, or the entire portfolio</li> <li>Performing repository meta analysis of components, projects, or the entire portfolio</li> <li>Calculation of metrics for components, projects, or the entire portfolio</li> </ul> <p><code>SingleThreadedEventService</code> is a worker pool with only a single thread. The purpose of this worker pool is execute tasks that must not be run in parallel. As such, it serves as a means to serialize task execution. Common tasks handled by this worker pool include:</p> <ul> <li>Mirroring of the NVD vulnerability database</li> <li>Updating Lucene indexes on disk</li> </ul> <p><code>NotificationService</code> is a dedicated worker pool for dispatching notifications. Its thread pool size defaults to <code>4</code> and is not configurable.</p>"},{"location":"#limitations-of-the-v4-architecture","title":"Limitations of the v4 architecture","text":"<p>While this architecture works great for small to medium workloads, it presents various challenges for larger ones:</p> <ol> <li>Not horizontally scalable. As pub-sub is happening entirely in-memory, it is not possible to distribute    the work to multiple application instances. The only way to handle more load using this architecture is to scale    vertically, e.g.<ul> <li>Increasing <code>ExecutorService</code> thread pool sizes (<code>alpine.worker.threads</code>, <code>alpine.worker.thread.multiplier</code>)</li> <li>Increasing database connection pool sizes</li> <li>Increasing resource allocations for CPU, RAM, and potentially disk / network</li> </ul> </li> <li>No ordering guarantees of events. As multiple threads work on a shared queue of tasks in parallel, there is no way    of enforcing a reliable ordering of events.</li> <li>Limited fault-tolerance. If an instance of Dependency-Track goes down, planned or unplanned, all queued tasks are    gone. Not only does this impact business-as-usual operation, but also limits the times when upgrades can be applied.</li> <li>Shared, multipurpose task queue. A single task queue is used to process all kinds of events. This means that    lots of events of a certain type can \"clog\" the task queue, preventing other types of events from being processed.    This is further amplified if processing of events \"clogging\" the queue relies on external services, introducing further    latency. Ideally, there should be a dedicated queue per event type, so that one busy queue doesn't block others.</li> <li>Prone to race conditions. As a consequence of (2), it is possible that multiple events addressing the same    thing are processed in parallel, leading to race conditions in cache lookups or database operations. Race conditions    would be an even bigger problem if the work was shared across multiple application instances, and would require    distributed locking as a countermeasure, which is inherently hard to get right.</li> </ol> <p>In order to scale Dependency-Track beyond its current capabilities, a distributed messaging service is required.</p>"},{"location":"#related-issues","title":"Related Issues","text":"<p>On multiple occasions in the past, the Dependency-Track community raised questions about high availability (HA) deployments, and / or how to better scale the platform:</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/issues/218</li> <li>https://github.com/DependencyTrack/dependency-track/issues/903</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1210</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1856</li> </ul>"},{"location":"architecture/","title":"Overview","text":""},{"location":"architecture/#components","title":"Components","text":""},{"location":"architecture/#api-server","title":"API Server","text":"<p>TBD</p>"},{"location":"architecture/#message-broker","title":"Message Broker","text":"<p>TBD</p>"},{"location":"architecture/#database","title":"Database","text":"<p>TBD</p>"},{"location":"architecture/#vulnerability-analyzer","title":"Vulnerability Analyzer","text":"<p>TBD</p>"},{"location":"architecture/#repository-metadata-analyzer","title":"Repository Metadata Analyzer","text":"<p>TBD</p>"},{"location":"architecture/#notification-publisher","title":"Notification Publisher","text":"<p>TBD</p>"},{"location":"architecture/design/workflow-state-tracking/","title":"Tracking of Workflow State for BOM Processing and Analysis","text":"<p>Note</p> <p>This document was extracted from #664.</p> <p>For CI/CD use-cases, Dependency-Track offers a mechanism that allows clients to poll whether the BOM they just uploaded is still being processed. \"Processing\" in this context refers to:</p> <ul> <li>Consumption (Parsing)</li> <li>Ingestion (Sync parsed data with database)</li> <li>Vulnerability analysis</li> <li>Policy Evaluation</li> </ul> <p>This is an important capability, allowing for implementation of quality gates and reporting in CI/CD pipelines.</p> <p>The mechanism works by assigning identifiers (UUIDs) to events in Alpine's in-memory event system. As long as an event associated to a given identifier can be found in its internal queue, the identifier is considered to be \"processing\":</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/blob/6153d286d1ac806462bc76cfe17d84a57c224671/src/main/java/org/dependencytrack/resources/v1/BomResource.java#L323-L342</li> <li>https://github.com/stevespringett/Alpine/blob/cd6aa7ed05376935ab32bc43819eba0e3a525b7f/alpine-infra/src/main/java/alpine/event/framework/BaseEventService.java#L158-L161</li> </ul> <p>Events can be chained, such that a <code>BomUploadEvent</code> will trigger a <code>VulnerabilityAnalysisEvent</code>, which will trigger a <code>PolicyEvaluationEvent</code>, and so on. The event identifier is inherited by chained events.</p> <p>As everything happens in-memory, this mechanism does not work when multiple instances of the API server are used in an active-active deployment. For the functionality to continue to work, the state of processing steps must be persistent to external storage.</p> <p>Decoupled from #633.</p>"},{"location":"architecture/design/workflow-state-tracking/#design","title":"Design","text":"<p>Note</p> <p>The goal for now is not to build a multi-purpose workflow engine, but to track state of one specific workflow. In a future iteration, we may invest more time into coming up with a generic workflow engine concept.</p>"},{"location":"architecture/design/workflow-state-tracking/#what-to-track","title":"What to track","text":"<ol> <li>BOM consumption (parsing, de-duplication, etc.)</li> <li>BOM processing (ingestion into database)</li> <li>Vulnerability analysis</li> <li>Repository metadata analysis</li> <li>Policy evaluation</li> <li>Metrics update</li> </ol> <p>The order of execution as of today is as follows:</p> <pre><code>flowchart LR\n    A(BOM_CONSUMPTION) --&gt; B(BOM_PROCESSING)\n    B --&gt; C(VULN_ANALYSIS)\n    B --&gt; D(REPO_META_ANALYSIS)\n    C --&gt; E(POLICY_EVALUATION)\n    E --&gt; F(METRICS_UPDATE)\n    B -. BOM contains no &lt;br/&gt;components to analyze .-&gt; F</code></pre> <p>Note</p> <p>Completion of repository metadata analysis can currently not be tracked. We'll need something similar to what we introduced in https://github.com/DependencyTrack/hyades-apiserver/pull/40 for vulnerability analysis completion tracking. For the initial implementation, it may be OK to not track it.</p>"},{"location":"architecture/design/workflow-state-tracking/#states","title":"States","text":"<p>There are multiple states a processing step can be in:</p> <ul> <li><code>PENDING</code>: Initial state</li> <li><code>COMPLETED</code>: Completion detected; No failures</li> <li><code>FAILED</code>: Completion detected; Failures</li> <li><code>CANCELLED</code>: Never started because a precondition failed<ul> <li>e.g. vulnerability analysis will be cancelled when BOM ingestion failed</li> </ul> </li> <li><code>NOT_APPLICABLE</code>: The step is not applicable to the subject of the analysis<ul> <li>e.g. the analysis was triggered for an existing project, without BOM upload</li> </ul> </li> <li><code>TIMED_OUT</code>: The deadline for the step completion was exceeded; Step is unlikely to complete</li> </ul> <pre><code>stateDiagram-v2\n    [*] --&gt; PENDING\n    PENDING --&gt; COMPLETED\n    COMPLETED --&gt; [*]\n    PENDING --&gt; FAILED\n    FAILED --&gt; [*]\n    PENDING --&gt; CANCELLED\n    CANCELLED --&gt; [*]\n    [*] --&gt; NOT_APPLICABLE\n    NOT_APPLICABLE --&gt; [*]\n    PENDING --&gt; TIMED_OUT\n    TIMED_OUT --&gt; COMPLETED\n    TIMED_OUT --&gt; FAILED</code></pre> <p>Each step is responsible for updating its own state.</p> <p>The overall state can be considered to be complete, if there's no step in <code>PENDING</code> state.</p> <p>When a step failure is detected, a \"failure reason\" message must be persisted. If multiple steps fail, (rough) failure details for each step must be available.</p> <p>Note</p> <p>There should be a deadline mechanism, which automatically transitions steps from <code>PENDING</code> into <code>TIMED_OUT</code> state. Steps in <code>TIMED_OUT</code> state communicate that it is unlikely that a terminal state can be reached (<code>COMPLETED</code>, <code>FAILED</code>). However, it is still possible (e.g. due to significant consumer lag, events may arrive late).</p>"},{"location":"architecture/design/workflow-state-tracking/#workflow","title":"Workflow","text":"<p>The API server will act as workflow orchestrator, kicking off new steps as needed. This implies that completion of a step must be registered by the API server.</p> <p>To illustrate how the initial stage of the workflow should look like:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ BomResource: Upload BOM\n    BomResource -&gt;&gt; BomResource: Validate BOM\n    BomResource -&gt;&gt; BomResource: Generate Correlation ID (UUID)\n    BomResource -&gt;&gt; Database: Create workflow steps\n    Note over BomResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=BOM_CONSUMPTION&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=BOM_PROCESSING&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Status=PENDING\n    BomResource -&gt;&gt; BomUploadProcessingTask: Dispatch BomUploadEvent\n    BomResource -&gt;&gt;- Client: Correlation ID\n    par\n        loop Continuously\n            Client -&gt;&gt; BomResource: Poll Status\n            Note over Client, BomResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n        end\n    and\n        BomUploadProcessingTask -&gt;&gt; Database: Update step start time\n        activate BomUploadProcessingTask\n        Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;StartedAt=&lt;NOW&gt;\n        BomUploadProcessingTask -&gt;&gt; BomUploadProcessingTask: Consume BOM&lt;br/&gt;(Parse, De-dupe)\n        alt Consumption succeeded\n            BomUploadProcessingTask -&gt;&gt; Database: Update step status\n            Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Process BOM&lt;br/&gt;(Insert, Update, Delete)\n            alt Processing succeeded\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.repo-meta-analysis.component\" topic\n            else Processing failed\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=VULN_ANALYSIS&lt;br/&gt;Status=CANCELLED\n            end\n        else Consumption failed\n            BomUploadProcessingTask -&gt;&gt; Database: Update step Status\n            Note over BomUploadProcessingTask, Database: Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=CANCELLED\n        end\n        deactivate BomUploadProcessingTask\n    end</code></pre> <p>When triggering a re-analysis of an already existing project, the workflow may be kicked off as follows:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ FindingResource: Re-Analyze project\n    FindingResource -&gt;&gt; FindingResource: Generate Correlation ID (UUID)\n    FindingResource -&gt;&gt; Database: Create workflow steps\n    Note over FindingResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=POLICY_EVALUATION&lt;br/&gt;Status=PENDING\n    FindingResource -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n    FindingResource -&gt;&gt;- Client: Correlation ID\n    loop Continuously\n        Client -&gt;&gt; FindingResource: Poll Status\n        Note over Client, FindingResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n    end</code></pre>"},{"location":"architecture/design/workflow-state-tracking/#proposed-table-schema","title":"Proposed Table Schema","text":"<p>Each step of the workflow will be represented in a dedicated row. This allows us to add or remove steps without altering the database schema (see original version of the schema further down below), or even add steps while the workflow is running. It also plays better with concurrent writes, as no two threads / instances will need to modify the same row.</p> Name Type Nullable Example ID <code>SERIAL</code> \u274c 1 PARENT_STEP_ID <code>SERIAL FK</code> \u2705 0 TOKEN <code>VARCHAR(36)</code> \u274c <code>484d9eaa-7ea4-4476-97d6-f36327b5a626</code> STARTED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> UPDATED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> STEP <code>VARCHAR(64)</code> \u274c <code>METRICS_UPDATE</code> STATUS <code>VARCHAR(64)</code> \u274c <code>PENDING</code> FAILURE_REASON <code>TEXT</code> \u2705 <code>Failed to acquire database connection</code> <p>Potential Future Improvements: * Do we need/want to capture the order in which steps are supposed to be executed? * Do we need/want to capture metadata of the overall workflow (who triggered it, when was it triggered, correlation id, ...)?</p> <p>Where applicable, the \"detailed\" status of a step is tracked in a dedicated table.</p>"},{"location":"architecture/design/workflow-state-tracking/#retention","title":"Retention","text":"<p>Rows in the table should be cleaned up on a recurring basis.</p> <p>This could be as simple as scheduling a job that executes this SQL query:</p> <pre><code>DELETE FROM org.dependencytrack.repometaanalyzer.model.WorkflowStep WHERE this.updatedAt &lt; :threshold\n</code></pre> <p>A retention time of 1-3 days since the last update should be reasonable.</p>"},{"location":"architecture/design/workflow-state-tracking/#rest-api-endpoints","title":"REST API endpoints","text":"<p>The existing endpoint to check whether a BOM is still being processed should continue to work as expected:</p> <p><pre><code>GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;\n</code></pre> <pre><code>{\n  \"processing\": true\n}\n</code></pre></p> <p>An additional endpoint may be added, which allows for retrieval of the individual step states:</p> <p><pre><code>GET /api/v1/workflow/token/&lt;CORRELATION_ID&gt;/status\n</code></pre> <pre><code>[\n  {\n    \"step\": \"BOM_CONSUMPTION\",\n    \"status\": \"COMPLETED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\"\n  },\n  {\n    \"step\": \"BOM_PROCESSING\",\n    \"status\": \"FAILED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\",\n    \"failureReason\": \"Failed to acquire database connection\"\n  },\n  {\n    \"step\": \"VULN_ANALYSIS\",\n    \"status\": \"CANCELLED\"\n  }\n]\n</code></pre></p> <p>If all a client cares about are vulnerability analysis results, they could stop polling immediately after <code>vulnerabilityAnalysis</code> transitions into the <code>COMPLETED</code> state.</p>"},{"location":"development/building/","title":"Building","text":""},{"location":"development/building/#hyades","title":"<code>hyades</code>","text":"<p>This segment provides build instructions for the <code>DependencyTrack/hyades</code> repository.</p>"},{"location":"development/building/#jars","title":"JARs","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build</code> run configuration.</p> <p>To build JARs for all modules in the repository:</p> <pre><code>mvn clean package -DskipTests\n</code></pre> <p>For application modules, this will produce a Quarkus fast-jar in their respective <code>target</code> directory.</p> <p>To only build JARs for specific modules, use Maven's <code>-pl</code> flag:</p> <pre><code>mvn -pl mirror-service,vulnerability-analyzer clean package -DskipTests\n</code></pre> <p>Note</p> <p>If you made changes to shared modules (e.g. <code>commons</code>), those changes may not be visible to other modules when building specific modules as shown above. Either include the shared modules in the <code>-pl</code> argument, or run <code>mvn clean install -DskipTests</code> beforehand.</p>"},{"location":"development/building/#containers","title":"Containers","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build Container Images</code> run configuration.</p> <p>To build JARs and container images for all modules in the repository:</p> <pre><code>mvn clean package \\\n  -Dquarkus.container-image.build=true \\\n  -Dquarkus.container-image.additional-tags=local \\\n  -DskipTests=true\n</code></pre> <p>As demonstrated before, you can use Maven's <code>-pl</code> flag to limit the build to specific modules.</p> <p>The resulting container images are tagged as:</p> <pre><code>ghcr.io/dependencytrack/hyades-${moduleName}:local\n</code></pre> <p>For example:</p> <pre><code>ghcr.io/dependencytrack/hyades-vulnerability-analyzer:local\n</code></pre>"},{"location":"development/building/#native-executables","title":"Native Executables","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build Native</code> run configuration.</p> <p>Application modules in the <code>hyades</code> repository can be compiled to native executables, by leveraging GraalVM Native Image.</p> <p>Warning</p> <p>Building native executables is resource intensive and can take a few minutes to complete.</p> <p>Note</p> <p>Building native executables requires GraalVM for JDK 21 or newer. You can install GraalVM with <code>sdkman</code>: https://sdkman.io/jdks#graalce</p> <p>To build native executables for all modules:</p> <pre><code>export GRAALVM_HOME=\"$(sdk home java 21.0.2-graalce)\"\nmvn clean package -Dnative -DskipTests\n</code></pre> <p>As demonstrated before, you can use Maven's <code>-pl</code> flag to limit the build to specific modules.</p> <p>If installing GraalVM is not possible, or you want to use the native executables in a container, but your host system is not Linux, you can leverage a GraalVM container to perform the build:</p> <pre><code>mvn clean package -Dnative -DskipTests -Dquarkus.native.container-build=true\n</code></pre>"},{"location":"development/building/#hyades-apiserver","title":"<code>hyades-apiserver</code>","text":"<p>This segment provides build instructions for the <code>DependencyTrack/hyades-apiserver</code> repository.</p>"},{"location":"development/building/#jar","title":"JAR","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build API Server</code> run configuration.</p> <p>To build an executable JAR:</p> <pre><code>mvn clean package \\\n  -Pclean-exclude-wars \\\n  -Penhance \\\n  -Pembedded-jetty \\\n  -DskipTests \\\n  -Dlogback.configuration.file=src/main/docker/logback.xml\n</code></pre> <p>The resulting file is placed in <code>./target</code> as <code>dependency-track-apiserver.jar</code>. The JAR ships with an embedded Jetty server, there's no need to deploy it in an application server like Tomcat or WildFly.</p>"},{"location":"development/building/#container","title":"Container","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build API Server Image</code> run configuration.</p> <p>Ensure you've built the API server JAR as outlined above.</p> <p>To build the API server image:</p> <pre><code>docker build \\\n  --build-arg WAR_FILENAME=dependency-track-apiserver.jar \\\n  -t ghcr.io/dependencytrack/hyades-apiserver:local \\\n  -f ./src/main/docker/Dockerfile \\\n  .\n</code></pre>"},{"location":"development/building/#hyades-frontend","title":"<code>hyades-frontend</code>","text":"<p>This segment provides build instructions for the <code>DependencyTrack/hyades-frontend</code> repository.</p>"},{"location":"development/building/#distribution","title":"Distribution","text":"<p>To build the frontend using webpack:</p> <pre><code>npm run build\n</code></pre> <p>The build artifacts are placed in <code>./dist</code>. The contents of <code>dist</code> can be deployed to any webserver capable of serving static files.</p>"},{"location":"development/building/#container_1","title":"Container","text":"<p>Ensure you've built the frontend as outlined above.</p> <p>To build the frontend image:</p> <pre><code>docker build -f docker/Dockerfile.alpine -t ghcr.io/dependencytrack/hyades-frontend:local .\n</code></pre>"},{"location":"development/database-migrations/","title":"Database Migrations","text":""},{"location":"development/database-migrations/#introduction","title":"Introduction","text":"<p>In contrast to Dependency-Track v4 and earlier, Dependency-Track v5 manages database migrations with Liquibase. The database schema is still owned by the API server though. It will execute migrations upon startup, unless explicitly disabled via <code>database.run.migrations</code>.</p> <p>Liquibase operates with the concept of changelogs. For the sake of better visibility, Dependency-Track uses separate changelogs for each release version. Individual changelogs are referenced by <code>changelog-main.xml</code>.</p> <p>Stored procedures and custom SQL functions are treated differently: They are re-created whenever their content changes. Their sources are located in the <code>procedures</code> directory.</p>"},{"location":"development/database-migrations/#adding-migrations","title":"Adding Migrations","text":"<ol> <li>If it doesn't exist already, create a <code>changelog-vX.Y.Z.xml</code> file<ul> <li><code>X</code>, <code>Y</code>, and <code>Z</code> must correspond to the current release version</li> </ul> </li> <li>Ensure the <code>changelog-vX.Y.Z.xml</code> file is referenced via <code>include</code> in <code>changelog-main.xml</code></li> <li>Add your changeset to <code>changelog-vX.Y.Z.xml</code></li> </ol> <p>When adding a new <code>changeset</code>, consider the following guidelines:</p> <ul> <li>The changeset ID must follow the <code>vX.Y.Z-&lt;NUM&gt;</code> format, where:<ul> <li><code>X</code>, <code>Y</code>, and <code>Z</code> match the changelog's version</li> <li><code>NUM</code> is an incrementing number, starting at <code>1</code> for the first <code>changeset</code> of the release</li> </ul> </li> <li>The <code>author</code> must correspond to your GitHub username</li> <li>Prefer built-in change types<ul> <li>Use the <code>sql</code> change type if no fitting built-in exists</li> <li>Use a custom change in edge cases, when additional computation is required</li> </ul> </li> <li>When using custom changes:<ul> <li>Use the <code>org.dependencytrack.persistence.migration.change</code> package</li> <li>Changes must not depend on domain logic</li> </ul> </li> <li>You must not modify <code>changeset</code>s that were already committed to <code>main</code></li> </ul>"},{"location":"development/database-migrations/#making-schema-changes-available-to-hyades-services","title":"Making Schema Changes Available to Hyades Services","text":"<p>Because the schema is owned by the API server, and the API server is also responsible for executing migrations, other services that access the database must replicate the current schema, in order to run tests against it.</p> <p>Currently, this is achieved by:</p> <ol> <li>Having Liquibase generate the schema SQL based on the changelog</li> <li>Adding the <code>schema.sql</code> file as resource to the <code>commons-persistence</code> module</li> <li>Having all services that require database access depend on <code>commons-persistence</code></li> <li>Configuring Quarkus Dev Services to initialize new database containers with <code>schema.sql</code><ul> <li>Using <code>quarkus.datasource.devservices.init-script-path</code></li> </ul> </li> </ol> <p>The schema can be generated using the <code>dbschema-generate.sh</code> script in the <code>hyades-apiserver</code> repository:</p> <pre><code>./dev/scripts/dbschema-generate.sh\n</code></pre> <p>Note</p> <ul> <li>You may need to build the API server project once before running the script</li> <li>Because Liquibase requires database to run against, the script will launch a temporary PostgreSQL container</li> </ul> <p>The output is written to <code>target/liquibase/migrate.sql</code>.</p>"},{"location":"development/documentation/","title":"Documentation","text":""},{"location":"development/documentation/#introduction","title":"Introduction","text":"<p>User-facing documentation is implemented with MkDocs and Material for MkDocs. The sources are located in <code>docs</code>. Changes to the documentation are automatically deployed to GitHub pages, using the <code>deploy-docs.yml</code> GitHub Actions workflow. Once deployed, the documentation is available at https://dependencytrack.github.io/hyades/snapshot.</p>"},{"location":"development/documentation/#versioning","title":"Versioning","text":"<p>Documentation is published for each version of the project, including unstable <code>SNAPSHOT</code> versions. This allows users to browse the docs most relevant to their Dependency-Track deployment.</p> Version selection on the documentation site <p>Documentation for unstable versions is aliased as <code>snapshot</code>, whereas for stable builds it is aliased as <code>latest</code>. They are accessible via <code>/snapshot</code> and <code>/latest</code> respectively.</p> <p>Tip</p> <p>When sharing links to the docs with others, prefer using specific versions instead of <code>latest</code> or <code>snapshot</code>. For example https://dependencytrack.github.io/hyades/0.4.0. This ensures that your links will not break as documentation evolves.</p> <p>The versioning logic is handled by mike as part of the <code>deploy-docs.yml</code> workflow.</p>"},{"location":"development/documentation/#local-development","title":"Local Development","text":"<p>For local building and rendering of the docs, use the  <code>docs-dev.sh</code> script:</p> <pre><code>./scripts/docs-dev.sh\n</code></pre> <p>It will launch a development server that listens on http://localhost:8000 and reloads whenever changes are made to the documentation sources. The script requires the <code>docker</code> command to be available.</p>"},{"location":"development/documentation/#configuration-documentation","title":"Configuration Documentation","text":"<p>To make it easier for users to discover available configuration options (i.e. environment variables), we generate human-readable documentation for it. You can see the result of this here.</p>"},{"location":"development/documentation/#applicationproperties-annotations","title":"<code>application.properties</code> Annotations","text":"<p>We leverage comments on property definitions to gather metadata. Other than a property's description, the following annotations are supported to provide further information:</p> Annotation Description <code>@category</code> Allows for categorization / grouping of related properties <code>@default</code> To be used for cases where the default value is implicit, for example when it is inherited from the framework or other properties <code>@example</code> To give an idea of what a valid value may look like, when it's not possible to provide a sensible default value <code>@hidden</code> Marks a property as to-be-excluded from the generated docs <code>@required</code> Marks a property as required <code>@type</code> Defines the type of the property <p>For example, a properly annotated property might look like this:</p> <pre><code># Defines the path to the secret key to be used for data encryption and decryption.\n# The key will be generated upon first startup if it does not exist.\n#\n# @category: General\n# @default:  ${alpine.data.directory}/keys/secret.key\n# @type:     string\nalpine.secret.key.path=\n</code></pre> <p>It is also possible to index properties that are commented out, for example:</p> <pre><code># Foo bar baz.\n#\n# @category: General\n# @example:  Some example value\n# @type:     string\n# foo.bar.baz=\n</code></pre> <p>This can be useful when it's not possible to provide a sensible default, and providing the property without a value would break something. Generally though, you should always prefer setting a sensible default.</p> <p>If a property depends on another property, or relates to it, mention it in the description. A deep-link will automatically be generated for it. For example:</p> Example of a generated deep-link for <code>alpine.cors.enabled</code>"},{"location":"development/documentation/#generation","title":"Generation","text":"<p>Configuration documentation is generated from <code>application.properties</code> files. We use the <code>GenerateConfigDocs</code> JBang script for this:</p> <pre><code>Usage: GenerateConfigDocs [--include-hidden] [-o=OUTPUT_PATH] -t=TEMPLATE_FILE\n                          PROPERTIES_FILE\n      PROPERTIES_FILE        The properties file to generate documentation for\n      --include-hidden       Include hidden properties in the output\n  -o, --output=OUTPUT_PATH   Path to write the output to, will write to STDOUT\n                               if not provided\n  -t, --template=TEMPLATE_FILE\n                             The Pebble template file to use for generation\n</code></pre> <p>Tip</p> <p>Usually you do not need to run the script yourself. We have a GitHub Actions workflow (<code>update-config-docs.yml</code>)  that does that automatically whenever a modification to <code>application.properties</code> files is pushed to the <code>main</code> branch. It also works across repositories, i.e. it will be triggered for changes in the <code>hyades-apiserver</code> repository as well.</p> <p>To generate documentation for the API server, you would run:</p> <pre><code>jbang scripts/GenerateConfigDocs.java \\\n    -t ./scripts/config-docs.md.peb \\\n    -o ./docs/reference/configuration/api-server.md \\\n    ../hyades-apiserver/src/main/resources/application.properties\n</code></pre> <p>Output is generated based on a customizable Pebble template  (currently <code>config-docs.md.peb</code>).</p>"},{"location":"development/overview/","title":"Overview","text":"<p>Want to hack on Hyades, the upcoming Dependency-Track v5? Awesome, here's what you need to know to get started!</p> <p>Important</p> <p>Please be sure to read <code>CONTRIBUTING.md</code> and <code>CODE_OF_CONDUCT.md</code> as well.</p>"},{"location":"development/overview/#repositories","title":"Repositories","text":"<p>The project consists of the following repositories:</p> Repository Description DependencyTrack/hyades Main repository. Includes Hyades services, end-to-end tests, documentation, and deployment manifests. GitHub issues and discussions are managed here. DependencyTrack/hyades-apiserver Fork of <code>DependencyTrack/dependency-track</code>.  GitHub issues and discussions are disabled. DependencyTrack/hyades-frontend Fork of <code>DependencyTrack/frontend</code>.  GitHub issues and discussions are disabled. <p>Note</p> <p>The <code>hyades</code> and <code>hyades-apiserver</code> repositories are split for historical reasons. We are planning to merge them, which should result in less overhead and more opportunities for code sharing.</p> <p>To clone them all:</p> <pre><code>git clone https://github.com/DependencyTrack/hyades.git\ngit clone https://github.com/DependencyTrack/hyades-apiserver.git\ngit clone https://github.com/DependencyTrack/hyades-frontend.git\n</code></pre>"},{"location":"development/overview/#prerequisites","title":"Prerequisites","text":"<p>There are a few things you'll need on your journey:</p> <ul> <li>Java Development Kit<sup>1</sup> &gt;=21 (Temurin distribution recommended)</li> <li>Maven<sup>1</sup> &gt;=3.9 (comes bundled with IntelliJ and Eclipse)</li> <li>NodeJS<sup>2</sup> &gt;=20</li> <li>A Java and JavaScript capable editor or IDE of your preference (we recommend IntelliJ<sup>3</sup>)</li> <li>Docker or Podman</li> <li>Docker Compose or Podman Compose</li> </ul> Tip <p><sup>1</sup> We recommend sdkman to install Java and Maven. When working in a corporate environment, you should obviously prefer the packages provided by your organization.</p> <p><sup>2</sup> If you need to juggle multiple NodeJS versions on your system, consider using nvm to make this more bearable.</p> <p><sup>3</sup> We provide common run configurations for IntelliJ in the <code>.idea/runConfigurations</code> directories of each repository for convenience. IntelliJ will automatically pick those up when you open this repository.</p>"},{"location":"development/overview/#core-technologies","title":"Core Technologies","text":"<p>Knowing about the core technologies may help you with understanding the code base.</p>"},{"location":"development/overview/#infrastructure","title":"Infrastructure","text":"Technology Purpose PostgreSQL Database Apache Kafka Messaging / Streaming"},{"location":"development/overview/#api-server","title":"API Server","text":"Technology Purpose JAX-RS REST API specification Jersey JAX-RS implementation Java Data Objects (JDO) Persistence specification DataNucleus JDO implementation JDBI Lightweight database operations Liquibase Database migrations Confluent Parallel Consumer Kafka message processing Jetty Servlet Container Alpine Framework / Scaffolding Maven Package manager / Build tool Java Programming language"},{"location":"development/overview/#hyades-services","title":"Hyades Services","text":"Technology Purpose Kafka Streams Stream processing Quarkus Framework / Scaffolding Maven Package manager / Build tool Java Programming language"},{"location":"development/overview/#frontend","title":"Frontend","text":"Technology Purpose webpack Asset bundler Vue.js Framework NPM Package manager / Build tool JavaScript Programming language"},{"location":"development/testing/","title":"Testing","text":""},{"location":"development/testing/#introduction","title":"Introduction","text":"<p>We generally aim for a test coverage of ~80%. This is also true for new code introduced through pull requests. We value integration tests more than unit tests, and try to avoid using mocks as much as possible. If reaching the 80% test coverage requires us to write tests that don't really test anything meaningful, or require loads of mocking, we rather take lower coverage than writing those tests.</p> <p>We use Testcontainers, Wiremock, and GreenMail to test how the system interacts with the outside world.</p>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":""},{"location":"development/testing/#hyades","title":"<code>hyades</code>","text":"<p>TBD</p>"},{"location":"development/testing/#hyades-apiserver","title":"<code>hyades-apiserver</code>","text":"<p>Warning</p> <p>To reduce execution time of the test suite in CI, the PostgreSQL Testcontainer is reused. While tables are truncated after each test, sequences (e.g. for <code>ID</code> columns) won't be reset. As a consequence, you should not assert on IDs of database records.</p>"},{"location":"development/testing/#hyades-frontend","title":"<code>hyades-frontend</code>","text":"<p>There are currently no unit tests for the frontend.</p>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":""},{"location":"development/testing/#hyades_1","title":"<code>hyades</code>","text":"<p>Integration tests in the <code>hyades</code> repository are implemented as @QuarkusIntegrationTest.  As such, they are executed against an actual build artifact (JAR, container, or native executable).</p> <p>Class names of integration tests are suffixed with <code>IT</code> instead of <code>Test</code>.</p>"},{"location":"development/testing/#execution","title":"Execution","text":"<p>Integration tests can be launched individually through your IDE, or all at once using Maven:</p> <pre><code>mvn -pl '!e2e' clean verify failsafe:integration-test -DskipITs=false\n</code></pre> <p>To limit the test run to specific modules, use <code>-pl &lt;module&gt;</code>, for example:</p> <pre><code>mvn -pl vulnerability-analyzer clean verify failsafe:integration-test -DskipITs=false\n</code></pre>"},{"location":"development/testing/#execution-in-ci","title":"Execution in CI","text":"<p>In CI, integration tests are executed:</p> <ul> <li>Against all JARs, as part of the <code>CI / Test</code> workflow</li> <li>Against native executables, as part of the <code>CI / Test Native Image</code> workflow(s)</li> </ul> <p>Both workflows run for pushes and pull requests to the <code>main</code> branch.</p>"},{"location":"development/testing/#hyades-apiserver_1","title":"<code>hyades-apiserver</code>","text":"<p>The API server repository does not currently differentiate between unit- and integration-tests.</p>"},{"location":"development/testing/#hyades-frontend_1","title":"<code>hyades-frontend</code>","text":"<p>There are currently no integration tests for the frontend.</p>"},{"location":"development/testing/#end-to-end-tests","title":"End-to-End Tests","text":"<p>End-to-End tests spin up containers for all services of the system. The test environment is torn down and rebuilt for every test case. Containers are started and managed using Testcontainers.</p> <p>The tests are located in the <code>e2e</code> module of the <code>hyades</code> repository. Container images used are defined in the <code>AbstractE2ET</code> class.</p> <p>Image versions can be overwritten using the following environment variables:</p> <ul> <li><code>APISERVER_VERSION</code></li> <li><code>HYADES_VERSION</code></li> </ul>"},{"location":"development/testing/#execution_1","title":"Execution","text":"<p>Tests can be launched individually through your IDE, or all at once using Maven:</p> <pre><code>mvn -pl e2e clean verify -Pe2e-all\n</code></pre> <p>To test against local changes:</p> <ol> <li>Build container images for the modified services</li> <li>Update the image tags in <code>AbstractE2ET</code> accordingly</li> <li>Run e2e tests as detailed above</li> </ol>"},{"location":"development/testing/#execution-in-ci_1","title":"Execution in CI","text":"<p>In CI, end-to-end tests are executed for every push to the <code>main</code> branch, as well as every night at 12AM.</p> <p>They can additionally be run manually, via the GitHub Actions UI. Both the API server and Hyades version can be customized before execution.</p>"},{"location":"development/testing/#manual-tests","title":"Manual Tests","text":""},{"location":"development/testing/#docker-compose","title":"Docker Compose","text":"<p>The easiest way to test the entire system is by using Docker Compose.</p> <p>A <code>docker-compose.yml</code> file is provided in the <code>DependencyTrack/hyades</code> repository.</p> <p>Without any profile specified, <code>docker compose up -d</code> will launch:</p> <ul> <li>PostgreSQL</li> <li>Kafka (currently Redpanda)</li> <li>Kafka UI (currently Redpanda Console)</li> </ul> <p>To launch Dependency-Track services, use the <code>demo</code> profile:</p> <pre><code>docker compose up -d --profile demo\n</code></pre> <p>To test different versions of the services, simply modify the <code>image</code> property of the respective Compose <code>service</code>.</p>"},{"location":"development/testing/#api-server","title":"API Server","text":"<p>When testing changes that are limited to the API server, such as updates to the REST API, it's possible to launch the API server in dev services mode:</p> <pre><code>mvn -Penhance -Pdev-services jetty:run -Dlogback.configurationFile=src/main/docker/logback.xml\n</code></pre> <p>The container images used may be configured via:</p> <ul> <li><code>dev.services.image.frontend</code></li> <li><code>dev.services.image.kafka</code></li> <li><code>dev.services.image.postgresql</code></li> </ul>"},{"location":"getting-started/changes-over-v4/","title":"Changes over v4","text":""},{"location":"getting-started/changes-over-v4/#new-features","title":"New Features","text":"<ul> <li>New powerful CEL-based policy engine, providing more flexibility while being more efficient than the engine shipped with v4. </li> <li>Ability to automatically audit vulnerabilities across the entire portfolio using CEL expressions. </li> <li>Hash-based integrity analysis for components. </li> <li>The API server now supports high availability (HA) deployments in active-active configuration.</li> <li>Zero downtime deployments when running API server in HA configuration.</li> <li>Greatly reduced resource footprint of the API server.</li> <li>The status of asynchronous tasks (e.g. vulnerability analysis) is now   tracked in a persistent manner,   improving observability.</li> </ul>"},{"location":"getting-started/changes-over-v4/#architecture-operations","title":"Architecture / Operations","text":"<ul> <li>PostgreSQL is the only supported database.<ul> <li>Support for H2, MySQL, and Microsoft SQL Server is dropped.</li> </ul> </li> <li>To facilitate communication between services, a Kafka-compatible broker is required.</li> <li>Publishing of notifications, fetching component metadata from repositories, and vulnerability analysis is performed by services separately from the API server.<ul> <li>The services can be scaled up and down as needed.</li> <li>Some services (i.e. <code>notification-publisher</code>) can be omitted entirely from a deployment,   if publishing of notification via e.g. Webhook is not needed.</li> </ul> </li> <li>All services except the API server can optionally be deployed as native executables (thanks to GraalVM), offering a lower resource footprint than their JVM-based counterparts.</li> <li>Database migrations are performed through a more reliable, changelog-based approach.</li> </ul>"},{"location":"getting-started/changes-over-v4/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>All deprecated endpoints mentioned below were removed:<ul> <li>POST <code>/api/v1/policy/{policyUuid}/tag/{tagName}</code></li> <li>DELETE <code>/api/v1/policy/{policyUuid}/tag/{tagName}</code></li> <li>GET <code>/api/v1/tag/{policyUuid}</code></li> <li>GET <code>/api/v1/bom/token/{uuid}</code></li> </ul> </li> </ul>"},{"location":"getting-started/changes-over-v4/#notifications","title":"Notifications","text":"<ul> <li><code>subject</code> objects passed to notification templates are now objects generated from Protobuf definitions.<ul> <li>The respective schema is defined in notification.proto.</li> <li>List fields now have a <code>List</code> suffix (i.e. <code>vulnerabilities</code> -&gt; <code>vulnerabilitiesList</code>).</li> </ul> </li> <li>Level values are now prefixed with <code>LEVEL_</code><ul> <li>Before: <code>INFORMATIONAL</code></li> <li>Now: <code>LEVEL_INFORMATIONAL</code></li> </ul> </li> <li>Scope values are now prefixed with <code>SCOPE_</code><ul> <li>Before: <code>SYSTEM</code></li> <li>Now: <code>SCOPE_SYSTEM</code></li> </ul> </li> <li>Group values are now prefixed with <code>GROUP_</code><ul> <li>Before: <code>NEW_VULNERABILITY</code></li> <li>Now: <code>GROUP_NEW_VULNERABILITY</code></li> </ul> </li> <li>The <code>timestamp</code> value passed to notification templates is now consistently formatted with three fractional digits.<ul> <li>Before, any of:<ul> <li><code>1970-01-01T00:11:06Z</code></li> <li><code>1970-01-01T00:11:06.000Z</code></li> <li><code>1970-01-01T00:11:06.000000Z</code></li> <li><code>1970-01-01T00:11:06.000000000Z</code></li> </ul> </li> <li>Now: <code>1970-01-01T00:11:06.000Z</code></li> </ul> </li> </ul>"},{"location":"getting-started/changes-over-v4/#search","title":"Search","text":"<ul> <li>The API server no longer maintains Lucene indexes.<ul> <li>The local <code>~/.dependency-track/index</code> directory is no longer required.</li> </ul> </li> <li>All REST endpoints under <code>/api/v1/search</code> were removed.</li> <li>Fuzzy matching for the internal analyzer is no longer supported.</li> </ul>"},{"location":"getting-started/migrating-from-v4/","title":"Migrating from v4","text":""},{"location":"getting-started/migrating-from-v4/#introduction","title":"Introduction","text":"<p>If you're currently running a Dependency-Track v4 deployment, don't worry!</p> <p>We're aiming to provide tooling and guides on how to migrate to v5 once it reaches general availability. The goal is to offer tools that perform the migration automatically, with little to no manual effort (except the provisioning of infrastructure of course).</p> <p>Tip</p> <p>Follow https://github.com/DependencyTrack/hyades/issues/881 for updates on this topic.</p>"},{"location":"getting-started/migrating-from-v4/#running-v4-and-v5-in-parallel","title":"Running v4 and v5 in parallel","text":"<p>Given an existing production deployment of v4, it can be helpful to run a v5 test deployment in parallel, to compare behavior and testing new features on real data.</p> <p>This can be achieved by leveraging notifications, in particular <code>BOM_PROCESSED</code> notifications. They are emitted by Dependency-Track after a BOM's contents are synchronized with the database.</p> <p>The subject of <code>BOM_PROCESSED</code> notifications contains the original BOM that way uploaded, encoded in Base64. It also contains the name and version of the project it was uploaded to. This information is sufficient to  construct a BOM upload request, that can be submitted to another Dependency-Track instance.</p> <p>All that's needed is an application that can:</p> <ul> <li>Receive Webhooks, and parse the JSON payload within them</li> <li>Perform a mapping from notification subject, to BOM upload request</li> <li>Forward the BOM upload request to another Dependency-Track instance</li> </ul> <p>This can, of course, be scripted. However, we recommend using Bento, which reduces it all to a single config file.</p> <p>Tip</p> <p>You can use the same approach outlined here to construct a pre-prod / staging environment.</p> <p>Conceptually, this is what the setup will accomplish:</p> <pre><code>sequenceDiagram\n    Client-&gt;&gt;DT v4: Upload BOM&lt;br/&gt;PUT /api/v1/bom\n    DT v4-&gt;&gt;DT v4: Validate and&lt;br/&gt;Process\n    DT v4-&gt;&gt;Bento: Notification&lt;br/&gt;BOM_PROCESSED\n    Bento-&gt;&gt;Bento: Map to BOM&lt;br/&gt;upload request\n    Bento-&gt;&gt;DT v5: Upload BOM&lt;br/&gt;PUT /api/v1/bom\n    DT v5-&gt;&gt;DT v5: Validate and&lt;br/&gt;Process</code></pre>"},{"location":"getting-started/migrating-from-v4/#creating-api-key","title":"Creating API Key","text":"<p>In order to upload BOMs to the Dependency-Track v5 system, an API key with <code>BOM_UPLOAD</code> and <code>PROJECT_CREATION_UPLOAD</code> permissions is required. Log into your Dependency-Track v5 instance, navigate to Administration -&gt; Access Management -&gt; Teams, and create a new team with accompanying API key:</p> <p></p>"},{"location":"getting-started/migrating-from-v4/#deploy-bento","title":"Deploy Bento","text":"<p>Bento works with the concept of pipelines, which are configured via YAML. The following pipeline will achieve the desired outcome:</p> <pre><code>---\ninput:\n  http_server:\n    path: /notification/bom-processed\n    allowed_verbs:\n    - POST\n    timeout: 5s\n    sync_response:\n      status: \"202\"\n\npipeline:\n  processors:\n  - mapping: |\n      root.projectName = this.notification.subject.project.name\n      root.projectVersion = this.notification.subject.project.version\n      root.projectTags = this.notification.subject.project.tags.split(\",\").catch([]).map_each(tag -&gt; {\"name\": tag})\n      root.bom = this.notification.subject.bom.content\n      root.autoCreate = true\n\noutput:\n  http_client:\n    url: \"${DTV5_API_URL}/api/v1/bom\"\n    verb: PUT\n    headers:\n      Content-Type: application/json\n      X-Api-Key: \"${DTV5_API_KEY}\"\n    max_in_flight: 10\n    # tls:\n    #  skip_cert_verify: true\n    #  ^-- Uncomment this if you're using self-signed certificates.\n</code></pre> <p>Refer to the respective pipeline component's documentation for more details:</p> <ul> <li><code>http_server</code> input</li> <li><code>mapping</code> processor</li> <li><code>http_client</code> output</li> </ul> <p>Run Bento as container:</p> <pre><code>docker run -d --name bento \\\n    -p \"4195:4195\" \\\n    -v \"$(pwd)/config.yaml:/bento.yaml\" \\\n    -e 'DTV5_API_URL=https://dtv5.example.com' \\\n    -e 'DTV5_API_KEY=odt_****************' \\\n    ghcr.io/warpstreamlabs/bento\n</code></pre>"},{"location":"getting-started/migrating-from-v4/#configure-notification","title":"Configure Notification","text":"<p>Log into your Dependency-Track v4 instance, navigate to Administration -&gt; Notifications -&gt; Alerts, and create a new alert with the following settings:</p> <ul> <li>Scope: Portfolio</li> <li>Notification level: Informational</li> <li>Publisher: Outbound Webhook</li> </ul> <p></p> <p>Once created, enable <code>BOM_PROCESSED</code> under Groups, and configure the URL of your Bento endpoint as Destination:</p> <p></p>"},{"location":"getting-started/migrating-from-v4/#testing","title":"Testing","text":"<ul> <li>Upload a BOM to a project in your Dependency-Track v4 instance.</li> <li>Head over to your Dependency-Track v5 instance and wait for the upload to replicate.</li> </ul> <p>If all goes well, you're done! Happy testing!</p> <p>Tip</p> <p>If the BOM upload does not replicate:</p> <ol> <li>Check the logs of your Dependency-Track v4 deployment for any errors during notification publishing.</li> <li>Check the logs of Bento for any errors or warnings.</li> <li>Check the logs of your Dependency-Track v5 deployment for any errors during BOM processing.</li> <li>Ensure that the API key you created has the correct permissions.</li> <li>Ensure that Bento is reachable from your Dependency-Track v4 deployment.</li> <li>Ensure that your Dependency-Track v5 deployment is reachable from Bento.</li> </ol>"},{"location":"getting-started/upgrading/","title":"Upgrading","text":""},{"location":"getting-started/upgrading/#upgrading-to-060","title":"Upgrading to 0.6.0","text":"<ul> <li>The <code>kafka.topic.prefix</code> configuration was renamed to <code>dt.kafka.topic.prefix</code> to prevent collisions with native Kafka properties (hyades/#1392).</li> <li> <p>Configuration names for task cron expressions and lock durations have changed (apiserver/#840). They now follow a consistent <code>task.&lt;task-name&gt;.&lt;config&gt;</code> scheme. Lock durations are now specified in ISO 8601 format instead of milliseconds. Refer to the task scheduling configuration reference for details. Example of name change:</p> Before After <code>task.cron.metrics.portfolio</code> <code>task.portfolio.metrics.update.cron</code> <code>task.metrics.portfolio.lockAtMostForInMillis</code> <code>task.portfolio.metrics.update.lock.max.duration</code> <code>task.metrics.portfolio.lockAtLeastForInMillis</code> <code>task.portfolio.metrics.update.lock.min.duration</code> </li> <li> <p>The <code>/api/v1/vulnerability/source/{source}/vuln/{vuln}/projects</code> REST API endpoint now supports pagination (apiserver/#888). Like all other paginated endpoints, the page size defaults to <code>100</code>. Clients currently expecting all items to be returned at once must be updated to deal with pagination.</p> </li> <li> <p>The <code>alpine.</code> prefix was removed from Kafka processor properties of the API server (apiserver/#904). Refer to the kafka configuration reference for details. Example of name change:</p> Before After <code>alpine.kafka.processor.vuln.scan.result.processing.order</code> <code>kafka.processor.vuln.scan.result.processing.order</code> </li> <li> <p>The endpoints deprecated in v4.x mentioned below were removed (apiserver/#910):</p> Removed endpoint Replacement <code>POST /api/v1/policy/{policyUuid}/tag/{tagName}</code> <code>POST /api/v1/tag/{name}/policy</code> <code>DELETE /api/v1/policy/{policyUuid}/tag/{tagName}</code> <code>DELETE /api/v1/tag/{name}/policy</code> <code>GET /api/v1/tag/{policyUuid}</code> <code>GET /api/v1/tag/policy/{uuid}</code> <code>GET /api/v1/bom/token/{uuid}</code> <code>GET /api/v1/event/token/{uuid}</code> </li> </ul>"},{"location":"operations/database/","title":"Database","text":"<p>Dependency-Track requires a PostgreSQL, or PostgreSQL-compatible database to operate.</p> <p>The lowest supported version is 11. You are encouraged to use the newest available version.</p> <p>Depending on available resources, individual preferences, or organizational policies, you will have to choose between a managed, or self-hosted solution.</p>"},{"location":"operations/database/#extensions","title":"Extensions","text":"<p>The following PostgreSQL extensions are required by Dependency-Track. When choosing a hosting solution, verify that the extensions listed here are supported.</p> <ul> <li><code>pg_trgm</code>: Support for similarity of text using trigram matching</li> </ul> <p>Note</p> <p>Dependency-Track will execute the necessary <code>CREATE EXTENSION IF NOT EXISTS</code> statements during schema migration. Enabling extensions manually is not necessary.</p> <p>Generally, we limit usage of extensions to those that:</p> <ol> <li>Ship with PostgreSQL out-of-the-box</li> <li>Are trusted by default</li> </ol> <p>This ensures compatibility with most managed solutions, and reduces setup effort for self-hosted deployments.</p>"},{"location":"operations/database/#managed-solutions","title":"Managed Solutions","text":"<p>The official PostgreSQL website hosts a list of well-known commercial hosting providers.</p> <p>Popular choices include:</p> <ul> <li>Amazon RDS for PostgreSQL</li> <li>Aiven for PostgreSQL</li> <li>Azure Database for PostgreSQL</li> <li>Google Cloud SQL for PostgreSQL</li> </ul> <p>We are not actively testing against cloud offerings. But as a rule of thumb, solutions offering \"vanilla\" PostgreSQL,  or extensions of it (for example Neon or Timescale), will most definitely work with Dependency-Track.</p> <p>The same is not necessarily true for platforms based on heavily modified PostgreSQL, or even entire re-implementations such as CockroachDB or YugabyteDB. Such solutions make certain trade-offs to achieve higher levels of scalability, which might impact functionality that Dependency-Track relies on. If you'd like to see support for those, please let us know!</p>"},{"location":"operations/database/#self-hosting","title":"Self-Hosting","text":""},{"location":"operations/database/#bare-metal-docker","title":"Bare Metal / Docker","text":"<p>For Docker deployments, use the official <code>postgres</code> image.</p> <p>Warning</p> <p>Do not use the <code>latest</code> tag! You may end up doing a major version upgrade without knowing it, ultimately breaking your database! Pin the tag to at least the major version (e.g. <code>16</code>), or better yet the minor version (e.g. <code>16.2</code>). Refer to Upgrades to upgrade instructions.</p> <p>For bare metal deloyments, it's usually best to install PostgreSQL from your distribution's package repository. See for example:</p> <ul> <li>PostgreSQL instructions for Debian</li> <li>Install and configure PostgreSQL on Ubuntu</li> <li>Using PostgreSQL with Red Hat Enterprise Linux</li> </ul> <p>To get the most out of your Dependency-Track installation, we recommend to run PostgreSQL on a separate machine than the application containers. You want PostgreSQL to be able to leverage the entire machine's resources, without being impacted by other applications.</p> <p>For smaller and non-critical deployments, it is totally fine to run everything on a single machine.</p>"},{"location":"operations/database/#basic-configuration","title":"Basic Configuration","text":"<p>You should be aware that the default PostgreSQL configuration is extremely conservative. It is intended to make PostgreSQL usable on minimal hardware, which is great for testing, but can seriously cripple performance in production environments. Not adjusting it to your specific setup will most certainly leave performance on the table.</p> <p>If you're lucky enough to have access to professional database administrators, ask them for help. They will know your organisation's best practices and can guide you in adjusting it for Dependency-Track.</p> <p>If you're not as lucky, we can wholeheartedly recommend PGTune. Given a bit of basic info about your system, it will provide a sensible baseline configuration. For the DB Type option, select <code>Online transaction processing system</code>.</p> <p></p> <p>The <code>postgresql.conf</code> is usually located at <code>/var/lib/postgresql/data/postgresql.conf</code>. Most of these settings require a restart of the application.</p> <p>In a Docker Compose setup, you can alternatively apply the desired configuration via command line flags. For example:</p> <pre><code>services:\n  postgres:\n    image: postgres:16\n    command: &gt;-\n        -c 'shared_buffers=2GB'\n        -c 'effective_cache_size=6GB'\n</code></pre>"},{"location":"operations/database/#advanced-configuration","title":"Advanced Configuration","text":"<p>For larger deployments, you may eventually run into situations where database performance degrades with just the basic configuration applied. Oftentimes, tweaking advanced settings can resolve such problems. But knowing which knobs to turn is a challenge in itself.</p> <p>If you happen to be in this situation, make sure you have database monitoring set up. Changing advanced configuration options blindly can potentially cause more damage than it helps.</p> <p>Below, you'll find a few options that, based on our observations with large-scale deployments, make sense to tweak. Note that some settings are applied system-wide, while others are only applied for certain tables.</p> <p>Note</p> <p>Got more tips to configure or tune PostgreSQL, that may be helpful to others? We'd love to include it in the docs, please do raise a PR!</p>"},{"location":"operations/database/#checkpoint_completion_target","title":"checkpoint_completion_target","text":"Default <ul> <li><code>0.5</code> (PostgreSQL &lt;= 13)</li> <li><code>0.9</code> (PostgreSQL &gt;= 14)</li> </ul> Recommendation <code>0.9</code> Tables <code>*</code> References Documentation <p>Spreads the WAL checkpoint creation across a longer period of time, resulting in a more evenly distributed I/O load. A lower value has been observed to cause undesirable spikes in I/O usage on the database server.</p> <pre><code>ALTER SYSTEM SET CHECKPOINT_COMPLETION_TARGET = 0.9;\n</code></pre>"},{"location":"operations/database/#autovacuum_vacuum_scale_factor","title":"autovacuum_vacuum_scale_factor","text":"Default <code>0.2</code> Recommendation <code>0.02</code> Tables <ul> <li><code>COMPONENT</code></li> <li><code>DEPENDENCYMETRICS</code></li> </ul> References Documentation <p>The default causes Autovacuum to start way too late on large tables with lots of churn, yielding long execution times. Reduction in scale factor causes autovacuum to happen more often, making each execution less time-intensive.</p> <p>The <code>COMPONENT</code> and <code>DEPENDENCYMETRICS</code> table are very frequently inserted into, updated, and deleted from. This causes lots of dead tuples that PostgreSQL needs to clean up. Because autovacuum also performs <code>ANALYZE</code>, slow vacuuming can cause the query planner to choose inefficient execution plans.</p> <pre><code>ALTER TABLE \"COMPONENT\" SET (AUTOVACUUM_VACUUM_SCALE_FACTOR = 0.02);\nALTER TABLE \"DEPENDENCYMETRICS\" SET (AUTOVACUUM_VACUUM_SCALE_FACTOR = 0.02);\n</code></pre>"},{"location":"operations/database/#upgrades","title":"Upgrades","text":"<p>Follow the official upgrading guide. Be sure to select the version of the documentation that corresponds to the PostgreSQL version you are running.</p> <p>Warning</p> <p>Pay attention to the fact that major version upgrades usually require a backup-and-restore cycle, due to potentially breaking changes in the underlying data storage format. Minor version upgrades are usually safe to perform in a rolling manor.</p>"},{"location":"operations/database/#kubernetes","title":"Kubernetes","text":"<p>We generally advise against running PostgreSQL on Kubernetes, unless you really know what you're doing. Wielding heavy machinery such as Postgres Operator is not something you should do lightheartedly.</p> <p>If you know what you're doing, you definitely don't need advice from us. Smooth sailing! \u2693\ufe0f</p>"},{"location":"operations/database/#schema-migrations","title":"Schema Migrations","text":"<p>Schema migrations are performed automatically by the API server upon startup. It leverages Liquibase for doing so. There is usually no manual action required when upgrading from an older Dependency-Track version, unless explicitly stated otherwise in the release notes.</p> <p>This behavior can be turned off by setting <code>database.run.migrations</code>  on the API server container to <code>false</code>.</p> <p>It is possible to use different credentials for migrations than for the application itself. This can be achieved with the following options:</p> <ul> <li><code>database.migration.url</code></li> <li><code>database.migration.username</code></li> <li><code>database.migration.password</code></li> </ul> <p>The above with default to the main database credentials if not provided explicitly.</p>"},{"location":"reference/topics/","title":"Topics","text":"Name Partitions Config <code>dtrack.notification.analyzer</code> 3 <code>dtrack.notification.bom</code> 3 <code>dtrack.notification.configuration</code> 3 <code>dtrack.notification.datasource-mirroring</code> 3 <code>dtrack.notification.file-system</code> 3 <code>dtrack.notification.integration</code> 3 <code>dtrack.notification.new-vulnerability</code> 3 <code>dtrack.notification.new-vulnerable-dependency</code> 3 <code>dtrack.notification.policy-violation</code> 3 <code>dtrack.notification.project-audit-change</code> 3 <code>dtrack.notification.project-created</code> 3 <code>dtrack.notification.repository</code> 3 <code>dtrack.notification.vex</code> 3 <code>dtrack.notification.user</code> 3 <code>dtrack.notification.project-vuln-analysis-complete</code> <sup>3</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>dtrack.repo-meta-analysis.component</code><sup>1A</sup> 3 <code>dtrack.repo-meta-analysis.result</code> 3 <code>dtrack.vuln-analysis.component</code><sup>1B</sup> 3 <code>dtrack.vuln-analysis.result</code> 3 <code>dtrack.vuln-analysis.result.processed</code> 3 <code>dtrack.vuln-analysis.scanner.result</code><sup>1B</sup> 3 <code>dtrack.vulnerability</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.epss</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.digest</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.mirror.command</code><sup>2</sup> 1 <code>dtrack.vulnerability.mirror.state</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>hyades-repository-meta-analyzer-command-by-purl-coordinates-repartition</code><sup>1A</sup> 3 <code>hyades-vulnerability-analyzer-completed-scans-table-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-last-update-store-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-table-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-ossindex-batch-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-ossindex-retry-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-scan-task-internal-repartition</code> 3 <code>hyades-vulnerability-analyzer-scan-task-ossindex-repartition</code><sup>1C</sup> 3 <code>hyades-vulnerability-analyzer-scan-task-snyk-repartition</code><sup>1D</sup> 3 <code>hyades-vulnerability-analyzer-snyk-batch-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-snyk-retry-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <p><sup>1X</sup> The topic is subject to co-partitioning requirements <sup>2</sup> The partition number of this topic should not be changed <sup>3</sup> To use this notification, the PROJECT_VULN_ANALYSIS_COMPLETE group needs to be manually added through api call to the notify on condition The put request to be executed is here</p>"},{"location":"reference/topics/#co-partitioning-requirements","title":"Co-Partitioning Requirements","text":"<p>Some topics must be co-partitioned, meaning they must share the exact same number of partitions. Applications using those topics will not work correctly when this is not the case.</p>"},{"location":"reference/configuration/api-server/","title":"API Server","text":""},{"location":"reference/configuration/api-server/#cors","title":"CORS","text":""},{"location":"reference/configuration/api-server/#alpinecorsallowcredentials","title":"alpine.cors.allow.credentials","text":"<p>Controls the content of the <code>Access-Control-Allow-Credentials</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_CORS_ALLOW_CREDENTIALS</code>"},{"location":"reference/configuration/api-server/#alpinecorsallowheaders","title":"alpine.cors.allow.headers","text":"<p>Controls the content of the <code>Access-Control-Allow-Headers</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, *</code> ENV <code>ALPINE_CORS_ALLOW_HEADERS</code>"},{"location":"reference/configuration/api-server/#alpinecorsallowmethods","title":"alpine.cors.allow.methods","text":"<p>Controls the content of the <code>Access-Control-Allow-Methods</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>GET POST PUT DELETE OPTIONS</code> ENV <code>ALPINE_CORS_ALLOW_METHODS</code>"},{"location":"reference/configuration/api-server/#alpinecorsalloworigin","title":"alpine.cors.allow.origin","text":"<p>Controls the content of the <code>Access-Control-Allow-Origin</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>*</code> ENV <code>ALPINE_CORS_ALLOW_ORIGIN</code>"},{"location":"reference/configuration/api-server/#alpinecorsenabled","title":"alpine.cors.enabled","text":"<p>Defines whether Cross Origin Resource Sharing  (CORS) headers shall be included in REST API responses.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_CORS_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpinecorsexposeheaders","title":"alpine.cors.expose.headers","text":"<p>Controls the content of the <code>Access-Control-Expose-Headers</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count</code> ENV <code>ALPINE_CORS_EXPOSE_HEADERS</code>"},{"location":"reference/configuration/api-server/#alpinecorsmaxage","title":"alpine.cors.max.age","text":"<p>Controls the content of the <code>Access-Control-Max-Age</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>integer</code> Default <code>3600</code> ENV <code>ALPINE_CORS_MAX_AGE</code>"},{"location":"reference/configuration/api-server/#database","title":"Database","text":""},{"location":"reference/configuration/api-server/#alpinedatabasepassword","title":"alpine.database.password","text":"<p>Specifies the password to use when authenticating to the database.  </p> Required false Type <code>string</code> Default <code>dtrack</code> ENV <code>ALPINE_DATABASE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepasswordfile","title":"alpine.database.password.file","text":"<p>Specifies the file to load the database password from.  If set, takes precedence over <code>alpine.database.password</code>.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>/var/run/secrets/database-password</code> ENV <code>ALPINE_DATABASE_PASSWORD_FILE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolenabled","title":"alpine.database.pool.enabled","text":"<p>Specifies if the database connection pool is enabled.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_DATABASE_POOL_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolidletimeout","title":"alpine.database.pool.idle.timeout","text":"<p>This property controls the maximum amount of time that a connection is  allowed to sit idle in the pool.  </p> Required false Type <code>integer</code> Default <code>300000</code> ENV <code>ALPINE_DATABASE_POOL_IDLE_TIMEOUT</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolmaxlifetime","title":"alpine.database.pool.max.lifetime","text":"<p>This property controls the maximum lifetime of a connection in the pool.  An in-use connection will never be retired, only when it is closed will  it then be removed.  </p> Required false Type <code>integer</code> Default <code>600000</code> ENV <code>ALPINE_DATABASE_POOL_MAX_LIFETIME</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolmaxsize","title":"alpine.database.pool.max.size","text":"<p>This property controls the maximum size that the pool is allowed to reach,  including both idle and in-use connections.  </p> Required false Type <code>integer</code> Default <code>20</code> ENV <code>ALPINE_DATABASE_POOL_MAX_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolminidle","title":"alpine.database.pool.min.idle","text":"<p>This property controls the minimum number of idle connections in the pool.  This value should be equal to or less than <code>alpine.database.pool.max.size</code>.  Warning: If the value is less than <code>alpine.database.pool.max.size</code>,  <code>alpine.database.pool.idle.timeout</code> will have no effect.  </p> Required false Type <code>integer</code> Default <code>10</code> ENV <code>ALPINE_DATABASE_POOL_MIN_IDLE</code>"},{"location":"reference/configuration/api-server/#alpinedatabaseurl","title":"alpine.database.url","text":"<p>Specifies the JDBC URL to use when connecting to the database.  For best performance, set the <code>reWriteBatchedInserts</code> query parameter to <code>true</code>.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>jdbc:postgresql://localhost:5432/dtrack?reWriteBatchedInserts=true</code> ENV <code>ALPINE_DATABASE_URL</code>"},{"location":"reference/configuration/api-server/#alpinedatabaseusername","title":"alpine.database.username","text":"<p>Specifies the username to use when authenticating to the database.  </p> Required false Type <code>string</code> Default <code>dtrack</code> ENV <code>ALPINE_DATABASE_USERNAME</code>"},{"location":"reference/configuration/api-server/#databasemigrationpassword","title":"database.migration.password","text":"<p>Defines the database password for executing migrations.  If not set, the value of <code>alpine.database.password</code> will be used.  </p> Required false Type <code>string</code> Default <code>${alpine.database.password}</code> ENV <code>DATABASE_MIGRATION_PASSWORD</code>"},{"location":"reference/configuration/api-server/#databasemigrationurl","title":"database.migration.url","text":"<p>Defines the database JDBC URL to use when executing migrations.  If not set, the value of <code>alpine.database.url</code> will be used.  Should generally not be set, unless TLS authentication is used,  and custom connection variables are required.  </p> Required false Type <code>string</code> Default <code>${alpine.database.url}</code> ENV <code>DATABASE_MIGRATION_URL</code>"},{"location":"reference/configuration/api-server/#databasemigrationusername","title":"database.migration.username","text":"<p>Defines the database user for executing migrations.  If not set, the value of <code>alpine.database.username</code> will be used.  </p> Required false Type <code>string</code> Default <code>${alpine.database.username}</code> ENV <code>DATABASE_MIGRATION_USERNAME</code>"},{"location":"reference/configuration/api-server/#databaserunmigrations","title":"database.run.migrations","text":"<p>Defines whether database migrations should be executed on startup.    From v5.6.0 onwards, migrations are considered part of the initialization tasks.  Setting <code>init.tasks.enabled</code> to <code>false</code> will disable migrations,  even if <code>database.run.migrations</code> is enabled.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>DATABASE_RUN_MIGRATIONS</code>"},{"location":"reference/configuration/api-server/#databaserunmigrationsonly","title":"database.run.migrations.only","text":"<p>Defines whether the application should exit upon successful execution of database migrations.  Enabling this option makes the application suitable for running as k8s init container.  Has no effect unless <code>database.run.migrations</code> is <code>true</code>.    From v5.6.0 onwards, usage of <code>init.and.exit</code> should be preferred.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>DATABASE_RUN_MIGRATIONS_ONLY</code>"},{"location":"reference/configuration/api-server/#development","title":"Development","text":""},{"location":"reference/configuration/api-server/#devservicesenabled","title":"dev.services.enabled","text":"<p>Whether dev services shall be enabled.    When enabled, Dependency-Track will automatically launch containers for:  <ul> <li>Frontend</li> <li>Kafka</li> <li>PostgreSQL</li> </ul>  at startup, and configures itself to use them. They are disposed when  Dependency-Track stops. The containers are exposed on randomized ports,  which will be logged during startup.    Trying to enable dev services in a production build will prevent  the application from starting.    Note that the containers launched by the API server can not currently  be discovered and re-used by other Hyades services. This is a future  enhancement tracked in https://github.com/DependencyTrack/hyades/issues/1188.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>DEV_SERVICES_ENABLED</code>"},{"location":"reference/configuration/api-server/#devservicesimagefrontend","title":"dev.services.image.frontend","text":"<p>The image to use for the frontend dev services container.  </p> Required false Type <code>string</code> Default <code>ghcr.io/dependencytrack/hyades-frontend:snapshot</code> ENV <code>DEV_SERVICES_IMAGE_FRONTEND</code>"},{"location":"reference/configuration/api-server/#devservicesimagekafka","title":"dev.services.image.kafka","text":"<p>The image to use for the Kafka dev services container.  </p> Required false Type <code>string</code> Default <code>apache/kafka-native:3.8.0</code> ENV <code>DEV_SERVICES_IMAGE_KAFKA</code>"},{"location":"reference/configuration/api-server/#devservicesimagepostgres","title":"dev.services.image.postgres","text":"<p>The image to use for the PostgreSQL dev services container.  </p> Required false Type <code>string</code> Default <code>postgres:16</code> ENV <code>DEV_SERVICES_IMAGE_POSTGRES</code>"},{"location":"reference/configuration/api-server/#general","title":"General","text":""},{"location":"reference/configuration/api-server/#alpineapikeyprefix","title":"alpine.api.key.prefix","text":"<p>Defines the prefix to be used for API keys. A maximum prefix length of 251  characters is supported. The prefix may also be left empty.  </p> Required false Type <code>string</code> Default <code>odt_</code> ENV <code>ALPINE_API_KEY_PREFIX</code>"},{"location":"reference/configuration/api-server/#alpineauthjwtttlseconds","title":"alpine.auth.jwt.ttl.seconds","text":"<p>Defines the number of seconds for which JWTs issued by Dependency-Track will be valid for.  </p> Required false Type <code>integer</code> Default <code>604800</code> ENV <code>ALPINE_AUTH_JWT_TTL_SECONDS</code>"},{"location":"reference/configuration/api-server/#alpinebcryptrounds","title":"alpine.bcrypt.rounds","text":"<p>Specifies the number of bcrypt rounds to use when hashing a user's password.  The higher the number the more secure the password, at the expense of  hardware resources and additional time to generate the hash.  </p> Required true Type <code>integer</code> Default <code>14</code> ENV <code>ALPINE_BCRYPT_ROUNDS</code>"},{"location":"reference/configuration/api-server/#alpinedatadirectory","title":"alpine.data.directory","text":"<p>Defines the path to the data directory. This directory will hold logs,  keys, and any database or index files along with application-specific  files or directories.  </p> Required true Type <code>string</code> Default <code>~/.dependency-track</code> ENV <code>ALPINE_DATA_DIRECTORY</code>"},{"location":"reference/configuration/api-server/#alpineprivatekeypath","title":"alpine.private.key.path","text":"<p>Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.  The keys will be generated upon first startup if they do not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/private.key</code> Example <code>/var/run/secrets/private.key</code> ENV <code>ALPINE_PRIVATE_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#alpinepublickeypath","title":"alpine.public.key.path","text":"<p>Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.  The keys will be generated upon first startup if they do not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/public.key</code> Example <code>/var/run/secrets/public.key</code> ENV <code>ALPINE_PUBLIC_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#alpinesecretkeypath","title":"alpine.secret.key.path","text":"<p>Defines the path to the secret key to be used for data encryption and decryption.  The key will be generated upon first startup if it does not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/secret.key</code> ENV <code>ALPINE_SECRET_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#initandexit","title":"init.and.exit","text":"<p>Whether to only execute initialization tasks and exit.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INIT_AND_EXIT</code>"},{"location":"reference/configuration/api-server/#inittasksenabled","title":"init.tasks.enabled","text":"<p>Whether to execute initialization tasks on startup.  Initialization tasks include:  <ul> <li>Execution of database migrations</li> <li>Populating the database with default objects (permissions, users, licenses, etc.)</li> </ul> </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>INIT_TASKS_ENABLED</code>"},{"location":"reference/configuration/api-server/#integritycheckenabled","title":"integrity.check.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INTEGRITY_CHECK_ENABLED</code>"},{"location":"reference/configuration/api-server/#integrityinitializerenabled","title":"integrity.initializer.enabled","text":"<p>Specifies whether the Integrity Initializer shall be enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INTEGRITY_INITIALIZER_ENABLED</code>"},{"location":"reference/configuration/api-server/#tmpdelaybomprocessednotification","title":"tmp.delay.bom.processed.notification","text":"<p>Delays the BOM_PROCESSED notification until the vulnerability analysis associated with a given BOM upload  is completed. The intention being that it is then \"safe\" to query the API for any identified vulnerabilities.  This is specifically for cases where polling the /api/v1/bom/token/ endpoint is not feasible.  THIS IS A TEMPORARY FUNCTIONALITY AND MAY BE REMOVED IN FUTURE RELEASES WITHOUT FURTHER NOTICE.   Required false Type <code>boolean</code> Default <code>false</code> ENV <code>TMP_DELAY_BOM_PROCESSED_NOTIFICATION</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicyanalysisenabled","title":"vulnerability.policy.analysis.enabled","text":"<p>Defines whether vulnerability policy analysis is enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>VULNERABILITY_POLICY_ANALYSIS_ENABLED</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleauthpassword","title":"vulnerability.policy.bundle.auth.password","text":"<p>For nginx server, if username and bearer token both are provided, basic auth will be used,  else the auth header will be added based on the not null values  Defines the password to be used for basic authentication against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_AUTH_PASSWORD</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleauthusername","title":"vulnerability.policy.bundle.auth.username","text":"<p>Defines the username to be used for basic authentication against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_AUTH_USERNAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundlebearertoken","title":"vulnerability.policy.bundle.bearer.token","text":"<p>Defines the token to be used as bearerAuth against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_BEARER_TOKEN</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundlesourcetype","title":"vulnerability.policy.bundle.source.type","text":"<p>Defines the type of source from which policy bundles are being fetched from.  Required when <code>vulnerability.policy.bundle.url</code> is set.  </p> Required false Type <code>enum</code> Valid Values <code>[nginx, s3]</code> Default <code>NGINX</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_SOURCE_TYPE</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleurl","title":"vulnerability.policy.bundle.url","text":"<p>Defines where to fetch the policy bundle from.For S3, just the base url needs to be provided with port  For nginx, the whole url with bundle name needs to be given  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>http://example.com:80/bundles/bundle.zip</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_URL</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3accesskey","title":"vulnerability.policy.s3.access.key","text":"<p>S3 related details. Access key, secret key, bucket name and bundle names are mandatory if S3 is chosen. Region is optional  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_ACCESS_KEY</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3bucketname","title":"vulnerability.policy.s3.bucket.name","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_BUCKET_NAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3bundlename","title":"vulnerability.policy.s3.bundle.name","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_BUNDLE_NAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3region","title":"vulnerability.policy.s3.region","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_REGION</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3secretkey","title":"vulnerability.policy.s3.secret.key","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_SECRET_KEY</code>"},{"location":"reference/configuration/api-server/#http","title":"HTTP","text":""},{"location":"reference/configuration/api-server/#alpinehttpproxyaddress","title":"alpine.http.proxy.address","text":"<p>HTTP proxy address. If set, then <code>alpine.http.proxy.port</code> must be set too.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>proxy.example.com</code> ENV <code>ALPINE_HTTP_PROXY_ADDRESS</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxypassword","title":"alpine.http.proxy.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_HTTP_PROXY_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxypasswordfile","title":"alpine.http.proxy.password.file","text":"<p>Specifies the file to load the HTTP proxy password from.  If set, takes precedence over <code>alpine.http.proxy.password</code>.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>/var/run/secrets/http-proxy-password</code> ENV <code>ALPINE_HTTP_PROXY_PASSWORD_FILE</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxyport","title":"alpine.http.proxy.port","text":"Required false Type <code>integer</code> Default <code>null</code> Example <code>8888</code> ENV <code>ALPINE_HTTP_PROXY_PORT</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxyusername","title":"alpine.http.proxy.username","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_HTTP_PROXY_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutconnection","title":"alpine.http.timeout.connection","text":"<p>Defines the connection timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>30</code> ENV <code>ALPINE_HTTP_TIMEOUT_CONNECTION</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutpool","title":"alpine.http.timeout.pool","text":"<p>Defines the request timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>60</code> ENV <code>ALPINE_HTTP_TIMEOUT_POOL</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutsocket","title":"alpine.http.timeout.socket","text":"<p>Defines the socket / read timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>30</code> ENV <code>ALPINE_HTTP_TIMEOUT_SOCKET</code>"},{"location":"reference/configuration/api-server/#alpinenoproxy","title":"alpine.no.proxy","text":"Required false Type <code>string</code> Default <code>null</code> Example <code>localhost,127.0.0.1</code> ENV <code>ALPINE_NO_PROXY</code>"},{"location":"reference/configuration/api-server/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/api-server/#dtkafkatopicprefix","title":"dt.kafka.topic.prefix","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>DT_KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/api-server/#kafkaautooffsetreset","title":"kafka.auto.offset.reset","text":"Required false Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"Required true Type <code>string</code> Default <code>null</code> Example <code>localhost:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/api-server/#kafkakeystorepassword","title":"kafka.keystore.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_KEYSTORE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#kafkakeystorepath","title":"kafka.keystore.path","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_KEYSTORE_PATH</code>"},{"location":"reference/configuration/api-server/#kafkamtlsenabled","title":"kafka.mtls.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>KAFKA_MTLS_ENABLED</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorconsumerautooffsetreset","title":"kafka.processor.epss.mirror.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorconsumergroupid","title":"kafka.processor.epss.mirror.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrormaxbatchsize","title":"kafka.processor.epss.mirror.max.batch.size","text":"Required true Type <code>integer</code> Default <code>500</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_MAX_BATCH_SIZE</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrormaxconcurrency","title":"kafka.processor.epss.mirror.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorprocessingorder","title":"kafka.processor.epss.mirror.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorretryinitialdelayms","title":"kafka.processor.epss.mirror.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorretrymaxdelayms","title":"kafka.processor.epss.mirror.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorretrymultiplier","title":"kafka.processor.epss.mirror.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorretryrandomizationfactor","title":"kafka.processor.epss.mirror.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultconsumerautooffsetreset","title":"kafka.processor.repo.meta.analysis.result.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultconsumergroupid","title":"kafka.processor.repo.meta.analysis.result.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultmaxconcurrency","title":"kafka.processor.repo.meta.analysis.result.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultprocessingorder","title":"kafka.processor.repo.meta.analysis.result.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultretryinitialdelayms","title":"kafka.processor.repo.meta.analysis.result.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultretrymaxdelayms","title":"kafka.processor.repo.meta.analysis.result.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultretrymultiplier","title":"kafka.processor.repo.meta.analysis.result.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultretryrandomizationfactor","title":"kafka.processor.repo.meta.analysis.result.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorconsumerautooffsetreset","title":"kafka.processor.vuln.mirror.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorconsumergroupid","title":"kafka.processor.vuln.mirror.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrormaxconcurrency","title":"kafka.processor.vuln.mirror.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorprocessingorder","title":"kafka.processor.vuln.mirror.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>partition</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorretryinitialdelayms","title":"kafka.processor.vuln.mirror.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorretrymaxdelayms","title":"kafka.processor.vuln.mirror.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorretrymultiplier","title":"kafka.processor.vuln.mirror.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorretryrandomizationfactor","title":"kafka.processor.vuln.mirror.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultconsumerautooffsetreset","title":"kafka.processor.vuln.scan.result.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultconsumergroupid","title":"kafka.processor.vuln.scan.result.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultmaxconcurrency","title":"kafka.processor.vuln.scan.result.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedconsumerautooffsetreset","title":"kafka.processor.vuln.scan.result.processed.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedconsumerfetchminbytes","title":"kafka.processor.vuln.scan.result.processed.consumer.fetch.min.bytes","text":"Required true Type <code>integer</code> Default <code>524288</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_FETCH_MIN_BYTES</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedconsumergroupid","title":"kafka.processor.vuln.scan.result.processed.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedconsumermaxpollrecords","title":"kafka.processor.vuln.scan.result.processed.consumer.max.poll.records","text":"Required true Type <code>integer</code> Default <code>10000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_MAX_POLL_RECORDS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedmaxbatchsize","title":"kafka.processor.vuln.scan.result.processed.max.batch.size","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_MAX_BATCH_SIZE</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedmaxconcurrency","title":"kafka.processor.vuln.scan.result.processed.max.concurrency","text":"Required true Type <code>integer</code> Default <code>1</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedprocessingorder","title":"kafka.processor.vuln.scan.result.processed.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>unordered</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedretryinitialdelayms","title":"kafka.processor.vuln.scan.result.processed.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedretrymaxdelayms","title":"kafka.processor.vuln.scan.result.processed.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedretrymultiplier","title":"kafka.processor.vuln.scan.result.processed.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedretryrandomizationfactor","title":"kafka.processor.vuln.scan.result.processed.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessingorder","title":"kafka.processor.vuln.scan.result.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultretryinitialdelayms","title":"kafka.processor.vuln.scan.result.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultretrymaxdelayms","title":"kafka.processor.vuln.scan.result.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultretrymultiplier","title":"kafka.processor.vuln.scan.result.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultretryrandomizationfactor","title":"kafka.processor.vuln.scan.result.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkasecurityprotocol","title":"kafka.security.protocol","text":"Required false Type <code>enum</code> Valid Values <code>[PLAINTEXT, SASL_SSL_PLAINTEXT, SASL_PLAINTEXT, SSL]</code> Default <code>null</code> ENV <code>KAFKA_SECURITY_PROTOCOL</code>"},{"location":"reference/configuration/api-server/#kafkatlsenabled","title":"kafka.tls.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>KAFKA_TLS_ENABLED</code>"},{"location":"reference/configuration/api-server/#kafkatruststorepassword","title":"kafka.truststore.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TRUSTSTORE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#kafkatruststorepath","title":"kafka.truststore.path","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TRUSTSTORE_PATH</code>"},{"location":"reference/configuration/api-server/#ldap","title":"LDAP","text":""},{"location":"reference/configuration/api-server/#alpineldapattributemail","title":"alpine.ldap.attribute.mail","text":"<p>Specifies the LDAP attribute used to store a users email address  </p> Required false Type <code>string</code> Default <code>mail</code> ENV <code>ALPINE_LDAP_ATTRIBUTE_MAIL</code>"},{"location":"reference/configuration/api-server/#alpineldapattributename","title":"alpine.ldap.attribute.name","text":"<p>Specifies the Attribute that identifies a users ID.    Example (Microsoft Active Directory):  <ul><li><code>userPrincipalName</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>uid</code></li></ul> </p> Required false Type <code>string</code> Default <code>userPrincipalName</code> ENV <code>ALPINE_LDAP_ATTRIBUTE_NAME</code>"},{"location":"reference/configuration/api-server/#alpineldapauthusernameformat","title":"alpine.ldap.auth.username.format","text":"<p>Specifies if the username entered during login needs to be formatted prior  to asserting credentials against the directory. For Active Directory, the  userPrincipal attribute typically ends with the domain, whereas the  samAccountName attribute and other directory server implementations do not.  The %s variable will be substituted with the username asserted during login.    Example (Microsoft Active Directory):  <ul><li><code>%s@example.com</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>%s</code></li></ul> </p> Required false Type <code>string</code> Default <code>null</code> Example <code>%s@example.com</code> ENV <code>ALPINE_LDAP_AUTH_USERNAME_FORMAT</code>"},{"location":"reference/configuration/api-server/#alpineldapbasedn","title":"alpine.ldap.basedn","text":"<p>Specifies the base DN that all queries should search from  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>dc=example,dc=com</code> ENV <code>ALPINE_LDAP_BASEDN</code>"},{"location":"reference/configuration/api-server/#alpineldapbindpassword","title":"alpine.ldap.bind.password","text":"<p>If anonymous access is not permitted, specify a password for the username  used to bind.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_BIND_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpineldapbindusername","title":"alpine.ldap.bind.username","text":"<p>If anonymous access is not permitted, specify a username with limited access  to the directory, just enough to perform searches. This should be the fully  qualified DN of the user.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_BIND_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpineldapenabled","title":"alpine.ldap.enabled","text":"<p>Defines if LDAP will be used for user authentication. If enabled,  <code>alpine.ldap.*</code> properties should be set accordingly.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpineldapgroupsfilter","title":"alpine.ldap.groups.filter","text":"<p>Specifies the LDAP search filter used to retrieve all groups from the directory.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=group)(objectCategory=Group))</code> ENV <code>ALPINE_LDAP_GROUPS_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapgroupssearchfilter","title":"alpine.ldap.groups.search.filter","text":"<p>Specifies the LDAP search filter used to search for groups by their name.  The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(cn={SEARCH_TERM}))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames)(cn={SEARCH_TERM}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))</code> ENV <code>ALPINE_LDAP_GROUPS_SEARCH_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapsecurityauth","title":"alpine.ldap.security.auth","text":"<p>Specifies the LDAP security authentication level to use. Its value is one of  the following strings: \"none\", \"simple\", \"strong\". If this property is empty  or unspecified, the behaviour is determined by the service provider.  </p> Required false Type <code>enum</code> Valid Values <code>[none, simple, strong]</code> Default <code>simple</code> ENV <code>ALPINE_LDAP_SECURITY_AUTH</code>"},{"location":"reference/configuration/api-server/#alpineldapserverurl","title":"alpine.ldap.server.url","text":"<p>Specifies the LDAP server URL.    Examples (Microsoft Active Directory):  <ul> <li><code>ldap://ldap.example.com:3268</code></li> <li><code>ldaps://ldap.example.com:3269</code></li> </ul>  Examples (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul> <li><code>ldap://ldap.example.com:389</code></li> <li><code>ldaps://ldap.example.com:636</code></li> </ul> </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_SERVER_URL</code>"},{"location":"reference/configuration/api-server/#alpineldapteamsynchronization","title":"alpine.ldap.team.synchronization","text":"<p>This option will ensure that team memberships for LDAP users are dynamic and  synchronized with membership of LDAP groups. When a team is mapped to an LDAP  group, all local LDAP users will automatically be assigned to the team if  they are a member of the group the team is mapped to. If the user is later  removed from the LDAP group, they will also be removed from the team. This  option provides the ability to dynamically control user permissions via an  external directory.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_TEAM_SYNCHRONIZATION</code>"},{"location":"reference/configuration/api-server/#alpineldapusergroupsfilter","title":"alpine.ldap.user.groups.filter","text":"<p>Specifies the LDAP search filter to use to query a user and retrieve a list  of groups the user is a member of. The <code>{USER_DN}</code> variable will be substituted  with the actual value of the users DN at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(member={USER_DN}))</code></li></ul>  Example (Microsoft Active Directory - with nested group support):  <ul><li><code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code> ENV <code>ALPINE_LDAP_USER_GROUPS_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapuserprovisioning","title":"alpine.ldap.user.provisioning","text":"<p>Specifies if mapped LDAP accounts are automatically created upon successful  authentication. When a user logs in with valid credentials but an account has  not been previously provisioned, an authentication failure will be returned.  This allows admins to control specifically which ldap users can access the  system and which users cannot. When this value is set to true, a local ldap  user will be created and mapped to the ldap account automatically. This  automatic provisioning only affects authentication, not authorization.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_USER_PROVISIONING</code>"},{"location":"reference/configuration/api-server/#alpineldapuserssearchfilter","title":"alpine.ldap.users.search.filter","text":"<p>Specifies the LDAP search filter used to search for users by their name.  The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(cn={SEARCH_TERM}))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=inetOrgPerson)(cn={SEARCH_TERM}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*))</code> ENV <code>ALPINE_LDAP_USERS_SEARCH_FILTER</code>"},{"location":"reference/configuration/api-server/#observability","title":"Observability","text":""},{"location":"reference/configuration/api-server/#alpinemetricsauthpassword","title":"alpine.metrics.auth.password","text":"<p>Defines the password required to access metrics.  Has no effect when <code>alpine.metrics.auth.username</code> is not set.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_METRICS_AUTH_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinemetricsauthusername","title":"alpine.metrics.auth.username","text":"<p>Defines the username required to access metrics.  Has no effect when <code>alpine.metrics.auth.password</code> is not set.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_METRICS_AUTH_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpinemetricsenabled","title":"alpine.metrics.enabled","text":"<p>Defines whether Prometheus metrics will be exposed.  If enabled, metrics will be available via the /metrics endpoint.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_METRICS_ENABLED</code>"},{"location":"reference/configuration/api-server/#openid-connect","title":"OpenID Connect","text":""},{"location":"reference/configuration/api-server/#alpineoidcclientid","title":"alpine.oidc.client.id","text":"<p>Defines the client ID to be used for OpenID Connect.  The client ID should be the same as the one configured for the frontend,  and will only be used to validate ID tokens.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_OIDC_CLIENT_ID</code>"},{"location":"reference/configuration/api-server/#alpineoidcenabled","title":"alpine.oidc.enabled","text":"<p>Defines if OpenID Connect will be used for user authentication.  If enabled, <code>alpine.oidc.*</code> properties should be set accordingly.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpineoidcissuer","title":"alpine.oidc.issuer","text":"<p>Defines the issuer URL to be used for OpenID Connect.  This issuer MUST support provider configuration via the <code>/.well-known/openid-configuration</code> endpoint.  See also:  <ul> <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata</li> <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig</li> </ul> </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_OIDC_ISSUER</code>"},{"location":"reference/configuration/api-server/#alpineoidcteamsynchronization","title":"alpine.oidc.team.synchronization","text":"<p>This option will ensure that team memberships for OpenID Connect users are dynamic and  synchronized with membership of OpenID Connect groups or assigned roles. When a team is  mapped to an OpenID Connect group, all local OpenID Connect users will automatically be  assigned to the team if they are a member of the group the team is mapped to. If the user  is later removed from the OpenID Connect group, they will also be removed from the team. This  option provides the ability to dynamically control user permissions via the identity provider.  Note that team synchronization is only performed during user provisioning and after successful  authentication.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_TEAM_SYNCHRONIZATION</code>"},{"location":"reference/configuration/api-server/#alpineoidcteamsclaim","title":"alpine.oidc.teams.claim","text":"<p>Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint.  The claim must be an array of strings. Most public identity providers do not support group or role management.  When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint  will most likely need to be configured.  </p> Required false Type <code>string</code> Default <code>groups</code> ENV <code>ALPINE_OIDC_TEAMS_CLAIM</code>"},{"location":"reference/configuration/api-server/#alpineoidcuserprovisioning","title":"alpine.oidc.user.provisioning","text":"<p>Specifies if mapped OpenID Connect accounts are automatically created upon successful  authentication. When a user logs in with a valid access token but an account has  not been previously provisioned, an authentication failure will be returned.  This allows admins to control specifically which OpenID Connect users can access the  system and which users cannot. When this value is set to true, a local OpenID Connect  user will be created and mapped to the OpenID Connect account automatically. This  automatic provisioning only affects authentication, not authorization.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_USER_PROVISIONING</code>"},{"location":"reference/configuration/api-server/#alpineoidcusernameclaim","title":"alpine.oidc.username.claim","text":"<p>Defines the name of the claim that contains the username in the provider's userinfo endpoint.  Common claims are <code>name</code>, <code>username</code>, <code>preferred_username</code> or <code>nickname</code>.  See also:  <ul> <li>https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse</li> </ul> </p> Required false Type <code>string</code> Default <code>name</code> ENV <code>ALPINE_OIDC_USERNAME_CLAIM</code>"},{"location":"reference/configuration/api-server/#task-execution","title":"Task Execution","text":""},{"location":"reference/configuration/api-server/#alpineworkerthreadmultiplier","title":"alpine.worker.thread.multiplier","text":"<p>Defines a multiplier that is used to calculate the number of threads used  by the event subsystem. This property is only used when <code>alpine.worker.threads</code>  is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most)  16 worker threads.  </p> Required true Type <code>integer</code> Default <code>4</code> ENV <code>ALPINE_WORKER_THREAD_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpineworkerthreads","title":"alpine.worker.threads","text":"<p>Defines the number of worker threads that the event subsystem will consume.  Events occur asynchronously and are processed by the Event subsystem. This  value should be large enough to handle most production situations without  introducing much delay, yet small enough not to pose additional load on an  already resource-constrained server.  A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This  can further be tweaked using the <code>alpine.worker.thread.multiplier</code> property.  </p> Required true Type <code>integer</code> Default <code>0</code> ENV <code>ALPINE_WORKER_THREADS</code>"},{"location":"reference/configuration/api-server/#task-scheduling","title":"Task Scheduling","text":""},{"location":"reference/configuration/api-server/#taskcomponentmetadatamaintenancecron","title":"task.component.metadata.maintenance.cron","text":"<p>Cron expression of the component metadata maintenance task.    The task deletes orphaned records from the <code>INTEGRITY_META_COMPONENT</code> and  <code>REPOSITORY_META_COMPONENT</code> tables.  </p> Required true Type <code>cron</code> Default <code>0 */12 * * *</code> ENV <code>TASK_COMPONENT_METADATA_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskcomponentmetadatamaintenancelockmaxduration","title":"task.component.metadata.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the component metadata maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_COMPONENT_METADATA_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskcomponentmetadatamaintenancelockminduration","title":"task.component.metadata.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the component metadata maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_COMPONENT_METADATA_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskdefectdojouploadcron","title":"task.defect.dojo.upload.cron","text":"<p>Cron expression of the DefectDojo upload task.  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_DEFECT_DOJO_UPLOAD_CRON</code>"},{"location":"reference/configuration/api-server/#taskepssmirrorcron","title":"task.epss.mirror.cron","text":"<p>Cron expression of the EPSS mirroring task.  </p> Required true Type <code>cron</code> Default <code>0 1 * * *</code> ENV <code>TASK_EPSS_MIRROR_CRON</code>"},{"location":"reference/configuration/api-server/#taskfortifysscuploadcron","title":"task.fortify.ssc.upload.cron","text":"<p>Cron expression of the Fortify SSC upload task.  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_FORTIFY_SSC_UPLOAD_CRON</code>"},{"location":"reference/configuration/api-server/#taskgithubadvisorymirrorcron","title":"task.git.hub.advisory.mirror.cron","text":"<p>Cron expression of the vulnerability GitHub Advisories mirroring task.  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_GIT_HUB_ADVISORY_MIRROR_CRON</code>"},{"location":"reference/configuration/api-server/#taskintegritymetainitializercron","title":"task.integrity.meta.initializer.cron","text":"<p>Cron expression of the integrity metadata initializer task.  </p> Required true Type <code>cron</code> Default <code>0 */12 * * *</code> ENV <code>TASK_INTEGRITY_META_INITIALIZER_CRON</code>"},{"location":"reference/configuration/api-server/#taskintegritymetainitializerlockmaxduration","title":"task.integrity.meta.initializer.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the integrity metadata initializer task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_INTEGRITY_META_INITIALIZER_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskintegritymetainitializerlockminduration","title":"task.integrity.meta.initializer.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the integrity metadata initializer task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_INTEGRITY_META_INITIALIZER_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskinternalcomponentidentificationcron","title":"task.internal.component.identification.cron","text":"<p>Cron expression of the internal component identification task.  </p> Required true Type <code>cron</code> Default <code>25 */6 * * *</code> ENV <code>TASK_INTERNAL_COMPONENT_IDENTIFICATION_CRON</code>"},{"location":"reference/configuration/api-server/#taskinternalcomponentidentificationlockmaxduration","title":"task.internal.component.identification.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the internal component identification task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_INTERNAL_COMPONENT_IDENTIFICATION_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskinternalcomponentidentificationlockminduration","title":"task.internal.component.identification.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the internal component identification task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_INTERNAL_COMPONENT_IDENTIFICATION_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskkennasecurityuploadcron","title":"task.kenna.security.upload.cron","text":"<p>Cron expression of the Kenna Security upload task.  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_KENNA_SECURITY_UPLOAD_CRON</code>"},{"location":"reference/configuration/api-server/#taskldapsynccron","title":"task.ldap.sync.cron","text":"<p>Cron expression of the LDAP synchronization task.  </p> Required true Type <code>cron</code> Default <code>0 */6 * * *</code> ENV <code>TASK_LDAP_SYNC_CRON</code>"},{"location":"reference/configuration/api-server/#taskldapsynclockmaxduration","title":"task.ldap.sync.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the LDAP synchronization task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_LDAP_SYNC_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskldapsynclockminduration","title":"task.ldap.sync.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the LDAP synchronization task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_LDAP_SYNC_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskmetricsmaintenancecron","title":"task.metrics.maintenance.cron","text":"<p>Cron expression of the metrics maintenance task.    The task deletes records older than the configured metrics retention duration from the following tables:  <ul> <li><code>DEPENDENCYMETRICS</code></li> <li><code>PROJECTMETRICS</code></li> <li><code>PORTFOLIOMETRICS</code></li> </ul> </p> Required true Type <code>cron</code> Default <code>0 */3 * * *</code> ENV <code>TASK_METRICS_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskmetricsmaintenancelockmaxduration","title":"task.metrics.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the metrics maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_METRICS_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskmetricsmaintenancelockminduration","title":"task.metrics.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the metrics maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_METRICS_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#tasknistmirrorcron","title":"task.nist.mirror.cron","text":"<p>Cron expression of the NIST / NVD mirroring task.  </p> Required true Type <code>cron</code> Default <code>0 4 * * *</code> ENV <code>TASK_NIST_MIRROR_CRON</code>"},{"location":"reference/configuration/api-server/#taskosvmirrorcron","title":"task.osv.mirror.cron","text":"<p>Cron expression of the OSV mirroring task.  </p> Required true Type <code>cron</code> Default <code>0 3 * * *</code> ENV <code>TASK_OSV_MIRROR_CRON</code>"},{"location":"reference/configuration/api-server/#taskportfoliometricsupdatecron","title":"task.portfolio.metrics.update.cron","text":"<p>Cron expression of the portfolio metrics update task.  </p> Required true Type <code>cron</code> Default <code>10 * * * *</code> ENV <code>TASK_PORTFOLIO_METRICS_UPDATE_CRON</code>"},{"location":"reference/configuration/api-server/#taskportfoliometricsupdatelockmaxduration","title":"task.portfolio.metrics.update.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the portfolio metrics update task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_PORTFOLIO_METRICS_UPDATE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskportfoliometricsupdatelockminduration","title":"task.portfolio.metrics.update.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the portfolio metrics update task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_PORTFOLIO_METRICS_UPDATE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskrepositorymetaanalysiscron","title":"task.repository.meta.analysis.cron","text":"<p>Cron expression of the portfolio repository metadata analysis task.  </p> Required true Type <code>cron</code> Default <code>0 1 * * *</code> ENV <code>TASK_REPOSITORY_META_ANALYSIS_CRON</code>"},{"location":"reference/configuration/api-server/#taskrepositorymetaanalysislockmaxduration","title":"task.repository.meta.analysis.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the portfolio repository metadata analysis task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_REPOSITORY_META_ANALYSIS_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskrepositorymetaanalysislockminduration","title":"task.repository.meta.analysis.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the portfolio repository metadata analysis task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_REPOSITORY_META_ANALYSIS_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskschedulerinitialdelay","title":"task.scheduler.initial.delay","text":"<p>Scheduling tasks after 3 minutes (3601000) of starting application  </p> Required true Type <code>integer</code> Default <code>180000</code> ENV <code>TASK_SCHEDULER_INITIAL_DELAY</code>"},{"location":"reference/configuration/api-server/#taskschedulerpollinginterval","title":"task.scheduler.polling.interval","text":"<p>Cron expressions for tasks have the precision of minutes so polling every minute  </p> Required true Type <code>integer</code> Default <code>60000</code> ENV <code>TASK_SCHEDULER_POLLING_INTERVAL</code>"},{"location":"reference/configuration/api-server/#tasktagmaintenancecron","title":"task.tag.maintenance.cron","text":"<p>Cron expression of the tag maintenance task.    The task deletes orphaned tags that are not used anymore.  </p> Required true Type <code>cron</code> Default <code>0 */12 * * *</code> ENV <code>TASK_TAG_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#tasktagmaintenancelockmaxduration","title":"task.tag.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the tag maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_TAG_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#tasktagmaintenancelockminduration","title":"task.tag.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the tag maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_TAG_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityanalysiscron","title":"task.vulnerability.analysis.cron","text":"<p>Cron expression of the portfolio vulnerability analysis task.  </p> Required true Type <code>cron</code> Default <code>0 6 * * *</code> ENV <code>TASK_VULNERABILITY_ANALYSIS_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityanalysislockmaxduration","title":"task.vulnerability.analysis.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the portfolio vulnerability analysis task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_VULNERABILITY_ANALYSIS_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityanalysislockminduration","title":"task.vulnerability.analysis.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the portfolio vulnerability analysis task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_VULNERABILITY_ANALYSIS_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitydatabasemaintenancecron","title":"task.vulnerability.database.maintenance.cron","text":"<p>Cron expression of the vulnerability database maintenance task.    The task deletes orphaned records from the <code>VULNERABLESOFTWARE</code> table.  </p> Required true Type <code>cron</code> Default <code>0 0 * * *</code> ENV <code>TASK_VULNERABILITY_DATABASE_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitydatabasemaintenancelockmaxduration","title":"task.vulnerability.database.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_VULNERABILITY_DATABASE_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitydatabasemaintenancelockminduration","title":"task.vulnerability.database.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_VULNERABILITY_DATABASE_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitymetricsupdatecron","title":"task.vulnerability.metrics.update.cron","text":"<p>Cron expression of the vulnerability metrics update task.  </p> Required true Type <code>cron</code> Default <code>40 * * * *</code> ENV <code>TASK_VULNERABILITY_METRICS_UPDATE_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitymetricsupdatelockmaxduration","title":"task.vulnerability.metrics.update.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the vulnerability metrics update task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_VULNERABILITY_METRICS_UPDATE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitymetricsupdatelockminduration","title":"task.vulnerability.metrics.update.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the vulnerability metrics update task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_VULNERABILITY_METRICS_UPDATE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitypolicyfetchcron","title":"task.vulnerability.policy.fetch.cron","text":"<p>Cron expression of the vulnerability policy bundle fetch task.  </p> Required true Type <code>cron</code> Default <code>*/5 * * * *</code> ENV <code>TASK_VULNERABILITY_POLICY_FETCH_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitypolicyfetchlockmaxduration","title":"task.vulnerability.policy.fetch.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the vulnerability policy bundle fetch task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT5M</code> ENV <code>TASK_VULNERABILITY_POLICY_FETCH_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitypolicyfetchlockminduration","title":"task.vulnerability.policy.fetch.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the vulnerability policy bundle fetch task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT5S</code> ENV <code>TASK_VULNERABILITY_POLICY_FETCH_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityscanmaintenancecron","title":"task.vulnerability.scan.maintenance.cron","text":"<p>Cron expression of the vulnerability scan maintenance task.    The task deletes records older than the configured retention duration from the <code>VULNERABILITYSCAN</code> table.  </p> Required true Type <code>cron</code> Default <code>0 * * * *</code> ENV <code>TASK_VULNERABILITY_SCAN_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityscanmaintenancelockmaxduration","title":"task.vulnerability.scan.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_VULNERABILITY_SCAN_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityscanmaintenancelockminduration","title":"task.vulnerability.scan.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_VULNERABILITY_SCAN_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskworkflowmaintenancecron","title":"task.workflow.maintenance.cron","text":"<p>Cron expression of the workflow maintenance task.    The task:  <ul> <li>Transitions workflow steps from <code>PENDING</code> to <code>TIMED_OUT</code> state</li> <li>Transitions workflow steps from <code>TIMED_OUT</code> to <code>FAILED</code> state</li> <li>Transitions children of <code>FAILED</code> steps to <code>CANCELLED</code> state</li> <li>Deletes finished workflows according to the configured retention duration</li> </ul> </p> Required true Type <code>cron</code> Default <code>*/15 * * * *</code> ENV <code>TASK_WORKFLOW_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskworkflowmaintenancelockmaxduration","title":"task.workflow.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the workflow maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT5M</code> ENV <code>TASK_WORKFLOW_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskworkflowmaintenancelockminduration","title":"task.workflow.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the workflow maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_WORKFLOW_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/mirror-service/","title":"Mirror Service","text":""},{"location":"reference/configuration/mirror-service/#datasource","title":"Datasource","text":""},{"location":"reference/configuration/mirror-service/#mirrordatasourceosvalias-sync-enabled","title":"mirror.datasource.osv.alias-sync-enabled","text":"<p>Defines whether vulnerability aliases should be parsed from OSV.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourceosvbase-url","title":"mirror.datasource.osv.base-url","text":"<p>Defines the URL of the OSV storage bucket.  </p> Required false Type <code>string</code> Default <code>https://osv-vulnerabilities.storage.googleapis.com</code> ENV <code>MIRROR_DATASOURCE_OSV_BASE_URL</code>"},{"location":"reference/configuration/mirror-service/#http","title":"HTTP","text":""},{"location":"reference/configuration/mirror-service/#quarkushttpport","title":"quarkus.http.port","text":"<p>HTTP port to listen on. Application metrics will be available via this port.  </p> Required false Type <code>integer</code> Default <code>8093</code> ENV <code>QUARKUS_HTTP_PORT</code>"},{"location":"reference/configuration/mirror-service/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/mirror-service/#dtkafkatopicprefix","title":"dt.kafka.topic.prefix","text":"<p>Defines an optional prefix to assume for all Kafka topics the application  consumes from, or produces to. The prefix will also be prepended to the  application's consumer group ID.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>acme-</code> ENV <code>DT_KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamscommitintervalms","title":"kafka-streams.commit.interval.ms","text":"<p>Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.  The Kafka default of <code>30s</code> has been modified to <code>5s</code>.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms for details.  </p> Required false Type <code>integer</code> Default <code>5000</code> ENV <code>KAFKA_STREAMS_COMMIT_INTERVAL_MS</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsdeserializationcount","title":"kafka-streams.exception.thresholds.deserialization.count","text":"<p>Defines the threshold for records failing to be deserialized within <code>kafka-streams.exception.thresholds.deserialization.interval</code>.  Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsdeserializationinterval","title":"kafka-streams.exception.thresholds.deserialization.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.deserialization.count</code> records are  allowed to fail deserialization. Deserialization failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsprocessingcount","title":"kafka-streams.exception.thresholds.processing.count","text":"<p>Defines the threshold for records failing to be processed within <code>kafka-streams.exception.thresholds.processing.interval</code>.  Processing failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>50</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsprocessinginterval","title":"kafka-streams.exception.thresholds.processing.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.processing.count</code> records are  allowed to fail processing. Processing failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsproductioncount","title":"kafka-streams.exception.thresholds.production.count","text":"<p>Defines the threshold for records failing to be produced within <code>kafka-streams.exception.thresholds.production.interval</code>.  Production failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsproductioninterval","title":"kafka-streams.exception.thresholds.production.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.production.count</code> records are  allowed to fail producing. Production failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsnumstreamthreads","title":"kafka-streams.num.stream.threads","text":"<p>The number of threads to allocate for stream processing tasks.  Note that Specifying a number higher than the number of input partitions provides no additional benefit,  as excess threads will simply run idle.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads for details.  </p> Required true Type <code>integer</code> Default <code>3</code> ENV <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code>"},{"location":"reference/configuration/mirror-service/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"<p>Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for details.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>broker-01.acme.com:9092,broker-02.acme.com:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/mirror-service/#kafkamaxrequestsize","title":"kafka.max.request.size","text":"<p>Defines the maximum size of a Kafka producer request in bytes.    Some messages like Bill of Vulnerabilities can be bigger than the default 1MiB.  Since the size check is performed before records are compressed, this value may need to be increased  even though the compressed value is much smaller. The Kafka default of 1MiB has been raised to 2MiB.    Refer to https://kafka.apache.org/documentation/#producerconfigs_max.request.size for details.  </p> Required true Type <code>integer</code> Default <code>2097152</code> ENV <code>KAFKA_MAX_REQUEST_SIZE</code>"},{"location":"reference/configuration/mirror-service/#quarkuskafka-streamsapplication-id","title":"quarkus.kafka-streams.application-id","text":"<p>Defines the ID to uniquely identify this application in the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_application.id for details.  </p> Required false Type <code>string</code> Default <code>${dt.kafka.topic.prefix}hyades-mirror-service</code> ENV <code>QUARKUS_KAFKA_STREAMS_APPLICATION_ID</code>"},{"location":"reference/configuration/mirror-service/#observability","title":"Observability","text":""},{"location":"reference/configuration/mirror-service/#quarkuslogconsolejson","title":"quarkus.log.console.json","text":"<p>Defines whether logs should be written in JSON format.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>QUARKUS_LOG_CONSOLE_JSON</code>"},{"location":"reference/configuration/overview/","title":"Configuration","text":"<p>All available configuration options used by all applications are listed in their respective <code>application.properties</code>. Options can be provided via environment variables as well, refer to the Quarkus docs for details.</p> <p>Not all options are supposed to be tweaked by users though. This document contains an overview of all options that are expected to be changed by users.</p>"},{"location":"reference/configuration/overview/#kafka-topic-configuration","title":"Kafka Topic Configuration","text":"<p>Kafka topics (including internal topics) can be configured with custom prefix. In order to provide custom prefix, below environment variable can be used.</p> Environment Variable Description Default Required <code>DT_KAFKA_TOPIC_PREFIX</code> Prefix for topic names - \u274c"},{"location":"reference/configuration/overview/#notification-publisher","title":"Notification Publisher","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>PARALLEL_CONSUMER_MAX_CONCURRENCY</code> Number of threads to process notifications with <code>6</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_INITIAL_DELAY</code> Initial delay before retrying notification delivery <code>3S</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_MULTIPLIER</code> Multiplier for retry delays <code>0.3</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_RANDOMIZATION_FACTOR</code> Randomization factory for jitter in retry delays <code>0.3</code> \u274c <code>PARALLEL_CONSUMER_RETRY_MAX_DURATION</code> Maximum duration of delays between retry attempts <code>2M</code> \u2705 <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#repository-meta-analyzer","title":"Repository Meta Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#vulnerability-analyzer","title":"Vulnerability Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>STATE_STORE_TYPE</code> Whether to use in-memory or persistent (RocksDB) Kafka Streams state stores <code>in_memory</code> \u2705 <code>STATE_STORE_ROCKS_DB_COMPACTION_STYLE</code> Compaction style to use for RocksDB state stores - \u274c <code>STATE_STORE_ROCKS_DB_COMPRESSION_TYPE</code> Compression type to use for RocksDB state stores - \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <code>SCANNER_INTERNAL_ENABLED</code> Enable the internal vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_ENABLED</code> Enable the OSS Index vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by OssIndex analyzer. This depends on standard and custom supported package urls <code>cargo,composer,gem,hex,maven,npm,nuget,pypi,rpm,conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by ossindex. These are not part of the standard package urls list <code>conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_API_USERNAME</code> OSS Index API username - \u274c <code>SCANNER_OSSINDEX_API_TOKEN</code> OSS Index API token - \u274c <code>SCANNER_OSSINDEX_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> \u274c <code>SCANNER_OSSINDEX_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSS Index <code>false</code> \u274c <code>SCANNER_SNYK_ENABLED</code> Enable the Snyk vulnerability scanner <code>false</code> \u274c <code>SCANNER_SNYK_API_ORG_ID</code> Snyk organization ID - When Snyk is enabled <code>SCANNER_SNYK_API_TOKENS</code> Comma-separated list of Snyk API tokens - When Snyk is enabled <code>SCANNER_SNYK_API_VERSION</code> Version of the Snyk API to use <code>2022-12-15</code> When Snyk is enabled <code>SCANNER_SNYK_SEVERITY_SOURCE_PRIORITY</code> Priority of preferred source for vulnerability severities <code>nvd,snyk,redhat,suse</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_SIZE</code> Max size of batch at which it will be submitted <code>100</code> When Snyk is enabled <code>SCANNER_SNYK_ALIAS_SYNC_ENABLED</code> Enable alias syncing for Snyk <code>false</code> \u274c <code>SCANNER_SNYK_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by Snyk analyzer. This depends on supported standard and custom package urls <code>cargo,cocoapods,composer,gem,generic,hex,maven,npm,nuget,pypi,swift,golang</code> \u274c <code>SCANNER_SNYK_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by Snyk. These are not part of the standard package urls list <code>cocoapods,apk,swift</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#mirror-service","title":"Mirror Service","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>MIRROR_DATASOURCE_GITHUB_ALIAS_SYNC_ENABLED</code> Enable alias syncing for GitHub Advisories <code>false</code> \u274c <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSV <code>false</code> \u274c <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/repo-meta-analyzer/","title":"Repository Metadata Analyzer","text":""},{"location":"reference/configuration/repo-meta-analyzer/#cache","title":"Cache","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscachecaffeinemetaanalyzerexpire-after-write","title":"quarkus.cache.caffeine.\"metaAnalyzer\".expire-after-write","text":"<p>Defines the time-to-live of cache entries.  </p> Required true Type <code>duration</code> Default <code>PT2H</code> ENV <code>QUARKUS_CACHE_CAFFEINE__METAANALYZER__EXPIRE_AFTER_WRITE</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscachecaffeinemetaanalyzerinitial-capacity","title":"quarkus.cache.caffeine.\"metaAnalyzer\".initial-capacity","text":"<p>Defines the initial capacity of the cache.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>QUARKUS_CACHE_CAFFEINE__METAANALYZER__INITIAL_CAPACITY</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscacheenabled","title":"quarkus.cache.enabled","text":"<p>Defines whether caching of analysis results shall be enabled.  </p> Required true Type <code>boolean</code> Default <code>true</code> ENV <code>QUARKUS_CACHE_ENABLED</code>"},{"location":"reference/configuration/repo-meta-analyzer/#database","title":"Database","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourcejdbcurl","title":"quarkus.datasource.jdbc.url","text":"<p>Specifies the JDBC URL to use when connecting to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_JDBC_URL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourcepassword","title":"quarkus.datasource.password","text":"<p>Specifies the password to use when authenticating to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_PASSWORD</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourceusername","title":"quarkus.datasource.username","text":"<p>Specifies the username to use when authenticating to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_USERNAME</code>"},{"location":"reference/configuration/repo-meta-analyzer/#general","title":"General","text":""},{"location":"reference/configuration/repo-meta-analyzer/#secretkeypath","title":"secret.key.path","text":"<p>Defines the path to the secret key to be used for data encryption and decryption.  </p> Required false Type <code>string</code> Default <code>~/.dependency-track/keys/secret.key</code> ENV <code>SECRET_KEY_PATH</code>"},{"location":"reference/configuration/repo-meta-analyzer/#http","title":"HTTP","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkushttpport","title":"quarkus.http.port","text":"<p>HTTP port to listen on. Application metrics will be available via this port.  </p> Required false Type <code>integer</code> Default <code>8091</code> ENV <code>QUARKUS_HTTP_PORT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/repo-meta-analyzer/#dtkafkatopicprefix","title":"dt.kafka.topic.prefix","text":"<p>Defines an optional prefix to assume for all Kafka topics the application  consumes from, or produces to. The prefix will also be prepended to the  application's consumer group ID.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>acme-</code> ENV <code>DT_KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsautooffsetreset","title":"kafka-streams.auto.offset.reset","text":"<p>Refer to https://kafka.apache.org/documentation/#consumerconfigs_auto.offset.reset for details.  </p> Required false Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_STREAMS_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamscommitintervalms","title":"kafka-streams.commit.interval.ms","text":"<p>Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.  The Kafka default of <code>30s</code> has been modified to <code>5s</code>.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms for details.  </p> Required false Type <code>integer</code> Default <code>5000</code> ENV <code>KAFKA_STREAMS_COMMIT_INTERVAL_MS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsdeserializationcount","title":"kafka-streams.exception.thresholds.deserialization.count","text":"<p>Defines the threshold for records failing to be deserialized within <code>kafka-streams.exception.thresholds.deserialization.interval</code>.  Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsdeserializationinterval","title":"kafka-streams.exception.thresholds.deserialization.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.deserialization.count</code> records are  allowed to fail deserialization. Deserialization failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsprocessingcount","title":"kafka-streams.exception.thresholds.processing.count","text":"<p>Defines the threshold for records failing to be processed within <code>kafka-streams.exception.thresholds.processing.interval</code>.  Processing failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>50</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsprocessinginterval","title":"kafka-streams.exception.thresholds.processing.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.processing.count</code> records are  allowed to fail processing. Processing failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsproductioncount","title":"kafka-streams.exception.thresholds.production.count","text":"<p>Defines the threshold for records failing to be produced within <code>kafka-streams.exception.thresholds.production.interval</code>.  Production failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsproductioninterval","title":"kafka-streams.exception.thresholds.production.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.production.count</code> records are  allowed to fail producing. Production failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsmetricsrecordinglevel","title":"kafka-streams.metrics.recording.level","text":"<p>Refer to https://kafka.apache.org/documentation/#adminclientconfigs_metrics.recording.level for details.  </p> Required false Type <code>enum</code> Valid Values <code>[INFO, DEBUG, TRACE]</code> Default <code>DEBUG</code> ENV <code>KAFKA_STREAMS_METRICS_RECORDING_LEVEL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsnumstreamthreads","title":"kafka-streams.num.stream.threads","text":"<p>Refer to https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads for details.  </p> Required true Type <code>integer</code> Default <code>3</code> ENV <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"<p>Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for details.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>broker-01.acme.com:9092,broker-02.acme.com:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuskafka-streamsapplication-id","title":"quarkus.kafka-streams.application-id","text":"<p>Defines the ID to uniquely identify this application in the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_application.id for details.  </p> Required false Type <code>string</code> Default <code>${dt.kafka.topic.prefix}hyades-repository-meta-analyzer</code> ENV <code>QUARKUS_KAFKA_STREAMS_APPLICATION_ID</code>"},{"location":"reference/configuration/repo-meta-analyzer/#observability","title":"Observability","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkuslogconsolejson","title":"quarkus.log.console.json","text":"<p>Defines whether logs should be written in JSON format.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>QUARKUS_LOG_CONSOLE_JSON</code>"},{"location":"reference/schemas/notification/","title":"Notification","text":""},{"location":"reference/schemas/notification/#notification","title":"Notification","text":"Field Type Description <code>level</code> <code>Level</code> <code>scope</code> <code>Scope</code> <code>group</code> <code>Group</code> <code>title</code> <code>string</code> <code>content</code> <code>string</code> <code>timestamp</code> <code>google.protobuf.Timestamp</code> <code>subject</code> <code>google.protobuf.Any</code>"},{"location":"reference/schemas/notification/#subjects","title":"Subjects","text":""},{"location":"reference/schemas/notification/#bomconsumedorprocessedsubject","title":"BomConsumedOrProcessedSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>bom</code> <code>Bom</code> <code>token</code> <code>string</code>"},{"location":"reference/schemas/notification/#bomprocessingfailedsubject","title":"BomProcessingFailedSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>bom</code> <code>Bom</code> <code>cause</code> <code>string</code> <code>token</code> <code>string</code>"},{"location":"reference/schemas/notification/#bomvalidationfailedsubject","title":"BomValidationFailedSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>bom</code> <code>Bom</code> <code>errors</code> <code>string[]</code>"},{"location":"reference/schemas/notification/#componentvulnanalysiscompletesubject","title":"ComponentVulnAnalysisCompleteSubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>vulnerabilities</code> <code>Vulnerability[]</code>"},{"location":"reference/schemas/notification/#newvulnerabilitysubject","title":"NewVulnerabilitySubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>vulnerability</code> <code>Vulnerability</code> <code>affected_projects_reference</code> <code>BackReference</code> <code>vulnerability_analysis_level</code> <code>string</code> <code>affected_projects</code> <code>Project[]</code> List of projects affected by the vulnerability. DEPRECATED: This list only holds one item, and it is identical to the one in the project field. The field is kept for backward compatibility of JSON notifications, but consumers should not expect multiple projects here. Transmitting all affected projects in one notification is not feasible for large portfolios, see https://github.com/DependencyTrack/hyades/issues/467 for details."},{"location":"reference/schemas/notification/#newvulnerabledependencysubject","title":"NewVulnerableDependencySubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>vulnerabilities</code> <code>Vulnerability[]</code>"},{"location":"reference/schemas/notification/#policyviolationanalysisdecisionchangesubject","title":"PolicyViolationAnalysisDecisionChangeSubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>policy_violation</code> <code>PolicyViolation</code> <code>analysis</code> <code>PolicyViolationAnalysis</code>"},{"location":"reference/schemas/notification/#policyviolationsubject","title":"PolicyViolationSubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>policy_violation</code> <code>PolicyViolation</code>"},{"location":"reference/schemas/notification/#projectvulnanalysiscompletesubject","title":"ProjectVulnAnalysisCompleteSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>findings</code> <code>ComponentVulnAnalysisCompleteSubject[]</code> <code>status</code> <code>ProjectVulnAnalysisStatus</code> <code>token</code> <code>string</code>"},{"location":"reference/schemas/notification/#usersubject","title":"UserSubject","text":"Field Type Description <code>username</code> <code>string</code> <code>email</code> <code>string</code>"},{"location":"reference/schemas/notification/#vexconsumedorprocessedsubject","title":"VexConsumedOrProcessedSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>vex</code> <code>bytes</code> <code>format</code> <code>string</code> <code>spec_version</code> <code>string</code>"},{"location":"reference/schemas/notification/#vulnerabilityanalysisdecisionchangesubject","title":"VulnerabilityAnalysisDecisionChangeSubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>vulnerability</code> <code>Vulnerability</code> <code>analysis</code> <code>VulnerabilityAnalysis</code>"},{"location":"reference/schemas/notification/#messages","title":"Messages","text":""},{"location":"reference/schemas/notification/#backreference","title":"BackReference","text":"Field Type Description <code>api_uri</code> <code>string</code> URI to the API endpoint from which additional information can be fetched. <code>frontend_uri</code> <code>string</code> URI to the frontend where additional information can be seen."},{"location":"reference/schemas/notification/#bom","title":"Bom","text":"Field Type Description <code>content</code> <code>string</code> <code>format</code> <code>string</code> <code>spec_version</code> <code>string</code>"},{"location":"reference/schemas/notification/#component","title":"Component","text":"Field Type Description <code>uuid</code> <code>string</code> <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>version</code> <code>string</code> <code>purl</code> <code>string</code> <code>md5</code> <code>string</code> <code>sha1</code> <code>string</code> <code>sha256</code> <code>string</code> <code>sha512</code> <code>string</code>"},{"location":"reference/schemas/notification/#policy","title":"Policy","text":"Field Type Description <code>uuid</code> <code>string</code> <code>name</code> <code>string</code> <code>violation_state</code> <code>string</code>"},{"location":"reference/schemas/notification/#policycondition","title":"PolicyCondition","text":"Field Type Description <code>uuid</code> <code>string</code> <code>subject</code> <code>string</code> <code>operator</code> <code>string</code> <code>value</code> <code>string</code> <code>policy</code> <code>Policy</code>"},{"location":"reference/schemas/notification/#policyviolation","title":"PolicyViolation","text":"Field Type Description <code>uuid</code> <code>string</code> <code>type</code> <code>string</code> <code>timestamp</code> <code>google.protobuf.Timestamp</code> <code>condition</code> <code>PolicyCondition</code>"},{"location":"reference/schemas/notification/#policyviolationanalysis","title":"PolicyViolationAnalysis","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>policy_violation</code> <code>PolicyViolation</code> <code>state</code> <code>string</code> <code>suppressed</code> <code>bool</code>"},{"location":"reference/schemas/notification/#project","title":"Project","text":"Field Type Description <code>uuid</code> <code>string</code> <code>name</code> <code>string</code> <code>version</code> <code>string</code> <code>description</code> <code>string</code> <code>purl</code> <code>string</code> <code>tags</code> <code>string[]</code>"},{"location":"reference/schemas/notification/#vulnerability","title":"Vulnerability","text":"Field Type Description <code>uuid</code> <code>string</code> <code>vuln_id</code> <code>string</code> <code>source</code> <code>string</code> <code>aliases</code> <code>Vulnerability.Alias[]</code> <code>title</code> <code>string</code> <code>sub_title</code> <code>string</code> <code>description</code> <code>string</code> <code>recommendation</code> <code>string</code> <code>cvss_v2</code> <code>double</code> <code>cvss_v3</code> <code>double</code> <code>owasp_rr_likelihood</code> <code>double</code> <code>owasp_rr_technical_impact</code> <code>double</code> <code>owasp_rr_business_impact</code> <code>double</code> <code>severity</code> <code>string</code> <code>cwes</code> <code>Vulnerability.Cwe[]</code> <code>cvss_v2_vector</code> <code>string</code> <code>cvss_v3_vector</code> <code>string</code> <code>owasp_rr_vector</code> <code>string</code>"},{"location":"reference/schemas/notification/#vulnerabilityalias","title":"Vulnerability.Alias","text":"Field Type Description <code>id</code> <code>string</code> <code>source</code> <code>string</code>"},{"location":"reference/schemas/notification/#vulnerabilitycwe","title":"Vulnerability.Cwe","text":"Field Type Description <code>cwe_id</code> <code>int32</code> <code>name</code> <code>string</code>"},{"location":"reference/schemas/notification/#vulnerabilityanalysis","title":"VulnerabilityAnalysis","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>vulnerability</code> <code>Vulnerability</code> <code>state</code> <code>string</code> <code>suppressed</code> <code>bool</code>"},{"location":"reference/schemas/notification/#enums","title":"Enums","text":""},{"location":"reference/schemas/notification/#group","title":"Group","text":"Name Description <code>GROUP_UNSPECIFIED</code> <code>GROUP_CONFIGURATION</code> <code>GROUP_DATASOURCE_MIRRORING</code> <code>GROUP_REPOSITORY</code> <code>GROUP_INTEGRATION</code> <code>GROUP_FILE_SYSTEM</code> <code>GROUP_ANALYZER</code> <code>GROUP_NEW_VULNERABILITY</code> <code>GROUP_NEW_VULNERABLE_DEPENDENCY</code> <code>GROUP_PROJECT_AUDIT_CHANGE</code> <code>GROUP_BOM_CONSUMED</code> <code>GROUP_BOM_PROCESSED</code> <code>GROUP_VEX_CONSUMED</code> <code>GROUP_VEX_PROCESSED</code> <code>GROUP_POLICY_VIOLATION</code> <code>GROUP_PROJECT_CREATED</code> <code>GROUP_BOM_PROCESSING_FAILED</code> <code>GROUP_PROJECT_VULN_ANALYSIS_COMPLETE</code> <code>GROUP_USER_CREATED</code> <code>GROUP_USER_DELETED</code> <code>GROUP_BOM_VALIDATION_FAILED</code>"},{"location":"reference/schemas/notification/#level","title":"Level","text":"Name Description <code>LEVEL_UNSPECIFIED</code> <code>LEVEL_INFORMATIONAL</code> <code>LEVEL_WARNING</code> <code>LEVEL_ERROR</code>"},{"location":"reference/schemas/notification/#projectvulnanalysisstatus","title":"ProjectVulnAnalysisStatus","text":"Name Description <code>PROJECT_VULN_ANALYSIS_STATUS_UNSPECIFIED</code> <code>PROJECT_VULN_ANALYSIS_STATUS_FAILED</code> <code>PROJECT_VULN_ANALYSIS_STATUS_COMPLETED</code>"},{"location":"reference/schemas/notification/#scope","title":"Scope","text":"Name Description <code>SCOPE_UNSPECIFIED</code> <code>SCOPE_PORTFOLIO</code> <code>SCOPE_SYSTEM</code>"},{"location":"reference/schemas/policy/","title":"Policy","text":""},{"location":"reference/schemas/policy/#messages","title":"Messages","text":""},{"location":"reference/schemas/policy/#component","title":"Component","text":"Field Type Description <code>uuid</code> <code>string</code> UUID of the component. <code>group</code> <code>string</code> Group / namespace of the component. <code>name</code> <code>string</code> Name of the component. <code>version</code> <code>string</code> Version of the component. <code>classifier</code> <code>string</code> Classifier / type of the component. May be any of: - APPLICATION - CONTAINER - DEVICE - FILE - FIRMWARE - FRAMEWORK - LIBRARY - OPERATING_SYSTEM <code>cpe</code> <code>string</code> CPE of the component. https://csrc.nist.gov/projects/security-content-automation-protocol/specifications/cpe <code>purl</code> <code>string</code> Package URL of the component. https://github.com/package-url/purl-spec <code>swid_tag_id</code> <code>string</code> SWID tag ID of the component. https://csrc.nist.gov/projects/Software-Identification-SWID <code>is_internal</code> <code>bool</code> Whether the component is internal to the organization. <code>md5</code> <code>string</code> <code>sha1</code> <code>string</code> <code>sha256</code> <code>string</code> <code>sha384</code> <code>string</code> <code>sha512</code> <code>string</code> <code>sha3_256</code> <code>string</code> <code>sha3_384</code> <code>string</code> <code>sha3_512</code> <code>string</code> <code>blake2b_256</code> <code>string</code> <code>blake2b_384</code> <code>string</code> <code>blake2b_512</code> <code>string</code> <code>blake3</code> <code>string</code> <code>license_name</code> <code>string</code> <code>license_expression</code> <code>string</code> <code>resolved_license</code> <code>License</code> <code>published_at</code> <code>google.protobuf.Timestamp</code> When the component current version last modified. <code>latest_version</code> <code>string</code>"},{"location":"reference/schemas/policy/#license","title":"License","text":"Field Type Description <code>uuid</code> <code>string</code> <code>id</code> <code>string</code> <code>name</code> <code>string</code> <code>groups</code> <code>License.Group[]</code> <code>is_osi_approved</code> <code>bool</code> <code>is_fsf_libre</code> <code>bool</code> <code>is_deprecated_id</code> <code>bool</code> <code>is_custom</code> <code>bool</code>"},{"location":"reference/schemas/policy/#licensegroup","title":"License.Group","text":"Field Type Description <code>uuid</code> <code>string</code> <code>name</code> <code>string</code>"},{"location":"reference/schemas/policy/#project","title":"Project","text":"Field Type Description <code>uuid</code> <code>string</code> <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>version</code> <code>string</code> <code>classifier</code> <code>string</code> <code>is_active</code> <code>bool</code> <code>tags</code> <code>string[]</code> <code>properties</code> <code>Project.Property[]</code> <code>cpe</code> <code>string</code> <code>purl</code> <code>string</code> <code>swid_tag_id</code> <code>string</code> <code>last_bom_import</code> <code>google.protobuf.Timestamp</code> <code>metadata</code> <code>Project.Metadata</code>"},{"location":"reference/schemas/policy/#projectmetadata","title":"Project.Metadata","text":"Field Type Description <code>tools</code> <code>Tools</code> <code>bom_generated</code> <code>google.protobuf.Timestamp</code>"},{"location":"reference/schemas/policy/#projectproperty","title":"Project.Property","text":"Field Type Description <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>value</code> <code>string</code> <code>type</code> <code>string</code>"},{"location":"reference/schemas/policy/#tools","title":"Tools","text":"Field Type Description <code>components</code> <code>Component[]</code> Components used as tools."},{"location":"reference/schemas/policy/#versiondistance","title":"VersionDistance","text":"Field Type Description <code>epoch</code> <code>string</code> <code>major</code> <code>string</code> <code>minor</code> <code>string</code> <code>patch</code> <code>string</code>"},{"location":"reference/schemas/policy/#vulnerability","title":"Vulnerability","text":"Field Type Description <code>uuid</code> <code>string</code> <code>id</code> <code>string</code> <code>source</code> <code>string</code> <code>aliases</code> <code>Vulnerability.Alias[]</code> <code>cwes</code> <code>int32[]</code> <code>created</code> <code>google.protobuf.Timestamp</code> <code>published</code> <code>google.protobuf.Timestamp</code> <code>updated</code> <code>google.protobuf.Timestamp</code> <code>severity</code> <code>string</code> <code>cvssv2_base_score</code> <code>double</code> <code>cvssv2_impact_subscore</code> <code>double</code> <code>cvssv2_exploitability_subscore</code> <code>double</code> <code>cvssv2_vector</code> <code>string</code> <code>cvssv3_base_score</code> <code>double</code> <code>cvssv3_impact_subscore</code> <code>double</code> <code>cvssv3_exploitability_subscore</code> <code>double</code> <code>cvssv3_vector</code> <code>string</code> <code>owasp_rr_likelihood_score</code> <code>double</code> <code>owasp_rr_technical_impact_score</code> <code>double</code> <code>owasp_rr_business_impact_score</code> <code>double</code> <code>owasp_rr_vector</code> <code>string</code> <code>epss_score</code> <code>double</code> <code>epss_percentile</code> <code>double</code>"},{"location":"reference/schemas/policy/#vulnerabilityalias","title":"Vulnerability.Alias","text":"Field Type Description <code>id</code> <code>string</code> <code>source</code> <code>string</code>"},{"location":"usage/policy-compliance/expressions/","title":"Expressions","text":""},{"location":"usage/policy-compliance/expressions/#introduction","title":"Introduction","text":"<p>Dependency-Track allows policy conditions to be defined using the Common Expression Language (CEL), enabling more flexibility, and more control compared to predefined conditions.</p> <p>To use CEL, simply select the subject <code>Expression</code> when adding a new condition. A code editor will appear in which expressions can be provided.</p> <p></p> <p>In addition to the expression itself, it's necessary to specify a violation type, which may be any of <code>License</code>, <code>Operational</code>, or <code>Security</code>. The violation type aids in communicating what kind of risk is introduced by the condition being matched.</p>"},{"location":"usage/policy-compliance/expressions/#syntax","title":"Syntax","text":"<p>The CEL syntax is similar to other C-style languages like Java and JavaScript. However, CEL is not Turing-complete. As such, it does not support constructs like <code>if</code> statements or loops (i.e. <code>for</code>, <code>while</code>).</p> <p>As a compensation for missing loops, CEL offers macros like <code>all</code>, <code>exists</code>, <code>exists_one</code>, <code>map</code>, and <code>filter</code>. Refer to the macros documentation for more details, or have a look at the examples to see how they may be utilized in practice.</p> <p>CEL syntax is described thoroughly in the official language definition.</p>"},{"location":"usage/policy-compliance/expressions/#evaluation-context","title":"Evaluation Context","text":"<p>Conditions are scoped to individual components. Each condition is evaluated for every single component in a project.</p> <p>The context in which expressions are evaluated in contains the following variables:</p> Variable Type Description <code>component</code> <code>Component</code> The component being evaluated <code>project</code> <code>Project</code> The project the component is part of <code>vulns</code> <code>list(Vulnerability)</code> Vulnerabilities the component is affected by"},{"location":"usage/policy-compliance/expressions/#best-practices","title":"Best Practices","text":"<ol> <li>Keep expressions simple and concise. The more complex an expression becomes, the harder it gets to determine why it did or did not match. Use policy operators (<code>Any</code>, <code>All</code>) to chain multiple expressions if practical.</li> <li>Call functions last. Custom functions involve additional computation that is more expensive than simple field accesses. Performing any checks on fields first, and calling functions last, oftentimes allows evaluation to short-circuit.</li> <li>Remove conditions that are no longer needed. Dependency-Track analyzes the configured expressions to determine what data it has to load from the database in order to evaluate them. The more fields are being accessed, the more data has to be loaded. Removal of outdated conditions thus has a direct positive performance impact.</li> </ol>"},{"location":"usage/policy-compliance/expressions/#examples","title":"Examples","text":""},{"location":"usage/policy-compliance/expressions/#component-age","title":"Component age","text":"<p>Besides out-of-date versions, component age is another indicator of potential risk. Components may be on the latest available version, but still be 20 years old. </p> <p>Component age can be evaluated using the <code>compare_age</code> function. The first function argument  is a numeric comparator (<code>&lt;</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&gt;=</code>), and the second is a duration in ISO8601 notation.</p> <p>The following expression matches Components that are two years old, or even older:</p> <pre><code>component.compare_age(\"&gt;=\", \"P2Y\")\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#component-blacklist","title":"Component blacklist","text":"<p>The following expression matches on the Component's Package URL, using a regular expression in RE2 syntax. Additionally, it checks whether the Component's version falls into a given vers range, consisting of multiple constraints.</p> <pre><code>component.purl.matches(\"^pkg:maven/com.acme/acme-lib\\\\b.*\")\n  &amp;&amp; component.matches_range(\"vers:maven/&gt;0|&lt;1|!=0.2.4\")\n</code></pre> <p>The expression will match:</p> <ul> <li><code>pkg:maven/com.acme/acme-lib@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.9.9</code></li> </ul> <p>but not:</p> <ul> <li><code>pkg:maven/com.acme/acme-library@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.2.4</code></li> </ul> <p><code>matches_range</code> currently supports the following versioning schemes:</p> Versioning Scheme Ecosystem <code>deb</code> Debian / Ubuntu <code>generic</code> Generic / Any <code>golang</code> Go <code>maven</code> Java / Maven <code>npm</code> JavaScript / NodeJS <code>rpm</code> CentOS / Fedora / Red Hat / SUSE <p>Note</p> <p>If the ecosystem of the component(s) to match against is known upfront, it's good practice to use the according versioning scheme in <code>matches_range</code>. This helps with accuracy, as versioning schemes have different nuances across ecosystems, which makes comparisons error-prone.</p>"},{"location":"usage/policy-compliance/expressions/#dependency-graph-traversal","title":"Dependency graph traversal","text":"<p>The following expression matches Components that are a (possibly transitive) dependency of a Component with name <code>foo</code>, but only if a Component with name <code>bar</code> is also present in the Project.</p> <pre><code>component.is_dependency_of(v1.Component{name: \"foo\"})\n  &amp;&amp; project.depends_on(v1.Component{name: \"bar\"})\n</code></pre> <p><code>is_dependency_of</code> and <code>depends_on</code> lookups currently support the following Component fields:</p> <ul> <li><code>uuid</code></li> <li><code>group</code></li> <li><code>name</code></li> <li><code>version</code></li> <li><code>classifier</code></li> <li><code>cpe</code></li> <li><code>purl</code></li> <li><code>swid_tag_id</code></li> <li><code>internal</code></li> </ul> <p>Initially, only exact matches on those fields are supported. In the future, more sophisticated matching options will be added.</p> <p>Note</p> <p>When constructing objects like Component on-the-fly, it is necessary to use their version namespace, i.e. <code>v1</code>. This is required in order to perform type checking, as well as ensuring backward compatibility.</p>"},{"location":"usage/policy-compliance/expressions/#license-blacklist","title":"License blacklist","text":"<p>The following expression matches Components that are not internal to the organization, and have either:</p> <ul> <li>No resolved License at all</li> <li>A resolved License that is not part of the <code>Permissive</code> license group</li> </ul> <pre><code>!component.is_internal &amp;&amp; (\n  !has(component.resolved_license)\n    || component.resolved_license.groups.exisits(licenseGroup, \n         licenseGroup.name == \"Permissive\")\n)\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerability-blacklist","title":"Vulnerability blacklist","text":"<p>The following expression matches Components in Projects tagged as <code>3rd-party</code>, with at least one Vulnerability being any of the given blacklisted IDs.</p> <pre><code>\"3rd-party\" in project.tags\n  &amp;&amp; vulns.exists(vuln, vuln.id in [\n       \"CVE-2017-5638\",  // struts RCE\n       \"CVE-2021-44228\", // log4shell\n       \"CVE-2022-22965\", // spring4shell\n     ])\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerabilities-with-high-severity-in-public-facing-projects","title":"Vulnerabilities with high severity in public facing projects","text":"<p>The following expression matches Components in Projects tagged as <code>public-facing</code>, with at least one <code>HIGH</code> or <code>CRITICAL</code> Vulnerability, where the CVSSv3 attack vector is <code>Network</code>.</p> <pre><code>\"public-facing\" in project.tags\n  &amp;&amp; vulns.exists(vuln,\n    vuln.severity in [\"HIGH\", \"CRITICAL\"]\n      &amp;&amp; vuln.cvssv3_vector.matches(\".*/AV:N/.*\")\n  )\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#reference","title":"Reference","text":""},{"location":"usage/policy-compliance/expressions/#types","title":"Types","text":""},{"location":"usage/policy-compliance/expressions/#component","title":"<code>Component</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>is_internal</code> <code>bool</code> Is internal? <code>md5</code> <code>string</code> MD5 hash <code>sha1</code> <code>string</code> SHA1 hash <code>sha256</code> <code>string</code> SHA256 hash <code>sha384</code> <code>string</code> SHA384 hash <code>sha512</code> <code>string</code> SHA512 hash <code>sha3_256</code> <code>string</code> SHA3-256 hash <code>sha3_384</code> <code>string</code> SHA3-384 hash <code>sha3_512</code> <code>string</code> SHA3-512 hash <code>blake2b_256</code> <code>string</code> BLAKE2b-256 hash <code>blake2b_384</code> <code>string</code> BLAKE2b-384 hash <code>blake2b_512</code> <code>string</code> BLAKE2b-512 hash <code>blake3</code> <code>string</code> BLAKE3 hash <code>license_name</code> <code>string</code> License name (if unresolved) <code>license_expression</code> <code>string</code> SPDX license expression <code>resolved_license</code> <code>License</code> Resolved license <code>published_at</code> <code>google.protobuf.Timestamp</code> When the component was published <code>latest_version</code> <code>string</code> Latest known version"},{"location":"usage/policy-compliance/expressions/#license","title":"<code>License</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> SPDX license ID <code>name</code> <code>string</code> License name <code>groups</code> <code>list(License.Group)</code> Groups this license is included in <code>is_osi_approved</code> <code>bool</code> Is OSI-approved? <code>is_fsf_libre</code> <code>bool</code> Is included in FSF license list? <code>is_deprecated_id</code> <code>bool</code> Uses a deprecated SPDX license ID? <code>is_custom</code> <code>bool</code> Is custom / not included in SPDX license list?"},{"location":"usage/policy-compliance/expressions/#licensegroup","title":"<code>License.Group</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>name</code> <code>string</code> Group name"},{"location":"usage/policy-compliance/expressions/#project","title":"<code>Project</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>is_active</code> <code>bool</code> Is active? <code>tags</code> <code>list(string)</code> Tags <code>properties</code> <code>list(Project.Property)</code> Properties <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>last_bom_import</code> <code>google.protobuf.Timestamp</code>"},{"location":"usage/policy-compliance/expressions/#projectproperty","title":"<code>Project.Property</code>","text":"Field Type Description <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>value</code> <code>string</code> <code>type</code> <code>string</code>"},{"location":"usage/policy-compliance/expressions/#vulnerability","title":"<code>Vulnerability</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>CVE-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>NVD</code>) <code>aliases</code> <code>list(Vulnerability.Alias)</code> Known aliases <code>cwes</code> <code>list(int)</code> CWE IDs <code>created</code> <code>google.protobuf.Timestamp</code> When the vulnerability was created <code>published</code> <code>google.protobuf.Timestamp</code> When the vulnerability was published <code>updated</code> <code>google.protobuf.Timestamp</code> Then the vulnerability was updated <code>severity</code> <code>string</code> <code>cvssv2_base_score</code> <code>double</code> CVSSv2 base score <code>cvssv2_impact_subscore</code> <code>double</code> CVSSv2 impact sub score <code>cvssv2_exploitability_subscore</code> <code>double</code> CVSSv2 exploitability sub score <code>cvssv2_vector</code> <code>string</code> CVSSv2 vector <code>cvssv3_base_score</code> <code>double</code> CVSSv3 base score <code>cvssv3_impact_subscore</code> <code>double</code> CVSSv3 impact sub score <code>cvssv3_exploitability_subscore</code> <code>double</code> CVSSv3 exploitability sub score <code>cvssv3_vector</code> <code>string</code> CVSSv3 vector <code>owasp_rr_likelihood_score</code> <code>double</code> OWASP Risk Rating likelihood score <code>owasp_rr_technical_impact_score</code> <code>double</code> OWASP Risk Rating technical impact score <code>owasp_rr_business_impact_score</code> <code>double</code> OWASP Risk Rating business impact score <code>owasp_rr_vector</code> <code>string</code> OWASP Risk Rating vector <code>epss_score</code> <code>double</code> EPSS score <code>epss_percentile</code> <code>double</code> EPSS percentile"},{"location":"usage/policy-compliance/expressions/#vulnerabilityalias","title":"<code>Vulnerability.Alias</code>","text":"Field Type Description <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>GHSA-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>GITHUB</code>)"},{"location":"usage/policy-compliance/expressions/#function-definitions","title":"Function Definitions","text":"<p>In addition to the standard definitions of the CEL specification, Dependency-Track offers additional functions to unlock even more use cases:</p> Symbol Type Description <code>depends_on</code> <code>(Project, Component)</code> -&gt; <code>bool</code> Check if <code>Project</code> depends on <code>Component</code> <code>compare_age</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s age matches a given duration <code>is_dependency_of</code> <code>(Component, Component)</code> -&gt; <code>bool</code> Check if a <code>Component</code> is a dependency of another <code>Component</code> <code>matches_range</code> <code>(Project, string)</code> -&gt; <code>bool</code><code>(Component, string)</code> -&gt; <code>bool</code> Check if a <code>Project</code> or <code>Component</code> matches a vers range <code>matches_version_distance</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s version matches a given distance"},{"location":"usage/policy-compliance/overview/","title":"Overview","text":"<p>Organizations can create policies and measure policy violations across the portfolio, and against individual projects and components. Policies are configurable and can be enforced for the portfolio, or can be limited to specific projects. Policies are evaluated when an SBOM is uploaded.</p> <p>There are three types of policy violations:</p> <ul> <li>License</li> <li>Security</li> <li>Operational</li> </ul>"},{"location":"usage/policy-compliance/overview/#license-violation","title":"License Violation","text":"<p>Policy conditions can specify zero or more SPDX license IDs as well as license groups. Dependency-Track comes with pre-configured groups of related licenses (e.g. Copyleft) that provide a starting point for organizations to create custom license policies.</p>"},{"location":"usage/policy-compliance/overview/#security-violation","title":"Security Violation","text":"<p>Policy conditions can specify the severity of vulnerabilities. A vulnerability affecting a component can result in a policy violation if the policy condition matches the severity of the vulnerability. Vulnerabilities that are suppressed will not result in a policy violation.</p>"},{"location":"usage/policy-compliance/overview/#operational-violation","title":"Operational Violation","text":"<p>Policy conditions can specify zero or more:</p> <ul> <li>Coordinates (group, name, version)</li> <li>Package URL</li> <li>CPE</li> <li>SWID Tag ID</li> <li>Hash (MD5, SHA, SHA3, Blake2b, Blake3)</li> </ul> <p>This allows organizations to create lists of allowable and/or prohibited components. Future versions of Dependency-Track will incorporate additional operational parameters into the policy framework.</p>"}]}