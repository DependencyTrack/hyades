{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"\"Hyades\" by Todd Vance (CC BY-SA 2.5)"},{"location":"#introduction","title":"Introduction","text":"<p>Project Hyades, named after the star cluster closest to earth, is an incubating project for decoupling responsibilities from Dependency-Track's monolithic API server into separate, scalable\u2122 services.</p> <p>The main objectives of Hyades are:</p> <ul> <li>Enable Dependency-Track to handle portfolios spanning hundreds of thousands of projects</li> <li>Improve resilience of Dependency-Track, providing more confidence when relying on it in critical workflows</li> <li>Improve deployment and configuration management experience for containerized / cloud native tech stacks</li> </ul> <p>Other than separating responsibilities, the API server has been modified to allow for high availability (active-active) deployments. Various \"hot paths\", like processing of uploaded BOMs, have been optimized in the existing code. Further optimization is an ongoing effort.</p> <p>Hyades already is a superset of Dependency-Track, as changes up to Dependency-Track v4.11.4 were ported, and features made possible by the new architecture have been implemented on top.</p> <p>Warning</p> <p>Hyades is not yet fully production ready, please refer to the GA roadmap for the current status.</p> <p>Already using Dependency-Track v4?</p> <p>Check out Changes over v4 for a complete overview of changes, and have a look at Migrating from v4 for migration instructions.</p>"},{"location":"#background","title":"Background","text":"tl;dr <p>The architecture of Dependency-Track v4 prevents it from scaling past a certain workload.</p> <p>Dependency-Track, for the most part, is an event-based system. As a platform for ingesting data (in the form of BOMs), listening for and itself emitting signals on it, an event-based architecture makes sense conceptually. The majority of operations Dependency-Track performs happen asynchronously, without client interaction.</p> <p>On a technical level, Dependency-Track uses an in-memory publish-subscribe architecture, implemented using Java <code>ExecutorService</code>s. An <code>ExecutorService</code> can be thought of as a pool of worker threads, consuming from an internal task queue. Tasks can be submitted to an <code>ExecutorService</code>, which will then execute them one-by-one. As multiple threads work on the queue in parallel, the order in which tasks are being processed is not guaranteed. Thread pool sizes can vary from one, up to unbounded numbers of threads.</p> <p>In Dependency-Track, when an event is published, subscribers to the event are looked up. Per API contract, event subscribers must implement an <code>inform</code> method, which takes the published event as argument. For any given event, 0-N tasks will be enqueued to the <code>ExecutorService</code>'s task queue - one for each subscriber.</p> <p></p> <p>There are three <code>ExecutorService</code> instances in Dependency-Track:</p> <ul> <li><code>EventService</code></li> <li><code>SingleThreadedEventService</code></li> <li><code>NotificationService</code></li> </ul> <p><code>EventService</code> forms the primary worker pool. Its thread pool size defaults to <code>&lt;NUM_CPU&gt; * 4</code>. A machine with a 4-core CPU will thus have a thread pool size of <code>16</code>. The size is configurable. Common tasks handled by this worker pool include:</p> <ul> <li>Processing of uploaded BOMs and VEXs</li> <li>Performing vulnerability analysis of components, projects, or the entire portfolio</li> <li>Performing repository meta analysis of components, projects, or the entire portfolio</li> <li>Calculation of metrics for components, projects, or the entire portfolio</li> </ul> <p><code>SingleThreadedEventService</code> is a worker pool with only a single thread. The purpose of this worker pool is execute tasks that must not be run in parallel. As such, it serves as a means to serialize task execution. Common tasks handled by this worker pool include:</p> <ul> <li>Mirroring of the NVD vulnerability database</li> <li>Updating Lucene indexes on disk</li> </ul> <p><code>NotificationService</code> is a dedicated worker pool for dispatching notifications. Its thread pool size defaults to <code>4</code> and is not configurable.</p>"},{"location":"#limitations-of-the-v4-architecture","title":"Limitations of the v4 architecture","text":"<p>While this architecture works great for small to medium workloads, it presents various challenges for larger ones:</p> <ol> <li>Not horizontally scalable. As pub-sub is happening entirely in-memory, it is not possible to distribute    the work to multiple application instances. The only way to handle more load using this architecture is to scale    vertically, e.g.<ul> <li>Increasing <code>ExecutorService</code> thread pool sizes (<code>alpine.worker.threads</code>, <code>alpine.worker.thread.multiplier</code>)</li> <li>Increasing database connection pool sizes</li> <li>Increasing resource allocations for CPU, RAM, and potentially disk / network</li> </ul> </li> <li>No ordering guarantees of events. As multiple threads work on a shared queue of tasks in parallel, there is no way    of enforcing a reliable ordering of events.</li> <li>Limited fault-tolerance. If an instance of Dependency-Track goes down, planned or unplanned, all queued tasks are    gone. Not only does this impact business-as-usual operation, but also limits the times when upgrades can be applied.</li> <li>Shared, multipurpose task queue. A single task queue is used to process all kinds of events. This means that    lots of events of a certain type can \"clog\" the task queue, preventing other types of events from being processed.    This is further amplified if processing of events \"clogging\" the queue relies on external services, introducing further    latency. Ideally, there should be a dedicated queue per event type, so that one busy queue doesn't block others.</li> <li>Prone to race conditions. As a consequence of (2), it is possible that multiple events addressing the same    thing are processed in parallel, leading to race conditions in cache lookups or database operations. Race conditions    would be an even bigger problem if the work was shared across multiple application instances, and would require    distributed locking as a countermeasure, which is inherently hard to get right.</li> </ol> <p>In order to scale Dependency-Track beyond its current capabilities, a distributed messaging service is required.</p>"},{"location":"#related-issues","title":"Related Issues","text":"<p>On multiple occasions in the past, the Dependency-Track community raised questions about high availability (HA) deployments, and / or how to better scale the platform:</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/issues/218</li> <li>https://github.com/DependencyTrack/dependency-track/issues/903</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1210</li> <li>https://github.com/DependencyTrack/dependency-track/issues/1856</li> </ul>"},{"location":"architecture/","title":"Overview","text":""},{"location":"architecture/#components","title":"Components","text":""},{"location":"architecture/#api-server","title":"API Server","text":"<p>TBD</p>"},{"location":"architecture/#message-broker","title":"Message Broker","text":"<p>TBD</p>"},{"location":"architecture/#database","title":"Database","text":"<p>TBD</p>"},{"location":"architecture/#vulnerability-analyzer","title":"Vulnerability Analyzer","text":"<p>TBD</p>"},{"location":"architecture/#repository-metadata-analyzer","title":"Repository Metadata Analyzer","text":"<p>TBD</p>"},{"location":"architecture/#notification-publisher","title":"Notification Publisher","text":"<p>TBD</p>"},{"location":"architecture/decisions/000-index/","title":"Overview","text":"<p>Here is where we keep our architecture decision records (ADRs).</p>"},{"location":"architecture/decisions/000-index/#template","title":"Template","text":"<p>Use the following template (source) when adding a new ADR:</p> <pre><code>| Status                                               | Date       | Author(s)                                        |\n|:-----------------------------------------------------|:-----------|:-------------------------------------------------|\n| Proposed, Accepted, Rejected, Deprecated, Superseded | yyyy-mm-dd | [@githubHandle](https://github.com/githubHandle) |\n\n## Context\n\nWhat is the issue that we're seeing that is motivating this decision or change?\n\n## Decision\n\nWhat is the change that we're proposing and/or doing?\n\n## Consequences\n\nWhat becomes easier or more difficult to do because of this change?\n</code></pre>"},{"location":"architecture/decisions/001-drop-kafka-dependency/","title":"ADR-001: Drop Kafka Dependency","text":"Status Date Author(s) Proposed 2025-01-07 @nscuro"},{"location":"architecture/decisions/001-drop-kafka-dependency/#context","title":"Context","text":""},{"location":"architecture/decisions/001-drop-kafka-dependency/#how-kafka-is-currently-used","title":"How Kafka is currently used","text":"<p>As of hyades version 0.6.0, Kafka is used for the following purposes:</p> <ul> <li>Notification dispatching. Each notification type has its own Kafka topic. The <code>notification-publisher</code> service   is responsible for consuming from those topics, and publishing notifications based on the configured rules,   i.e. sending Slack messages or Webhooks. Because Kafka does not delete messages after consumption, notifications   can be consumed by multiple clients, and replayed if necessary. Given a consistent message key, Kafka can further   guarantee message ordering.</li> <li>Vulnerability mirroring. Vulnerability records downloaded from the NVD and other sources are not immediately   written to the database. Instead, they are sent to a compacted Kafka topic, from where they are consumed and ingested.   Kafka acts as a firehose that allows ingestion to be performed at a steady rate, without overloading the database.</li> <li>Vulnerability analysis. The API server publishes a record for each to-be-analyzed component to Kafka.   The <code>vulnerability-analyzer</code> service consumes from this topic, scans each component with all configured scanners,   and publishes the results to a separate Kafka topic. The results topic is consumed by the API server,   which is responsible for ingesting them into the database. Analysis makes heavy use of stream processing techniques   to improve performance. The full process is documented here.</li> <li>Repository metadata analysis. Similar to vulnerability analysis, but for component metadata such as latest   available versions, publish timestamps, and hashes. The process is documented here.</li> </ul> <p>The Kafka ecosystem is huge, and there exist many managed offerings for it. Over the recent years, many more projects and products were released that implement the Kafka protocol, giving users more choice. Operating Kafka has gotten easier, both due to the many new implementations, and because Kafka has dropped the previously mandatory ZooKeeper dependency.</p> <p>Message throughput is fantastic. Durability and ordering guarantees are great.</p>"},{"location":"architecture/decisions/001-drop-kafka-dependency/#issues-and-limitations","title":"Issues and limitations","text":"<ul> <li>Retries. Kafka does not yet support ACKs or NACKs of individual messages. It works with log offsets,   making it difficult to implement fine-grained retry mechanisms for individual failed messages,   without complex client-side solutions. Hyades implements retries using Kafka Streams state stores,   or by leveraging Confluent Parallel Consumer. With KIP-932, native ACKs of individual messages are on the horizon.</li> <li>Prioritization. Kafka is an append-only log, and as such does not support prioritization of messages.   Prioritization is required to ensure that actions triggered by users or clients take precedence over scheduled ones.   Implementing prioritization on top of Kafka primitives   is complex and inflexible. KIP-932 does not cover priorities. The lack of prioritization can partly be compensated   by ensuring high throughput of the system, which Kafka does support. But it's not a sustainable solution.</li> <li>Message sizes. Kafka has a default message size limit of <code>1 MiB</code>. We observed notifications and vulnerability   analysis results growing larger than <code>1 MiB</code>, even when compressed. The size limit can be increased on a per-topic basis,   but it comes with a performance penalty. We further found that some organizations disallow increasing size limits   entirely to limit impact on other teams sharing the same brokers.</li> <li>End-to-end observability. Tracking message flow and workflow progress across multiple topics and services   requires dedicated monitoring for logs, metrics, and traces. This raises the barrier to entry for operating   Dependency-Track clusters, and complicates debugging of issues and development. Relying solely on a PubSub broker like   Kafka or DT v4's internal message bus promotes choreography over orchestration. Choreography makes processes   increasingly hard to understand and follow. The initial workflow state tracking implementation attempted to lessen   the pain, but the logic being scattered across all choreography participants is not helpful.</li> <li>Spotty support for advanced Kafka features. Kafka comes with advanced features like transactions, compacted topics,   and more. We found that support for these is very spotty across alternative implementations (in particular transactions).   Further, we received feedback that organizations that operate shared Kafka clusters may prohibit usage of compacted   topics. With only bare-bones features left available, the argument for Kafka becomes a lot less compelling.</li> <li>Topic management. Partitions are what enables parallelism for Kafka consumers. The number of partitions must   be decided before topics are created. Increasing partitions later is possible, decreasing is not. Adding partitions   impacts ordering guarantees and can be tricky to coordinate. In order to leverage stream processing techniques,   some topics must be co-partitioned.    Generic tooling around topic management, comparable to database migration tooling, is severely lacking,   making it hard to maintain for a diverse user base. Vendor-specific tooling is available,   such as Strimzi's topic operator.</li> <li>Community support. Running an additional piece of infrastructure ourselves is one thing. Supporting a whole   community in doing that correctly and efficiently is another. Unfortunately there is no single deployment   or configuration that works for everyone. We don't have dedicated support staff and need to be pragmatic about   what we can realistically support. Requiring Kafka doesn't help.</li> </ul> <p>In summary, Kafka on its own provides not enough benefit for us to justify its usage.</p> <p>We hoped it would help in more areas, but ended up realizing that working around these issues required even more additional overhead and infrastructure to address. Which is not sustainable, given we already spent innovation tokens on Kafka itself, and have limited team capacities.</p>"},{"location":"architecture/decisions/001-drop-kafka-dependency/#possible-solutions","title":"Possible Solutions","text":""},{"location":"architecture/decisions/001-drop-kafka-dependency/#a-replace-kafka-for-another-message-broker","title":"A: Replace Kafka for another message broker","text":"<p>We could replace Kafka with another, more lightweight broker, like ActiveMQ, RabbitMQ, NATS, or Redis.</p> <p>ActiveMQ and RabbitMQ support AMQP and JMS as common messaging protocols. Managed offerings are widely available, both for these specific brokers, and alternative AMQP implementations.</p> <p>NATS is capable to cater to the widest range of use cases, but managed offerings are mainly limited to one vendor (Synadia). Realistically, users would need to maintain their own clusters. NATS JetStream can provide Kafka-like semantics, but also work queues, key-value and object stores. While its protocols are public and well-documented, there are currently no alternative server implementations. </p> <p>Redis provides data structures for classic queues (i.e. lists) and priority queues (i.e. sorted sets). It can act as publish-subscribe broker, although it only provides at-most-once delivery guarantees there.</p> <p>Pro:</p> <ol> <li>AMQP-compatible brokers come with support for retries and prioritization built-in.</li> <li>NATS could also be used as blob storage.</li> <li>Redis could also be used for caching.</li> </ol> <p>Con:</p> <ol> <li>Still requires an additional dependency.</li> <li>Still inherits many of the issues we have with Kafka (i.e. topic / queue management, e2e observability).</li> <li>We don't have expertise in configuring and operating any of these.</li> <li>Fully managed offerings are more scarce, especially NATS.</li> <li>Following a license change in 2024, the Redis ecosystem has become fragmented. Redis itself is no longer    permissively licensed. Forks like ValKey exist, but the whole situation is concerning.</li> </ol>"},{"location":"architecture/decisions/001-drop-kafka-dependency/#b-use-an-in-memory-data-grid","title":"B: Use an in-memory data grid","text":"<p>In-memory data grids (IMDGs) are a popular option for various use cases in the JVM ecosystem, including messaging. Prominent solutions in this space include Hazelcast, Ignite, and Infinispan.</p> <p>IMDGs could further be combined with frameworks such as Eclipse Vert.x, which use them for clustering.</p> <p>Pro:</p> <ol> <li>Could also be used for caching.</li> </ol> <p>Con:</p> <ol> <li>Most IMDGs still require a central server and are thus not necessarily simpler than a normal message broker.</li> <li>No managed offering in any major cloud(?).</li> <li>We only have very limited experience in configuring and operating any of these.</li> <li>Except Hazelcast, only very limited support for advanced data structures like priority queues.</li> <li>Upgrades are tricky to coordinate and require downtime. Rolling upgrades are a paid feature in Hazelcast.</li> </ol>"},{"location":"architecture/decisions/001-drop-kafka-dependency/#c-just-use-postgres","title":"C: Just use Postgres","text":"<p>We already decided to focus entirely on Postgres for our database. We dropped support for H2, MSSQL, and MySQL as a result. This decision opens up a lot more possibilities when it comes to other parts of the stack.</p> <p>Solutions like JobRunr, Hatchet, Oban, pgmq, River, and Solid Queue demonstrate that building queues or queue-like systems on a RDBMSes and Postgres specifically is viable.</p> <p>Running such workloads on a database does not necessarily mean that the database must be shared with the core application. It can be done for smaller deployments to keep complexity low, but larger deployments can simply leverage a separate database.</p> <p>Both architecture and operations are simpler, even if more database servers were to be involved.</p> <p>Database migrations are well understood, easy to test and to automate. With Liquibase, we already have great tooling in our stack.</p> <p>Organizations that are able to provision and support a Postgres database for the core application will also have an easier time to provision more instances if needed, versus having to procure another technology altogether.</p> <p>Postgres is also a lot more common than any of the message brokers or IMDGs.</p> <p>Performance-wise, messaging and queueing is not our main bottleneck. Since all asynchronous operations involve access to a database or external service anyway, raw message throughput is not a primary performance driver for us.</p> <p>Impact on database performance can be reduced by sizing units of work a little bigger. For example, processing all components of a project in a single task, rather than each component individually. Fewer writes and fewer transactions lead to more headroom.</p> <p>Pro:</p> <ol> <li>Drastically simplified tech stack. Easier to develop with and to support.</li> <li>We already have expertise in configuration and operation.</li> <li>Abundance of managed offerings across wide range of vendors.</li> <li>Retries, priorities, observability are easier to implement with a strongly consistent SQL database.</li> <li>Migrations are simpler, we already have tooling for it.</li> <li>More flexible: If we have special needs for queueing, we can just build it ourselves,    rather than adopting yet another technology, or implementing more workarounds.</li> </ol> <p>Con:</p> <ol> <li>Will not scale as far as Kafka or other dedicated brokers could.</li> <li>We're binding ourselves more to one specific technology.</li> </ol>"},{"location":"architecture/decisions/001-drop-kafka-dependency/#decision","title":"Decision","text":"<p>We propose to follow solution C. Go all-in on Postgres.</p> <p>TODO: Update with final decision.</p>"},{"location":"architecture/decisions/001-drop-kafka-dependency/#consequences","title":"Consequences","text":"<ul> <li>Functionalities that currently rely on Kafka will need to be re-architected for Postgres.</li> <li>Since we already have a few adopters of hyades, the transition will need to be gradual.</li> <li>We need a Kafka notification publisher to ensure that users relying on this functionality are not cut off.</li> </ul>"},{"location":"architecture/decisions/002-workflow-orchestration/","title":"ADR-002: Workflow Orchestration","text":"Status Date Author(s) Proposed 2025-01-16 @nscuro"},{"location":"architecture/decisions/002-workflow-orchestration/#context","title":"Context","text":"<p>By dropping the Kafka dependency (ADR-001), we are now missing a way to enqueue and distribute work for asynchronous execution.</p> <p>Dependency-Track v4 used an in-memory queue, which is neither reliable (queued messages are lost on restart), nor cluster-friendly (queue is not shared by multiple instances).</p> <p>As part of ADR-001, we explored options such as introducing a different, more lightweight message broker. Another alternative was the introduction of an in-memory data grid (IMDG). We decided against these options, and opted for leveraging the existing Postgres infrastructure instead.</p> <p>We also expressed having grown unhappy with the choreography-style architecture, as it complicates observability and is hard to grasp. A solution that is more akin to an orchestrator is desirable.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#requirements","title":"Requirements","text":"<p>To better understand our requirements, we explain them based on workflows we're dealing with today. One of the core workflows in Dependency-Track is processing uploaded BOMs:</p> <pre><code>---\ntitle: BOM Processing Workflow\n---\nflowchart LR\n    A@{ shape: circle, label: \"Start\" }\n    B[\"BOM Ingestion\"]\n    C@{ shape: fork, label: \"Fork\" }\n    D[\"Vulnerability Analysis\"]\n    E[\"Repository Metadata Analysis\"]\n    F@{ shape: fork, label: \"Join\" }\n    G[\"Policy Evaluation\"]\n    H[\"Metrics Update\"]\n    I@{ shape: dbl-circ, label: \"Stop\" }\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    C --&gt; E\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I</code></pre> <p>Workflow simplified for brevity. Omitted failure compensations and further fork-join patterns. In reality, Vulnerability Analysis would split into more steps, one for each enabled analyzer.</p> <p>Each step is either I/O intensive, or relies on external systems. It is not practical to execute all steps synchronously. Depending on the size of the BOM and system load, an execution of this workflow can take anywhere from a few milliseconds, to multiple minutes.</p> <p>However, BOM uploads are not the only way in which a project analysis may be triggered:</p> <ul> <li>All projects are re-analyzed on a recurring basis, at least daily.</li> <li>Users can manually request a re-analysis of specific projects.</li> </ul> <p>This means that project analysis should ideally be its own, reusable workflow:</p> <pre><code>---\ntitle: Project Analysis Workflow\n---\nflowchart LR\n    A@{ shape: circle, label: \"Start\" }\n    C@{ shape: fork, label: \"Fork\" }\n    D[\"Vulnerability Analysis\"]\n    E[\"Repository Metadata Analysis\"]\n    F@{ shape: fork, label: \"Join\" }\n    G[\"Policy Evaluation\"]\n    H[\"Metrics Update\"]\n    I@{ shape: dbl-circ, label: \"Stop\" }\n    A --&gt; C\n    C --&gt; D\n    C --&gt; E\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I</code></pre> <p>It could then be launched individually, or reused as follows:</p> <pre><code>---\ntitle: Goal BOM Processing Workflow\n---\nflowchart LR\n    A@{ shape: circle, label: \"Start\" }\n    B[\"BOM Ingestion\"]\n    C@{ shape: subproc, label: \"Project Analysis Workflow\" }\n    D@{ shape: dbl-circ, label: \"Stop\" }\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D</code></pre>"},{"location":"architecture/decisions/002-workflow-orchestration/#concurrency-control","title":"Concurrency control","text":"<p>To prevent race conditions and redundant work, an instance of the Project Analysis workflow for any given project should prevent other instances of Project Analysis for the same project from executing concurrently. This should be true even if the Project Analysis is part of another workflow, e.g. BOM Upload Processing. As such, workflow instances must be treated as dedicated units of work, rather than a simple chain of tasks.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#fairness","title":"Fairness","text":"<p>Since workflows can be triggered as part of a client interaction (i.e. BOM upload, explicit request), and on schedule, it is critical that the former take precedence over the latter. The system performing a scheduled analysis of all projects in the portfolio should not prevent clients from receiving feedback in a timely manner. There must be a mechanism to prioritize work.</p> <p>We expect to run multiple types of workflows on the system. Many instances of workflow <code>A</code> being scheduled for execution should not cause execution of instances of workflow <code>B</code> to be delayed. Work queues must be isolated.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#performance","title":"Performance","text":"<p>Throughput is more important than latency for Dependency-Track. This is particularly true as we aim to raise the ceiling of how many projects we can support in a portfolio.</p> <p>Due to tasks being I/O-heavy, speed of scheduling and queueing of tasks in itself is not anticipated to be a limiting factor of the system. If the execution of a single task takes longer than a second, it's acceptable for dequeueing of <code>N &gt;= 1</code> tasks to take up to a few hundreds of milliseconds. Given proper indexing, autovacuum configuration, and fast disks, Postgres should support dequeue queries well below 100ms, even for very large queues.</p> <p>Since we have at least one workflow that will be scheduled for each project in the portfolio, at least daily, the system must be able to handle large queue depths. Assuming a portfolio of 100k projects, a queue depth of at least 100k tasks must be supported with little to no performance degradation as the bare minimum.</p> <p>We chose to rely on Postgres. A relational, MVCC-based database. The workflow orchestration solution should avoid usage patterns that are suboptimal for Postgres (e.g. lots of small transactions), and make use of features that allow Postgres to operate more efficiently (e.g. batching, advisory locks,  <code>FOR UPDATE SKIP LOCKED</code>, partitioned tables).</p> <p>It is expected that the solution can optionally be pointed to a Postgres instance separate from the main application. This may be necessary for larger deployments, as more tasks are enqueued and more workers poll for tasks.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#resiliency","title":"Resiliency","text":"<p>We frequently interact with external systems which can experience temporary downtime, or enforce rate limiting, causing RPCs to fail or be rejected. In the majority of cases, retries are sufficient to recover from such failures. </p> <p>Individual workflow steps must be retryable. It should be possible to define a backoff strategy, as well as a limit after which no more retries will be attempted. A workflow step being retried should not block the execution of steps belonging to other workflow instances.</p> <p>In case of node failures, it is expected that uncompleted work is not lost, and will be picked up by the same node upon restart, or other nodes in the cluster.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#scheduling","title":"Scheduling","text":"<p>We have many tasks that need to run repeatedly on a schedule. The currently implemented solution is based on a combination of in-memory timers and database-backed locks to prevent duplicate executions.</p> <p>The orchestration system should allow us to schedule recurring workflow executions, e.g. using cron expressions.</p> <p>Ideally, the system would allow for schedules to be adjusted at runtime, instead of requiring a restart.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#observability","title":"Observability","text":"<p>Based on interactions with the community, it is clear that not everyone is able or willing to operate infrastructure for centralized log monitoring and alerting. Further, it is very common for users, even administrative ones, to not have direct access to application logs.</p> <p>We can thus not rely on logging and metrics instrumentation to ensure observability. We need a solution that has observability built in, and allows us to expose it on the application layer.</p> <p>In extension, records of workflow instances should be retained for a specified amount of time. This allows retrospective investigation of failures. The retention duration should be configurable.</p> <p>The intention is to make it easier for users without log access to see what's going on. The intention is not to replace logs and metrics. Both remain crucial for operators of Dependency-Track and must continue to be part of our implementation.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#possible-solutions","title":"Possible solutions","text":""},{"location":"architecture/decisions/002-workflow-orchestration/#a-adopt-a-job-scheduling-framework","title":"A: Adopt a job scheduling framework","text":"<p>Multiple open source frameworks for job scheduling using relational databases exist. Among them db-scheduler, JobRunr and Quartz. While they focus on execution of individual jobs, they usually ship with basic workflow capabilities as well.</p> <p>Quartz jobs can be chained to form workflows, using <code>JobChainingJobListener</code>. This functionality is self-described as \"poor man's workflow\".</p> <p>JobRunr has a similar concept,  but it's only available in the commercial Pro version. As an open source project, we cannot use commercial tooling.</p> <p>db-scheduler appears to be the most capable option, while being entirely free without commercial offering. It, too supports job chaining to form workflows of multiple steps.</p> <p>Job chaining is an implementation of the routing slip pattern. This model is akin to what Dependency-Track v4 relied on, and we found it to be both unreliable and limiting at times. Since there is no orchestration layer in this setup, failure compensations are harder to implement.</p> <p>Fork-join / scatter-gather patterns are not supported by this approach. We need this capability to run certain workflow steps concurrently, e.g. Vulnerability Analysis and Repository Metadata Analysis. Additional state-keeping and coordination is necessary to achieve the desired behavior with job chaining.</p> <p>Also, none of these options allow job chains to be treated as logical units of work, which complicates concurrency control, fairness, and monitoring.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#b-adopt-an-embedded-workflow-engine","title":"B: Adopt an embedded workflow engine","text":"<p>Per the awesome-workflow-engines list, available Java-based embedded workflow engines that can cater to our requirements include COPPER and nFlow.</p> <p>At the time of writing, the COPPER repository has not seen any activity for over 6 months. The documentation is scarce, but COPPER appears to be achieving durability by serializing stack frames of Java code, which leads to many restrictions when it comes to modifying workflows.</p> <p>nFlow requires Spring Framework which we do not use, and we don't plan on changing this.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#c-adopt-an-external-workflow-engine","title":"C: Adopt an external workflow engine","text":"<p>Out of all external engines we encountered, Temporal is a perfect match. It supports everything we need, allows us to write workflows as code, and is open source. The same is true for Cadence, the spiritual predecessor of Temporal.</p> <p>But systems like the above are built so scale, and scale in such a way that can support entire enterprises in running their workflows on shared infrastructure. This is great, but comes with a lot of additional complexity that we can't justify to introduce.  We just need something that supports our own use cases, without additional operational burden.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#d-build-our-own","title":"D: Build our own","text":"<p>Temporal is a spiritual successor to Microsoft's Durable Task Framework (DTFx). DTFx is not an external service, but an embeddable library. Unfortunately, DTFx is written in C#, so we can not use it.</p> <p>Microsoft has further invested into a Go port of DTFx: durabletask-go, which can be used as a Go library, but also as an external service (i.e. sidecar). It powers Dapr Workflow in this configuration.  durabletask-go can be used with various storage backends, among them Postgres. A Java SDK for the  durabletask-go sidecar is available with durabletask-java. We don't want to require an external service though, even if it's just a sidecar.</p> <p>There are also some features that the DTFx implementations do not offer, for example scheduling, prioritization, and concurrency control. But the foundation for everything else we want is there. And judging by durabletask-go's code, implementing a DTFx-like engine is doable without a lot of complexity. Various patterns enabled by the DTFx model can be seen in the Dapr Workflow Patterns documentation.</p> <p>In short, the plan for this solution is as follows:</p> <ul> <li>Build a Java-based, embeddable port of DTFx, roughly based on the logic of durabletask-go.</li> <li>Omit abstractions to support multiple storage backends, focus entirely on Postgres instead.</li> <li>Omit features that we do not need to keep the porting effort low.</li> <li>Build features we need on top of this foundation.</li> </ul> <p>This approach allows us to fulfill our current requirements, but also adapt more quickly to new ones in the future. We completely avoid introduction of additional infrastructure or services, keeping operational complexity low. The solution is also not entirely bespoke, since we can lean on design decisions of the mature and proven DTFx ecosystem.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#decision","title":"Decision","text":"<p>We will follow option D.</p>"},{"location":"architecture/decisions/002-workflow-orchestration/#consequences","title":"Consequences","text":"<p>A DTFx port as outlined in D: Build our own needs to be developed.</p> <p>This work was already started to evaluate feasibility of this approach. The current state of the code is available here:</p> <ul> <li><code>DependencyTrack/hyades-apiserver@workflow-v2</code></li> <li><code>DependencyTrack/hyades-frontend@workflow-v2</code></li> </ul> <p>A corresponding design document that describes our implementation will follow.</p>"},{"location":"architecture/decisions/003-notification-publishing/","title":"ADR-003: Notification Publishing","text":"Status Date Author(s) Proposed 2025-01-19 @nscuro"},{"location":"architecture/decisions/003-notification-publishing/#context","title":"Context","text":"<p>By dropping the Kafka dependency (ADR-001), we are now missing a means to reliably dispatch notifications. Users are building processes around the notifications we send, so we must ensure that whatever we replace Kafka with offers the same or better delivery guarantees.</p>"},{"location":"architecture/decisions/003-notification-publishing/#background","title":"Background","text":"<p>Users can configure multiple notification rules (known as Alerts in the UI). A rule semantically acts like a consumer, which subscribes to one or more subjects (aka Notification Groups, e.g. <code>BOM_PROCESSED</code>, <code>NEW_VULNERABILITY</code>), and publishes those notification to a destination (e.g. email, Slack, Webhook).</p> <p>Rules can further be limited to specific projects or tags, which acts like an additional filter.</p> <p>This means that a notification emitted by Dependency-Track \"fans-out\" to zero or more rules:</p> <pre><code>---\ntitle: Notification Publishing Process\n---\nflowchart LR\n    A@{ shape: circle, label: \"Start\" }\n    B[\"Emit Notification\"]\n    C[\"Route Notification\"]\n    D[\"Send email to foo\\@example.com\"]\n    E[\"Send email to bar\\@example.com\"]\n    F[\"Send Slack message to channel X\"]\n    G[\"Send Webhook to example.com\"]\n    I@{ shape: dbl-circ, label: \"Stop\" }\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    C --&gt; E\n    C --&gt; F\n    C --&gt; G\n    D --&gt; I\n    E --&gt; I\n    F --&gt; I\n    G --&gt; I</code></pre> <p>Because each rule has a separate destination, the publishing process for them can fail independently. For example, the email server could be down, the Jira credentials could have expired, or Slack could enforce rate limiting.</p> <p>In Dependency-Track v4, the routing of notifications according to the configured rules, as well as the publishing according to those rules, are performed as a single unit of work. There are no retries.</p> <p>So far in Hyades, we emit notifications to Kafka instead. The routing and publishing still happens in a single unit of work, but in the separate <code>notification-publisher</code> service. The service uses Confluent Parallel Consumer to implement retries. However, these retries can cause duplicates, as documented in hyades/#771.</p>"},{"location":"architecture/decisions/003-notification-publishing/#requirements","title":"Requirements","text":""},{"location":"architecture/decisions/003-notification-publishing/#atomicity","title":"Atomicity","text":"<p>A limitation of the in-memory, as well as the Kafka-based notification mechanism, is that emission of notifications can't happen atomically with the state change they inform about. Both approaches suffer from the dual write problem. Although not technically a hard requirement, we want atomic notification emission to be possible.</p> <p>Note that notification delivery to external systems can't be atomic. Exactly-once delivery is impossible.</p>"},{"location":"architecture/decisions/003-notification-publishing/#at-least-once-delivery","title":"At-least-once delivery","text":"<p>Notifications must be delivered to destination systems at least once. Duplicate deliveries are acceptable, loss of notifications is not.</p>"},{"location":"architecture/decisions/003-notification-publishing/#isolation-of-deliveries","title":"Isolation of deliveries","text":"<p>Notification deliveries must be processed in isolation, and separate from the routing itself. Only then can deliveries be retried without causing the issue described in hyades/#771. More specifically, each delivery must be a separate unit of work in a queue.</p>"},{"location":"architecture/decisions/003-notification-publishing/#constraints","title":"Constraints","text":""},{"location":"architecture/decisions/003-notification-publishing/#notification-format","title":"Notification format","text":"<p>All solutions should treat notification contents as binary blobs. During our initial move to Kafka, we adopted Protobuf to serialize notifications over-the-wire. We intend to keep it this way, since Protobuf makes it easy to perform backward-compatible changes over time. It is also faster to serialize, and more compact than JSON.</p> <p>The current Protobuf model is defined here.</p>"},{"location":"architecture/decisions/003-notification-publishing/#no-new-infrastructure","title":"No new infrastructure","text":"<p>No new infrastructure must be introduced. The solution should leverage the workflow orchestration capabilities described in ADR-002. This will take care of queueing, observability, and resiliency needs.</p>"},{"location":"architecture/decisions/003-notification-publishing/#payload-size","title":"Payload size","text":"<p>As noted in ADR-001, notifications can be large: <code>BOM_CONSUMED</code> and <code>BOM_PROCESSED</code> notifications include the entire BOM in Base64-encoded format. <code>PROJECT_VULN_ANALYSIS_COMPLETE</code> notifications include all vulnerabilities of a project. While Postgres can store large columns (see TOAST), it comes with penalties affecting performance, maintenance, and backups. Potent compression (e.g. using <code>zstd</code>) will most definitely be necessary for all solutions. Postgres itself will compress large values, too, but using a less effective algorithm (<code>pglz</code>).</p> <p>Starting with Postgres v14, the default compression algorithm for TOAST-ed tables can be changed to <code>lz4</code>. This should be added to our database operations guide. <code>lz4</code> performs noticeably better than <code>pglz</code>.</p>"},{"location":"architecture/decisions/003-notification-publishing/#possible-solutions","title":"Possible Solutions","text":""},{"location":"architecture/decisions/003-notification-publishing/#a-use-transactional-outbox-pattern","title":"A: Use transactional outbox pattern","text":"<p>The transactional outbox pattern involves a separate outbox table, where to-be-dispatched notifications are inserted into as part of a database transaction. A simple outbox table might look like this:</p> Column Name Column Type <code>ID</code> <code>BIGINT</code> <code>TIMESTAMP</code> <code>TIMESTAMPTZ</code> <code>CONTENT</code> <code>BYTEA</code> <p>The pattern is mostly meant to deal with the dual write problem, but it could also act as a work queue: A pool of workers polls the table in regular intervals, and either deletes polled records, or marks them as delivered:</p> <pre><code>---\ntitle: Notification Publishing Process with Outbox Table\n---\nsequenceDiagram\n    participant D as Domain\n    participant P as Postgres\n    participant N as Notification Router\n    participant W as \"Publish Notification\"&lt;br/&gt;Workflow\n\n    activate D\n    D-&gt;&gt;P: Begin TX\n    D-&gt;&gt;P: ...\n    D-&gt;&gt;P: INSERT INTO \"OUTBOX\"&lt;br/&gt;(NOW(), &lt;content&gt;)\n    D-&gt;&gt;-P: Commit TX\n    loop continuously\n       activate N\n       N-&gt;&gt;P: Begin TX\n       N-&gt;&gt;P: SELECT * FROM \"OUTBOX\" ...&lt;br/&gt;FOR UPDATE SKIP LOCKED&lt;br/&gt;LIMIT 1\n       N-&gt;&gt;N: Evaluate&lt;br/&gt;notification rules\n       opt At least one rule matched\n           N-&gt;&gt;W: Schedule \"Publish Notification\" workflow&lt;br/&gt;Args: Rule names, notification\n       end \n       N-&gt;&gt;P: DELETE FROM \"OUTBOX\"&lt;br/&gt;WHERE \"ID\" = ANY(...)\n       N-&gt;&gt;-P: Commit TX\n    end</code></pre> <p>Outbox items are dequeued, processed, and marked as completed in a single database transaction, using the <code>FOR UPDATE SKIP LOCKED</code> clause to allow for multiple concurrent pollers.</p> <p>If workflow scheduling fails, the transaction is rolled back, and the respective record will be retried during the next poll.</p> <p>The workflow engine may reside on a separate database, so scheduling of workflows  can't happen atomically with the polling of outbox records. It is possible that a workflow gets scheduled, but committing of the transaction fails. In worst case, multiple workflows get scheduled for the same notification. This should be rare, but it would still satisfy our at-least-once delivery goal.</p> <p>The actual delivery is then taken care of by a workflow:</p> <pre><code>---\ntitle: Notification Publishing Workflow\n---\nsequenceDiagram\n    participant W as \"Publish Notification\"&lt;br/&gt;Workflow\n    participant A as \"Publish Notification\"&lt;br/&gt;Activity\n    participant P as Postgres\n    participant D as Destination\n\n    activate W\n    loop for each matched rule\n        W-&gt;&gt;+A: Invoke&lt;br/&gt;Args: Rule name, notification\n        deactivate W\n        A-&gt;&gt;P: Retrieve publisher config\n        P--&gt;&gt;A: Destination URL,&lt;br/&gt;credentials, template\n        A-&gt;&gt;-D: Publish\n    end</code></pre> <p>Pro:</p> <ol> <li>Allows for atomic emission of notifications.</li> <li>When marking outbox records as processed instead of deleting them, enables (targeted) replay of past notifications.</li> <li>When partitioning the outbox table by timestamp, cheap retention enforcement via <code>DROP TABLE</code>.</li> <li>Allows for multiple concurrent routers. </li> </ol> <p>Con:</p> <ol> <li>More database overhead: <code>INSERT</code>s, <code>UPDATE</code>s / <code>DELETE</code>s, polling, vacuuming, retention, storage.</li> <li>Overhead can't be delegated to separate database without losing transactional guarantees.</li> <li>Duplicates queueing logic we already have in the workflow orchestration system.</li> <li>Partitioning by timestamp requires partition management, either manually or via <code>pg_partman</code>.</li> <li>Multiple concurrent routers increase the chance of delivering notifications out-of-order.</li> <li>When not using partitioning, requires a separate retention enforcement mechanism.</li> </ol>"},{"location":"architecture/decisions/003-notification-publishing/#b-use-postgres-logical-replication-messages","title":"B: Use Postgres logical replication messages","text":"<p>A way to sidestep the drawbacks of maintaining an outbox table is to emit and consume logical replication messages. Here, notifications are written to Postgres' write-ahead-log (WAL), but never materialized into an actual table. This still provides transactional guarantees, but completely avoids the overhead of table maintenance.</p> <p>The procedure is inspired by Gunnar Morling's  The Wonders of Postgres Logical Decoding Messages article.</p> <p>Conceptually, the process of publishing notification remains mostly identical to option A:</p> <pre><code>---\ntitle: Notification Publishing Process with Logical Replication Messages\n---\nsequenceDiagram\n    participant D as Domain\n    participant P as Postgres\n    participant N as Notification Router\n    participant W as \"Publish Notification\"&lt;br/&gt;Workflow\n\n    activate D\n    D-&gt;&gt;P: Begin TX\n    D-&gt;&gt;P: ...\n    D-&gt;&gt;P: pg_logical_emit_message&lt;br/&gt;(?, 'notification', &lt;content&gt;)\n    D-&gt;&gt;-P: Commit TX\n    loop continuously\n        activate N\n        N-&gt;&gt;P: Consume logical&lt;br/&gt;replication message\n        N-&gt;&gt;N: Evaluate&lt;br/&gt;notification rules\n        opt At least one rule matched\n            N-&gt;&gt;W: Schedule \"Publish Notification\" workflow&lt;br/&gt;Args: Rule names, notification\n        end\n        N-&gt;&gt;-P: Mark message LSN as&lt;br/&gt;applied / flushed\n    end</code></pre> <p>Instead of polling a table, the router reads a stream of messages from a logical replication slot. Each message has an associated log sequence number (LSN), which represents its position in the WAL. In order for Postgres to know that a message was delivered successfully, the router acknowledges the LSN of processed messages. This is similar to how Kafka uses offset commits to track progress within a topic partition.</p> <p>The Publish Notification workflow being scheduled remains identical to option A.</p> <p>Pro:</p> <ol> <li>Allows for atomic emission of notifications.</li> <li>Less pressure on the WAL. Option A involves at least one <code>INSERT</code>,    and one <code>UPDATE</code> or <code>DELETE</code> per notification, each of which writes to the WAL, too.</li> <li>No increase in storage requirements.</li> <li>No retention logic necessary.</li> <li>No additional overhead for autovacuum.</li> </ol> <p>Con:</p> <ol> <li>Only a single instance can consume from a replication slot at a time.</li> <li>Logical replication requires a special kind of connection, thus can't go through a connection pooler.</li> <li>Requires Postgres v14 or later. This is when the default <code>pgoutput</code> decoding plugin started to support    consumption of replication messages.</li> </ol>"},{"location":"architecture/decisions/003-notification-publishing/#c-schedule-workflows-directly","title":"C: Schedule workflows directly","text":"<p>This option is similar to A and B, but skips the respective intermediary step.</p> <p>Notifications are no longer emitted atomically with the domain's persistence operations,  but instead after the database transaction committed successfully, effectively re-introducing the dual write problem:</p> <pre><code>sequenceDiagram\n    participant D as Domain\n    participant P as Postgres\n    participant W as \"Publish Notification\"&lt;br/&gt;Workflow\n\n    activate D\n    D-&gt;&gt;P: Begin TX\n    D-&gt;&gt;P: ...\n    D-&gt;&gt;P: Commit TX\n    D-&gt;&gt;-W: Schedule&lt;br/&gt;Args: Notification</code></pre> <p>The routing based on configured notification rules is performed as part of the Publish Notification workflow.</p> <pre><code>---\ntitle: Notification Publishing Workflow with Routing\n---\nsequenceDiagram\n   participant W as \"Publish Notification\"&lt;br/&gt;Workflow\n   participant A1 as \"Route Notification\"&lt;br/&gt;Activity\n   participant A2 as \"Publish Notification\"&lt;br/&gt;Activity\n   participant P as Postgres\n   participant D as Destination\n\n   activate W\n   W-&gt;&gt;A1: Invoke&lt;br/&gt;Args: Notification\n   A1--&gt;&gt;W: Matched rule names\n   loop for each matched rule\n      W-&gt;&gt;+A2: Invoke&lt;br/&gt;Args: Rule name, notification\n      deactivate W\n      A2-&gt;&gt;P: Retrieve publisher config\n      P--&gt;&gt;A2: Destination URL,&lt;br/&gt;credentials, template\n      A2-&gt;&gt;-D: Publish\n   end</code></pre> <p>Pro:</p> <ol> <li>Fewer moving parts than options A and    B.</li> <li>All concerns related to publishing are neatly encapsulated in a workflow.</li> <li>Rule resolution benefits from the same reliability guarantees as the publishing itself.</li> </ol> <p>Con:</p> <ol> <li>Atomic emission of notifications is impossible.</li> <li>Workflows are scheduled even if no rule may be configured for the notification at hand,    increasing pressure on the workflow system.</li> </ol>"},{"location":"architecture/decisions/003-notification-publishing/#decision","title":"Decision","text":"<p>We propose to follow option B, because:</p> <ol> <li>Maintaining an outbox table as detailed in option A comes with a lot of overhead that we prefer to not deal with.</li> <li>Option C does not provide transactional guarantees.</li> </ol> <p>TODO: Update with final decision.</p>"},{"location":"architecture/decisions/003-notification-publishing/#consequences","title":"Consequences","text":"<ul> <li>The minimum required Postgres version becomes 14. This version is over three years old and well-supported   across all managed offerings. We don't anticipate it to be a problem.</li> <li>It must be possible to configure separate database connection details for the notification router,   in case a pooler like PgBouncer is used. Logical replication requires a direct connection.</li> </ul>"},{"location":"architecture/decisions/004-file-storage-plugin/","title":"ADR-004: File Storage Plugin","text":"Status Date Author(s) Accepted 2025-01-29 @nscuro"},{"location":"architecture/decisions/004-file-storage-plugin/#context","title":"Context","text":"<p>In ADR-001, we surfaced the complication of transmitting large payloads (BOMs, notifications, analysis results).</p> <p>Neither message brokers, nor RDBMSes are meant to store large blobs of data. The introduction of a Postgres-based workflow orchestration solution (see ADR-002) does not change this reality.</p> <p>To ensure our core systems stay performant, we should fall back to storing files externally, and instead only pass references to those files around. This strategy is followed by messaging services such as AWS SNS, which offloads payloads to S3 if they exceed a size of <code>256KB</code>.</p> <p>We do not need fully-fledged filesystem capabilities. Pragmatically speaking, all we need is a glorified key-value store. An obvious choice would be to delegate this to an object store such as S3. However, we recognize that not all users are able or willing to deploy additional infrastructure.</p> <p>Thus, at minimum, the following storage solutions must be supported:</p> <ol> <li>Local filesystem. This option is viable for users running single-node clusters, or those with    access to reasonably fast network storage. This should be the default, as it does not require any    additional setup.</li> <li>S3-compatible object storage. This option is viable for users running multi-node clusters,    operating in the cloud, and / or without access to network storage. It could also be required    to support very large clusters with increased storage requirements.</li> </ol> <p>Optionally, the following solutions may be offered as well:</p> <ul> <li>In-memory. For unit tests, integration tests, and single-node demo clusters.</li> <li>Non-S3-compatible object storage. Like Azure Blob Storage and similar proprietary offerings.</li> </ul> <p>To reduce networking and storage costs, as well as network latencies, files should be compressed prior to sending them over the wire.</p> <p>When retrieving files from storage, providers should verify their integrity,  to prevent processing of files that have been tampered with.</p>"},{"location":"architecture/decisions/004-file-storage-plugin/#decision","title":"Decision","text":"<p>To communicate file references, we will leverage metadata objects. Metadata objects will hold a unique reference to a file, within the context of a given storage provider. To enable use cases such as encryption and integrity verification, we allow providers to attach additional metadata.</p> <p>Since our primary means of internal communication is based on Protobuf, we will define the file metadata in this format. It will allow us to easily attach it to other Protobuf messages.</p> <pre><code>syntax = \"proto3\";\n\npackage org.dependencytrack.storage.v1alpha1;\n\n// Metadata of a stored file.\nmessage FileMetadata {\n  // Location of the file in URI format.\n  // The URI's scheme is the name of the storage provider.\n  // Examples: \"memory:///foo/bar\", \"s3://bucket/foo/bar\".\n  string location = 1;\n\n  // Media type of the file.\n  // https://www.iana.org/assignments/media-types/media-types.xhtml\n  string media_type = 2;\n\n  // Hex-encoded SHA-256 digest of the file content.\n  string sha256_digest = 3;\n\n  // Additional metadata of the storage provider,\n  // i.e. values used for integrity verification.\n  map&lt;string, string&gt; additional_metadata = 100;\n}\n</code></pre> <p>The API surface will evolve around the <code>FileStorage</code> interface, which exposes methods to store, retrieve, and delete files:</p> <pre><code>package org.dependencytrack.storage;\n\nimport org.dependencytrack.plugin.api.ExtensionPoint;\nimport org.dependencytrack.proto.storage.v1alpha1.FileMetadata;\n\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.util.Collection;\n\npublic interface FileStorage extends ExtensionPoint {\n\n    /**\n     * Persist data to a file in storage.\n     * &lt;p&gt;\n     * Storage providers may transparently perform additional steps,\n     * such as encryption and compression.\n     *\n     * @param fileName  Name of the file. This fileName is not guaranteed to be reflected\n     *                  in storage as-is. It may be modified or changed entirely.\n     * @param mediaType Media type of the file.\n     * @param content   Data to store.\n     * @return Metadata of the stored file.\n     * @throws IOException When storing the file failed.\n     * @see &lt;a href=\"https://www.iana.org/assignments/media-types/media-types.xhtml\"&gt;IANA Media Types&lt;/a&gt;\n     */\n    FileMetadata store(final String fileName, final String mediaType, final byte[] content) throws IOException;\n\n    /**\n     * Persist data to a file in storage, assuming the media type to be {@code application/octet-stream}.\n     *\n     * @see #store(String, String, byte[])\n     */\n    default FileMetadata store(final String fileName, final byte[] content) throws IOException {\n        return store(fileName, \"application/octet-stream\", content);\n    }\n\n    /**\n     * Retrieves a file from storage.\n     * &lt;p&gt;\n     * Storage providers may transparently perform additional steps,\n     * such as integrity verification, decryption and decompression.\n     * &lt;p&gt;\n     * Trying to retrieve a file from a different storage provider\n     * is an illegal operation and yields an exception.\n     *\n     * @param fileMetadata Metadata of the file to retrieve.\n     * @return The file's content.\n     * @throws IOException           When retrieving the file failed.\n     * @throws FileNotFoundException When the requested file was not found.\n     */\n    byte[] get(FileMetadata fileMetadata) throws IOException;\n\n    /**\n     * Deletes a file from storage.\n     * &lt;p&gt;\n     * Trying to delete a file from a different storage provider\n     * is an illegal operation and yields an exception.\n     *\n     * @param fileMetadata Metadata of the file to delete.\n     * @return {@code true} when the file was deleted, otherwise {@code false}.\n     * @throws IOException When deleting the file failed.\n     */\n    boolean delete(FileMetadata fileMetadata) throws IOException;\n\n}\n</code></pre> <p>To support multiple, configurable providers, we will leverage the plugin mechanism introduced in hyades-apiserver/#805. The mechanism, once its API is published as separate Maven artifact, will allow users to develop their own storage providers if required, without requiring changes to the Dependency-Track codebase.</p> <p>This allows storage providers to be configured as follows:</p> <pre><code># Defines the file storage extension to use.\n# When not set, an enabled extension will be chosen based on its priority.\n# It is recommended to explicitly configure an extension for predictable behavior.\n#\n# @category:     Storage\n# @type:         enum\n# @valid-values: [local, memory, s3]\nfile.storage.default.extension=\n\n# Whether the local file storage extension shall be enabled.\n#\n# @category: Storage\n# @type:     boolean\nfile.storage.extension.local.enabled=true\n\n# Defines the local directory where files shall be stored.\n# Has no effect unless file.storage.extension.local.enabled is `true`.\n#\n# @category: Storage\n# @default:  ${alpine.data.directory}/storage\n# @type:     string\nfile.storage.extension.local.directory=\n\n# Whether the in-memory file storage extension shall be enabled.\n#\n# @category: Storage\n# @type:     boolean\nfile.storage.extension.memory.enabled=false\n\n# Whether the s3 file storage extension shall be enabled.\n#\n# @category: Storage\n# @type:     boolean\nfile.storage.extension.s3.enabled=false\n\n# Defines the S3 endpoint URL.\n#\n# @category: Storage\n# @type:     string\nfile.storage.extension.s3.endpoint=\n\n# Defines the name of the S3 bucket.\n#\n# @category: Storage\n# @type:     string\nfile.storage.extension.s3.bucket=\n\n# Defines the S3 access key / username.\n#\n# @category: Storage\n# @type:     string\nfile.storage.extension.s3.access.key=\n\n# Defines the S3 secret key / password.\n#\n# @category: Storage\n# @type:     string\nfile.storage.extension.s3.secret.key=\n\n# Defines the region of the S3 bucket.\n#\n# @category: Storage\n# @type:     string\nfile.storage.extension.s3.region=\n</code></pre> <p>Application code can interact with <code>FileStorage</code> via the <code>PluginManager</code>:</p> <pre><code>package org.dependencytrack.foobar;\n\nimport org.dependencytrack.plugin.PluginManager;\nimport org.dependencytrack.proto.storage.v1alpha1.FileMetadata;\nimport org.dependencytrack.storage.FileStorage;\n\nclass Foo {\n\n    void bar() {\n        try (var fileStorage = PluginManager.getInstance().getExtension(FileStorage.class)) {\n            FileMetadata fileMetadata = fileStorage.store(\"filename\", \"content\".getBytes());\n\n            byte[] fileContent = fileStorage.get(fileMetadata);\n\n            fileStorage.delete(fileMetadata);\n        }\n    }\n\n}\n</code></pre>"},{"location":"architecture/decisions/004-file-storage-plugin/#consequences","title":"Consequences","text":"<ul> <li>There is a non-zero chance of orphaned files remaining in storage. Crashes or service outages on either end   can prevent Dependency-Track from deleting files if they're no longer needed. Some storage providers such as   AWS S3 allow retention policies to be configured. This is not true for local file storage, however.   As a consequence, storage providers should make an effort to make the creation timestamp of files obvious,   i.e. as part of the file's name, if relying on the file system's metadata is not possible.</li> <li>Storage operations are not atomic with database operations. This is an acceptable tradeoff,   because it does not impact the integrity of the system. Application code is expected to gracefully   deal with missing files, and perform compensating actions accordingly. Since file storage is not the   primary system of record, files existing without the application knowing about them is not an issue.</li> </ul>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/","title":"ADR-005: Materialize Project Hierarchies","text":"Status Date Author(s) Accepted 2025-03-08 @nscuro"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#context","title":"Context","text":""},{"location":"architecture/decisions/005-materialize-project-hierarchies/#current-model","title":"Current model","text":"<p>Heavily simplified data model of projects and how they relate to other functionalities of the system:</p> <pre><code>erDiagram\n    NOTIFICATION_RULE {\n        bigint ID pk\n        text NAME\n    }\n\n    POLICY {\n        bigint ID pk\n        text NAME\n    }\n\n    PROJECT {\n        bigint ID pk\n        bigint PARENT_PROJECT_ID fk\n        text NAME\n    }\n\n    TEAM {\n        bigint ID pk\n        text NAME\n    }\n\n    PROJECT o|--o{ PROJECT: \"is parent of\"\n    PROJECT }o--o{ TEAM: \"is accessible by\"\n    NOTIFICATION_RULE }o--o{ PROJECT: \"is limited to\"\n    POLICY }o--o{ PROJECT: \"is limited to\"</code></pre> <ul> <li>Projects can form hierarchies by referencing their respective parent.</li> <li>Portfolio ACL requires teams to be given explicit access to specific projects. For project hierarchies, child projects are expected to be implicitly accessible, if access was granted to a respective parent project. </li> <li>Notification rules can be limited to specific projects, such that they only trigger for a subset of projects in a portfolio. Rules can further decide to include children of projects associated with them.</li> <li>Policies can be limited to apply to only a subset of projects, similar to notification rules.</li> </ul>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#access-patterns","title":"Access patterns","text":"<p>With this model, evaluation of project hierarchies necessitates a recursive CTE. For example, the following query retrieves the IDs of all parents of a project with ID <code>projectId</code>:</p> <pre><code>with recursive project_hierarchy(id, parent_id) as(\n  select \"ID\" as id\n       , \"PARENT_PROJECT_ID\" as parent_id\n    from \"PROJECT\"\n   where \"ID\" = :projectId\n   union all\n  select \"PROJECT\".\"ID\" as id\n       , \"PROJECT\".\"PARENT_PROJECT_ID\" as parent_id\n    from \"PROJECT\"\n   inner join project_hierarchy\n      on project_hierarchy.parent_id = \"PROJECT\".\"ID\"\n)\nselect id\n  from project_hierarchy\n</code></pre> <p>To support portfolio ACL inheritance, we consider a project to be accessible by a team, if either the project itself is, or any of its parents are accessible. Given the SQL query above, an inheritance-aware ACL check can be expressed like this:</p> <pre><code>with recursive project_hierarchy(id, parent_id) as(/* See above */)\nselect exists(\n  select 1\n    from project_hierarchy\n   inner join \"PROJECT_ACCESS_TEAMS\"\n      on \"PROJECT_ACCESS_TEAMS\".\"PROJECT_ID\" = project_hierarchy.id\n   where \"PROJECT_ACCESS_TEAMS\".\"TEAM_ID\" = :teamId\n)\n</code></pre> <p>For historical reasons, we also have various cases of hierarchy checks in Java code. This is horribly inefficient of course, because it requires loading of all projects forming the hierarchy into memory.  </p> <p>An example is <code>ProjectQueryManager#isChildOf(Project, UUID)</code>:</p> <pre><code>private static boolean isChildOf(Project project, UUID uuid) {\n    boolean isChild = false;\n    if (project.getParent() != null) {\n        if (project.getParent().getUuid().equals(uuid)) {\n            return true;\n        } else {\n            isChild = isChildOf(project.getParent(), uuid);\n        }\n    }\n    return isChild;\n}\n</code></pre>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#challenges","title":"Challenges","text":"<p>While this \"works\", it still poses challenges:</p> <ol> <li>The complete hierarchy must be constructed before ACL checks can be performed.</li> <li>Constructing project hierarchies ad-hoc is expensive, especially for hierarchies spanning multiple levels.</li> <li>Expensiveness is amplified for list operations, i.e. \"list all projects a user has access to\".</li> <li>Hierarchy checks are common in the \"hot path\", e.g. during notification routing and policy evaluation.</li> </ol>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#decision","title":"Decision","text":"<p>Introduce a new table <code>PROJECT_HIERARCHY</code>. The table is responsible for storing \"flattened\" parent-child relationships:</p> <pre><code>erDiagram\n    PROJECT_HIERARCHY {\n        bigint PARENT_PROJECT_ID pk, fk\n        bigint CHILD_PROJECT_ID pk, fk\n        smallint DEPTH\n    }</code></pre> <p>\"Flattened\" in this case means that instead of only direct relationships, it will also contain transitive relationships. The depth always denotes the distance between the respective parent and child.</p> <p>For the hierarchy <code>A -&gt; B -&gt; C</code>, we will store:</p> <ul> <li><code>A -&gt; B</code> (depth <code>1</code>)</li> <li><code>B -&gt; C</code> (depth <code>1</code>)</li> <li><code>A -&gt; C</code> (depth <code>2</code>)</li> </ul> <p>This allows for fast lookups no matter how deep the hierarchy is: A check for whether <code>C</code> is a child of <code>A</code> is as efficient as a check for whether <code>C</code> is a child of <code>B</code> and vice versa.</p> <p>We will additionally use self-referential records for each project in the form of:</p> <ul> <li><code>A -&gt; A</code> (depth <code>0</code>)</li> <li><code>B -&gt; B</code> (depth <code>0</code>)</li> <li><code>C -&gt; C</code> (depth <code>0</code>)</li> </ul> <p>This ensures that projects are present in the table, even if they're not explicitly part of a hierarchy. Self-referential records will always have the depth <code>0</code>, making them easy to filter out if needed.</p>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#maintenance","title":"Maintenance","text":"<p>The <code>PROJECT_HIERARCHY</code> table being separate from the <code>PROJECT</code> table, additional maintenance work is required in order to keep it up-to-date as projects are created, updated, or deleted.</p>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#project-creation","title":"Project creation","text":"<p>When a new project gets created, we have to:</p> <ol> <li>Create a self-referential record with depth <code>0</code> for the new project.</li> <li>Create new records pointing from all parents of the new project's parent, to the new project.</li> </ol>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#project-update-with-new-parent","title":"Project update with new parent","text":"<p>When an existing project's parent project changes, we have to:</p> <ol> <li>Delete records which reference the updated project as child.<ul> <li>By design this includes self-referential records.</li> </ul> </li> <li>Create a self-referential record with depth <code>0</code> for the project.</li> <li>Create new records pointing from all parents of the project's new parent, to the project.</li> </ol>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#project-deletion","title":"Project deletion","text":"<p>When a project is deleted, we have to:</p> <ol> <li>Delete records which reference the updated project as child.<ul> <li>By design this includes self-referential records.</li> </ul> </li> </ol>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#using-database-triggers","title":"Using database triggers","text":"<p>We're going to use database triggers to maintain the <code>PROJECT_HIERARCHY</code> table, because:</p> <ul> <li>Leveraging DataNucleus lifecycle listeners requires affected projects to be loaded into memory first.<ul> <li>Incompatible with usage of <code>ON DELETE CASCADE</code> on foreign keys: If the database performs cascading deletes,   the ORM can no longer load those deleted records. We specifically added cascading deletes for efficiency reasons,   and don't want to drop them again.</li> <li>Becomes extremely inefficient for batch deletes, particularly in the context of project retention enforcement,   where we might delete multiple hundreds or thousands of projects in one go.</li> </ul> </li> <li>Manual maintenance is too error-prone. We don't want to play whack-a-mole with code where we forgot to perform maintenance.<ul> <li>Would need to duplicate maintenance logic for code that uses raw SQL instead of DataNucleus.</li> <li>Logic would need to be ported to the <code>hyades</code> repository to make <code>PROJECT_HIERARCHY</code> usable in tests.</li> </ul> </li> <li>Updating the <code>PROJECT_HIERARCHY</code> table is extremely cheap and does not involve any business logic.</li> <li>Data will remain consistent even when project records are modified outside the application context.</li> <li>Triggers require no changes in existing code. Everything just works.</li> </ul>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#example","title":"Example","text":"<p>Consider the following project hierarchy:</p> <pre><code>flowchart TD\n    A[Project A] --&gt;|is parent of| B[Project B]\n    A[Project A] --&gt;|is parent of| C[Project C]\n    B[Project B] --&gt;|is parent of| D[Project D]\n    A[Project A] -.-&gt;|is transitive parent of| D[Project D]</code></pre> <p>It would be represented as follows in the <code>PROJECT</code> table:</p> ID NAME PARENT_PROJECT_ID 1 Project A null 2 Project B 1 3 Project C 1 4 Project D 2 <p>The corresponding <code>PROJECT_HIERARCHY</code> table content would look like this:</p> PARENT_PROJECT_ID CHILD_PROJECT_ID DEPTH 1 1 0 2 2 0 3 3 0 4 4 0 1 2 1 1 3 1 2 4 1 1 4 2 <p>To facilitate access control, assume the team with ID <code>1</code> to have been given access to project <code>B</code>. Consequently, <code>PROJECT_ACCESS_TEAMS</code> will contain the following record:</p> PROJECT_ID TEAM_ID 2 1 <p>In order to check whether team <code>1</code> can access <code>Project D</code> (child of <code>Project B</code>), we can use this query:</p> <pre><code>select exists(\n  select 1\n    from \"PROJECT_ACCESS_TEAMS\" as access_teams\n   inner join \"PROJECT_HIERARCHY\" as hierarchy\n      on hierarchy.\"PARENT_PROJECT_ID\" = access_teams.\"PROJECT_ID\"\n   where access_teams.\"TEAM_ID\" = 1\n     and hierarchy.\"CHILD_PROJECT_ID\" = 4\n)\n</code></pre> <p>Which should return <code>true</code>.</p> <p>Alternatively, a check for whether <code>Project D</code> is a child of <code>Project A</code> becomes:</p> <pre><code>select exists(\n  select 1\n    from \"PROJECT_HIERARCHY\" as hierarchy\n   inner join \"PROJECT\" as parent_project\n      on parent_project.\"ID\" = hierarchy.\"PARENT_PROJECT_ID\"\n   inner join \"PROJECT\" as child_project\n      on child_project.\"ID\" = hierarchy.\"CHILD_PROJECT_ID\"\n   where parent_project.\"NAME\" = 'Project A'\n     and child_project.\"NAME\" = 'Project D'\n)\n</code></pre>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#considered-alternatives","title":"Considered alternatives","text":"<p>@mulder999 suggested a similar approach that reuses the existing <code>PROJECT</code> table:</p> <pre><code>CREATE TABLE PROJECT (\n  project_id SERIAL PRIMARY KEY,\n  name VARCHAR(255) NOT NULL,\n  root_parent_id INT NOT NULL,\n  depth INT NOT NULL,\n  parent_id INT NOT NULL,    \n  CONSTRAINT fk_root_parent FOREIGN KEY (root_parent_id) REFERENCES PROJECT(project_id),\n  CONSTRAINT fk_parent FOREIGN KEY (parent_id) REFERENCES PROJECT(project_id)    \n);\n\nINSERT INTO PROJECT (project_id, name, root_parent_id, depth, parent_id) VALUES\n  (1, 'Project A', 1, 0, 1),\n  (2, 'Project B', 1, 1, 1),\n  (3, 'Project D', 1, 2, 2),\n  (4, 'Project F', 1, 3, 3),\n  (5, 'Project H', 1, 4, 4),\n  (6, 'Project J', 1, 5, 5),\n  (40, 'Project Fxx', 1, 3, 3),\n  (50, 'Project Hxx', 1, 4, 40),\n  (60, 'Project Jxx', 1, 5, 50);\n\nSELECT * -- To show there is no extra records captured\n  FROM PROJECT AS project_parent\nINNER JOIN PROJECT AS project_child ON project_parent.root_parent_id = project_child.root_parent_id\n  AND project_parent.depth &lt; project_child.depth\nWHERE project_parent.name like 'Project D'\n  AND project_child.name like 'Project H';\n</code></pre> <p>But also noted:</p> <p>Sorry this does not work as it captures some mixed up trees. For eg: <code>Project Fxx</code> and <code>Project J</code> would be captured and indeed one would need to load more info to complete filtering in memory.  If you need the subtree in memory and don't have too many sibling subtrees you are not interested in,  this might still be a valid approach.</p>"},{"location":"architecture/decisions/005-materialize-project-hierarchies/#consequences","title":"Consequences","text":"<ul> <li>The <code>PROJECT_HIERARCHY</code> table must be maintained whenever a project is created, deleted,   or its <code>PARENT_PROJECT_ID</code> column is updated. While cheap, it still introduces additional overhead for write operations.</li> <li>Code that historically used recursive CTEs or Java code can trivially be optimized by leveraging the new table.</li> </ul>"},{"location":"architecture/decisions/006-consolidate-user-tables/","title":"ADR-006: Consolidate User Tables","text":"Status Date Author(s) Proposed 2025-04-16 @nscuro"},{"location":"architecture/decisions/006-consolidate-user-tables/#context","title":"Context","text":"<p>The current data model treats managed users, LDAP users, and OIDC users as different entities. This leads to redundancies, both in database tables and application code.</p>"},{"location":"architecture/decisions/006-consolidate-user-tables/#data-model","title":"Data Model","text":"<pre><code>erDiagram\n    ldap_user {\n        bigint id pk\n        text dn\n        text username\n        text email\n    }\n\n    managed_user {\n        bigint id pk\n        text username\n        text fullname\n        text email\n        timestamptz last_password_change\n        bool non_expiry_password\n        bool force_password_change\n        bool suspended\n    }\n\n    oidc_user {\n        bigint id pk\n        text subject_identifier\n        text username\n        text email\n    }\n\n    permission {\n        bigint id pk\n        text name\n    }\n\n    team {\n        bigint id pk\n        text name\n    }\n\n    ldap_user }o--o{ permission: \"has\"\n    managed_user }o--o{ permission: \"has\"\n    oidc_user }o--o{ permission: \"has\"\n    ldap_user }o--o{ team: \"is member of\"\n    managed_user }o--o{ team: \"is member of\"\n    oidc_user }o--o{ team: \"is member of\"\n    team }o--o{ permission: \"has\"</code></pre>"},{"location":"architecture/decisions/006-consolidate-user-tables/#drawbacks","title":"Drawbacks","text":"<ul> <li>The uniqueness of usernames cannot be enforced across all user tables.</li> <li>Queries to determine or modify permissions of a user are unnecessarily complex.</li> <li>Higher cognitive burden due to the many redundant relationships.</li> </ul>"},{"location":"architecture/decisions/006-consolidate-user-tables/#decision","title":"Decision","text":"<ul> <li>Consolidate all user tables in a single table.</li> <li>Use a discriminator column to keep different types apart.</li> <li>Leverage check constraints in the database to handle invariants.</li> <li>Use inheritance strategies for mapping the new model to JDO classes.</li> </ul> <p>@jhoward-lm provides a more thorough draft implementation in this Gist.</p>"},{"location":"architecture/decisions/006-consolidate-user-tables/#data-model_1","title":"Data Model","text":"<pre><code>erDiagram\n    permission {\n        bigint id pk\n        text name\n    }\n\n    team {\n        bigint id pk\n        text name\n    }\n\n    user {\n        bigint id pk\n        enum type\n        text username\n        text fullname\n        text email\n        text ldap_dn\n        text oidc_subject_identifier\n        text password\n        timestamptz last_password_change\n        bool non_expiry_password\n        bool force_password_change\n        bool suspended\n    }\n\n    user }o--o{ permission: \"has\"\n    user }o--o{ team: \"is member of\"\n    team }o--o{ permission: \"has\"</code></pre>"},{"location":"architecture/decisions/006-consolidate-user-tables/#invariants","title":"Invariants","text":"<p>Not all fields make sense for all user types:</p> <ul> <li>LDAP and OIDC users don't have a password.</li> <li>Managed users have no LDAP DN or OIDC subject identifier.</li> </ul> <p>Such invariants should be prevented at the database level, using <code>check</code> constraints. For example:</p> <pre><code>  ADD CONSTRAINT check_user_type\nCHECK (\"TYPE\" IN ('MANAGED', 'LDAP', 'OIDC'));\n\nALTER TABLE \"USER\"\n  ADD CONSTRAINT check_managed_columns\nCHECK (\n    (\"TYPE\" = 'MANAGED'\n        AND \"FORCE_PASSWORD_CHANGE\" IS NOT NULL\n        AND \"LAST_PASSWORD_CHANGE\" IS NOT NULL\n        AND \"NON_EXPIRY_PASSWORD\" IS NOT NULL\n        AND \"PASSWORD\" IS NOT NULL\n        AND \"SUSPENDED\" IS NOT NULL)\n    OR\n    (\"TYPE\" != 'MANAGED'\n        AND \"FORCE_PASSWORD_CHANGE\" IS NULL\n        AND \"FULLNAME\" IS NULL\n        AND \"LAST_PASSWORD_CHANGE\" IS NULL\n        AND \"NON_EXPIRY_PASSWORD\" IS NULL\n        AND \"PASSWORD\" IS NULL\n        AND \"SUSPENDED\" IS NULL)\n);\n\nALTER TABLE \"USER\"\n  ADD CONSTRAINT check_ldap_columns\nCHECK (\n    (\"TYPE\" = 'LDAP' AND \"DN\" IS NOT NULL)\n    OR (\"TYPE\" != 'LDAP' AND \"DN\" IS NULL)\n);\n\nALTER TABLE \"USER\"\n  ADD CONSTRAINT check_oidc_columns\nCHECK (\"TYPE\" = 'OIDC' OR \"SUBJECT_IDENTIFIER\" IS NULL);\n</code></pre>"},{"location":"architecture/decisions/006-consolidate-user-tables/#jdo-mapping","title":"JDO Mapping","text":"<p>To ease interaction with this consolidated model using JDO / DataNucleus, the mapping will be adjusted to use the inheritance strategy <code>SUPERCLASS_TABLE</code>. </p> <p>The abstract <code>User</code> class will hold all fields and relationships that are common among all user types.</p> <pre><code>@PersistenceCapable(table = \"USER\")\n@Discriminator(column = \"TYPE\", strategy = DiscriminatorStrategy.VALUE_MAP, value = \"USER\")\n@Inheritance(strategy = InheritanceStrategy.NEW_TABLE)\npublic abstract class User implements Serializable, Principal {\n\n    @PrimaryKey\n    @Persistent(valueStrategy = IdGeneratorStrategy.NATIVE)\n    private long id;\n\n    @Persistent(table = \"USERS_TEAMS\")\n    @Join(column = \"USER_ID\")\n    @Element(column = \"TEAM_ID\")\n    private List&lt;UserTeam&gt; teams;\n\n    @Persistent(table = \"USERS_PERMISSIONS\")\n    @Join(column = \"USER_ID\")\n    @Element(column = \"PERMISSION_ID\")\n    private List&lt;PermissionsImpl&gt; permissions;\n\n    @Persistent\n    @Column(name = \"USERNAME\")\n    private String username;\n\n    @Persistent\n    @Column(name = \"EMAIL\")\n    private String email;\n\n    @PersistenceCapable\n    @Inheritance(strategy = InheritanceStrategy.SUPERCLASS_TABLE)\n    @Discriminator(value = \"LDAP\")\n    public static class Ldap extends User {\n\n        @Persistent\n        @Column(name = \"DN\")\n        private String dn;\n\n    }\n\n    @PersistenceCapable\n    @Inheritance(strategy = InheritanceStrategy.SUPERCLASS_TABLE)\n    @Discriminator(value = \"MANAGED\")\n    public static class Managed extends User {\n\n        @Persistent\n        @Column(name = \"PASSWORD\")\n        private String password;\n\n        @Persistent\n        @Column(name = \"LAST_PASSWORD_CHANGE\")\n        private Date lastPasswordChange;\n\n        @Persistent\n        @Column(name = \"FULLNAME\")\n        private String fullname;\n\n        @Persistent\n        @Column(name = \"SUSPENDED\")\n        private boolean suspended;\n\n        @Persistent\n        @Column(name = \"FORCE_PASSWORD_CHANGE\")\n        private boolean forcePasswordChange;\n\n        @Persistent\n        @Column(name = \"NON_EXPIRY_PASSWORD\")\n        private boolean nonExpiryPassword;\n\n    }\n\n    @PersistenceCapable\n    @Inheritance(strategy = InheritanceStrategy.SUPERCLASS_TABLE)\n    @Discriminator(value = \"OIDC\")\n    public static class Oidc extends User {\n\n        @Persistent\n        @Column(name = \"SUBJECT_IDENTIFIER\")\n        private String subjectIdentifier;\n\n    }\n\n}\n</code></pre>"},{"location":"architecture/decisions/006-consolidate-user-tables/#consequences","title":"Consequences","text":"<ul> <li>Existing user records will need to be migrated.</li> <li>Need to decide if we also want to consolidate REST API endpoints or keep the current endpoints and responses to avoid breaking changes. This could be a multi-stage effort as well.</li> </ul>"},{"location":"architecture/design/workflow-state-tracking/","title":"Tracking of Workflow State for BOM Processing and Analysis","text":"<p>Note</p> <p>This document was extracted from #664.</p> <p>For CI/CD use-cases, Dependency-Track offers a mechanism that allows clients to poll whether the BOM they just uploaded is still being processed. \"Processing\" in this context refers to:</p> <ul> <li>Consumption (Parsing)</li> <li>Ingestion (Sync parsed data with database)</li> <li>Vulnerability analysis</li> <li>Policy Evaluation</li> </ul> <p>This is an important capability, allowing for implementation of quality gates and reporting in CI/CD pipelines.</p> <p>The mechanism works by assigning identifiers (UUIDs) to events in Alpine's in-memory event system. As long as an event associated to a given identifier can be found in its internal queue, the identifier is considered to be \"processing\":</p> <ul> <li>https://github.com/DependencyTrack/dependency-track/blob/6153d286d1ac806462bc76cfe17d84a57c224671/src/main/java/org/dependencytrack/resources/v1/BomResource.java#L323-L342</li> <li>https://github.com/stevespringett/Alpine/blob/cd6aa7ed05376935ab32bc43819eba0e3a525b7f/alpine-infra/src/main/java/alpine/event/framework/BaseEventService.java#L158-L161</li> </ul> <p>Events can be chained, such that a <code>BomUploadEvent</code> will trigger a <code>VulnerabilityAnalysisEvent</code>, which will trigger a <code>PolicyEvaluationEvent</code>, and so on. The event identifier is inherited by chained events.</p> <p>As everything happens in-memory, this mechanism does not work when multiple instances of the API server are used in an active-active deployment. For the functionality to continue to work, the state of processing steps must be persistent to external storage.</p> <p>Decoupled from #633.</p>"},{"location":"architecture/design/workflow-state-tracking/#design","title":"Design","text":"<p>Note</p> <p>The goal for now is not to build a multi-purpose workflow engine, but to track state of one specific workflow. In a future iteration, we may invest more time into coming up with a generic workflow engine concept.</p>"},{"location":"architecture/design/workflow-state-tracking/#what-to-track","title":"What to track","text":"<ol> <li>BOM consumption (parsing, de-duplication, etc.)</li> <li>BOM processing (ingestion into database)</li> <li>Vulnerability analysis</li> <li>Repository metadata analysis</li> <li>Policy evaluation</li> <li>Metrics update</li> </ol> <p>The order of execution as of today is as follows:</p> <pre><code>flowchart LR\n    A(BOM_CONSUMPTION) --&gt; B(BOM_PROCESSING)\n    B --&gt; C(VULN_ANALYSIS)\n    B --&gt; D(REPO_META_ANALYSIS)\n    C --&gt; E(POLICY_EVALUATION)\n    E --&gt; F(METRICS_UPDATE)\n    B -. BOM contains no &lt;br/&gt;components to analyze .-&gt; F</code></pre> <p>Note</p> <p>Completion of repository metadata analysis can currently not be tracked. We'll need something similar to what we introduced in https://github.com/DependencyTrack/hyades-apiserver/pull/40 for vulnerability analysis completion tracking. For the initial implementation, it may be OK to not track it.</p>"},{"location":"architecture/design/workflow-state-tracking/#states","title":"States","text":"<p>There are multiple states a processing step can be in:</p> <ul> <li><code>PENDING</code>: Initial state</li> <li><code>COMPLETED</code>: Completion detected; No failures</li> <li><code>FAILED</code>: Completion detected; Failures</li> <li><code>CANCELLED</code>: Never started because a precondition failed<ul> <li>e.g. vulnerability analysis will be cancelled when BOM ingestion failed</li> </ul> </li> <li><code>NOT_APPLICABLE</code>: The step is not applicable to the subject of the analysis<ul> <li>e.g. the analysis was triggered for an existing project, without BOM upload</li> </ul> </li> <li><code>TIMED_OUT</code>: The deadline for the step completion was exceeded; Step is unlikely to complete</li> </ul> <pre><code>stateDiagram-v2\n    [*] --&gt; PENDING\n    PENDING --&gt; COMPLETED\n    COMPLETED --&gt; [*]\n    PENDING --&gt; FAILED\n    FAILED --&gt; [*]\n    PENDING --&gt; CANCELLED\n    CANCELLED --&gt; [*]\n    [*] --&gt; NOT_APPLICABLE\n    NOT_APPLICABLE --&gt; [*]\n    PENDING --&gt; TIMED_OUT\n    TIMED_OUT --&gt; COMPLETED\n    TIMED_OUT --&gt; FAILED</code></pre> <p>Each step is responsible for updating its own state.</p> <p>The overall state can be considered to be complete, if there's no step in <code>PENDING</code> state.</p> <p>When a step failure is detected, a \"failure reason\" message must be persisted. If multiple steps fail, (rough) failure details for each step must be available.</p> <p>Note</p> <p>There should be a deadline mechanism, which automatically transitions steps from <code>PENDING</code> into <code>TIMED_OUT</code> state. Steps in <code>TIMED_OUT</code> state communicate that it is unlikely that a terminal state can be reached (<code>COMPLETED</code>, <code>FAILED</code>). However, it is still possible (e.g. due to significant consumer lag, events may arrive late).</p>"},{"location":"architecture/design/workflow-state-tracking/#workflow","title":"Workflow","text":"<p>The API server will act as workflow orchestrator, kicking off new steps as needed. This implies that completion of a step must be registered by the API server.</p> <p>To illustrate how the initial stage of the workflow should look like:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ BomResource: Upload BOM\n    BomResource -&gt;&gt; BomResource: Validate BOM\n    BomResource -&gt;&gt; BomResource: Generate Correlation ID (UUID)\n    BomResource -&gt;&gt; Database: Create workflow steps\n    Note over BomResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=BOM_CONSUMPTION&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=BOM_PROCESSING&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Status=PENDING\n    BomResource -&gt;&gt; BomUploadProcessingTask: Dispatch BomUploadEvent\n    BomResource -&gt;&gt;- Client: Correlation ID\n    par\n        loop Continuously\n            Client -&gt;&gt; BomResource: Poll Status\n            Note over Client, BomResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n        end\n    and\n        BomUploadProcessingTask -&gt;&gt; Database: Update step start time\n        activate BomUploadProcessingTask\n        Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;StartedAt=&lt;NOW&gt;\n        BomUploadProcessingTask -&gt;&gt; BomUploadProcessingTask: Consume BOM&lt;br/&gt;(Parse, De-dupe)\n        alt Consumption succeeded\n            BomUploadProcessingTask -&gt;&gt; Database: Update step status\n            Note over BomUploadProcessingTask, Database: Step=BOM_CONSUMPTION&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Process BOM&lt;br/&gt;(Insert, Update, Delete)\n            alt Processing succeeded\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=COMPLETED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n                BomUploadProcessingTask -&gt;&gt; Kafka: Publish events to \"dtrack.repo-meta-analysis.component\" topic\n            else Processing failed\n                BomUploadProcessingTask -&gt;&gt; Database: Update step status\n                Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n                BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=VULN_ANALYSIS&lt;br/&gt;Status=CANCELLED\n            end\n        else Consumption failed\n            BomUploadProcessingTask -&gt;&gt; Database: Update step Status\n            Note over BomUploadProcessingTask, Database: Status=FAILED&lt;br/&gt;UpdatedAt=&lt;NOW&gt;\n            BomUploadProcessingTask -&gt;&gt; Database: Cancel follow-up steps\n            Note over BomUploadProcessingTask, Database: Step=BOM_PROCESSING&lt;br/&gt;Status=CANCELLED\n        end\n        deactivate BomUploadProcessingTask\n    end</code></pre> <p>When triggering a re-analysis of an already existing project, the workflow may be kicked off as follows:</p> <pre><code>sequenceDiagram\n    Client -&gt;&gt;+ FindingResource: Re-Analyze project\n    FindingResource -&gt;&gt; FindingResource: Generate Correlation ID (UUID)\n    FindingResource -&gt;&gt; Database: Create workflow steps\n    Note over FindingResource, Database: Token=&lt;CORRELATION_ID&gt;, Step=VULN_ANALYSIS&lt;br/&gt;Token=&lt;CORRELATION_ID&gt;, Step=POLICY_EVALUATION&lt;br/&gt;Status=PENDING\n    FindingResource -&gt;&gt; Kafka: Publish events to \"dtrack.vuln-analysis.component\" topic\n    FindingResource -&gt;&gt;- Client: Correlation ID\n    loop Continuously\n        Client -&gt;&gt; FindingResource: Poll Status\n        Note over Client, FindingResource: GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;/status\n    end</code></pre>"},{"location":"architecture/design/workflow-state-tracking/#proposed-table-schema","title":"Proposed Table Schema","text":"<p>Each step of the workflow will be represented in a dedicated row. This allows us to add or remove steps without altering the database schema (see original version of the schema further down below), or even add steps while the workflow is running. It also plays better with concurrent writes, as no two threads / instances will need to modify the same row.</p> Name Type Nullable Example ID <code>SERIAL</code> \u274c 1 PARENT_STEP_ID <code>SERIAL FK</code> \u2705 0 TOKEN <code>VARCHAR(36)</code> \u274c <code>484d9eaa-7ea4-4476-97d6-f36327b5a626</code> STARTED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> UPDATED_AT <code>TIMESTAMP</code> \u2705 <code>1999-01-08 04:05:06</code> STEP <code>VARCHAR(64)</code> \u274c <code>METRICS_UPDATE</code> STATUS <code>VARCHAR(64)</code> \u274c <code>PENDING</code> FAILURE_REASON <code>TEXT</code> \u2705 <code>Failed to acquire database connection</code> <p>Potential Future Improvements: * Do we need/want to capture the order in which steps are supposed to be executed? * Do we need/want to capture metadata of the overall workflow (who triggered it, when was it triggered, correlation id, ...)?</p> <p>Where applicable, the \"detailed\" status of a step is tracked in a dedicated table.</p>"},{"location":"architecture/design/workflow-state-tracking/#retention","title":"Retention","text":"<p>Rows in the table should be cleaned up on a recurring basis.</p> <p>This could be as simple as scheduling a job that executes this SQL query:</p> <pre><code>DELETE FROM org.dependencytrack.repometaanalyzer.model.WorkflowStep WHERE this.updatedAt &lt; :threshold\n</code></pre> <p>A retention time of 1-3 days since the last update should be reasonable.</p>"},{"location":"architecture/design/workflow-state-tracking/#rest-api-endpoints","title":"REST API endpoints","text":"<p>The existing endpoint to check whether a BOM is still being processed should continue to work as expected:</p> <p><pre><code>GET /api/v1/bom/token/&lt;CORRELATION_ID&gt;\n</code></pre> <pre><code>{\n  \"processing\": true\n}\n</code></pre></p> <p>An additional endpoint may be added, which allows for retrieval of the individual step states:</p> <p><pre><code>GET /api/v1/workflow/token/&lt;CORRELATION_ID&gt;/status\n</code></pre> <pre><code>[\n  {\n    \"step\": \"BOM_CONSUMPTION\",\n    \"status\": \"COMPLETED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\"\n  },\n  {\n    \"step\": \"BOM_PROCESSING\",\n    \"status\": \"FAILED\",\n    \"startedAt\": \"1999-01-08 04:05:06\",\n    \"updatedAt\": \"1999-01-08 04:05:06\",\n    \"failureReason\": \"Failed to acquire database connection\"\n  },\n  {\n    \"step\": \"VULN_ANALYSIS\",\n    \"status\": \"CANCELLED\"\n  }\n]\n</code></pre></p> <p>If all a client cares about are vulnerability analysis results, they could stop polling immediately after <code>vulnerabilityAnalysis</code> transitions into the <code>COMPLETED</code> state.</p>"},{"location":"development/building/","title":"Building","text":""},{"location":"development/building/#hyades","title":"<code>hyades</code>","text":"<p>This segment provides build instructions for the <code>DependencyTrack/hyades</code> repository.</p>"},{"location":"development/building/#jars","title":"JARs","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build</code> run configuration.</p> <p>To build JARs for all modules in the repository:</p> <pre><code>mvn clean package -DskipTests\n</code></pre> <p>For application modules, this will produce a Quarkus fast-jar in their respective <code>target</code> directory.</p> <p>To only build JARs for specific modules, use Maven's <code>-pl</code> flag:</p> <pre><code>mvn -pl mirror-service,vulnerability-analyzer clean package -DskipTests\n</code></pre> <p>Note</p> <p>If you made changes to shared modules (e.g. <code>commons</code>), those changes may not be visible to other modules when building specific modules as shown above. Either include the shared modules in the <code>-pl</code> argument, or run <code>mvn clean install -DskipTests</code> beforehand.</p>"},{"location":"development/building/#containers","title":"Containers","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build Container Images</code> run configuration.</p> <p>To build JARs and container images for all modules in the repository:</p> <pre><code>mvn clean package \\\n  -Dquarkus.container-image.build=true \\\n  -Dquarkus.container-image.additional-tags=local \\\n  -DskipTests=true\n</code></pre> <p>As demonstrated before, you can use Maven's <code>-pl</code> flag to limit the build to specific modules.</p> <p>The resulting container images are tagged as:</p> <pre><code>ghcr.io/dependencytrack/hyades-${moduleName}:local\n</code></pre> <p>For example:</p> <pre><code>ghcr.io/dependencytrack/hyades-vulnerability-analyzer:local\n</code></pre>"},{"location":"development/building/#native-executables","title":"Native Executables","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build Native</code> run configuration.</p> <p>Application modules in the <code>hyades</code> repository can be compiled to native executables, by leveraging GraalVM Native Image.</p> <p>Warning</p> <p>Building native executables is resource intensive and can take a few minutes to complete.</p> <p>Note</p> <p>Building native executables requires GraalVM for JDK 21 or newer. You can install GraalVM with <code>sdkman</code>: https://sdkman.io/jdks#graalce</p> <p>To build native executables for all modules:</p> <pre><code>export GRAALVM_HOME=\"$(sdk home java 21.0.2-graalce)\"\nmvn clean package -Dnative -DskipTests\n</code></pre> <p>As demonstrated before, you can use Maven's <code>-pl</code> flag to limit the build to specific modules.</p> <p>If installing GraalVM is not possible, or you want to use the native executables in a container, but your host system is not Linux, you can leverage a GraalVM container to perform the build:</p> <pre><code>mvn clean package -Dnative -DskipTests -Dquarkus.native.container-build=true\n</code></pre>"},{"location":"development/building/#hyades-apiserver","title":"<code>hyades-apiserver</code>","text":"<p>This segment provides build instructions for the <code>DependencyTrack/hyades-apiserver</code> repository.</p>"},{"location":"development/building/#jar","title":"JAR","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build API Server</code> run configuration.</p> <p>To build an executable JAR:</p> <pre><code>mvn clean package -DskipTests\n</code></pre> <p>The resulting file is placed in <code>./apiserver/target</code> as <code>dependency-track-apiserver.jar</code>. The JAR ships with an embedded Jetty server, there's no need to deploy it in an application server like Tomcat or WildFly.</p>"},{"location":"development/building/#container","title":"Container","text":"<p>Tip</p> <p>IntelliJ users can simply execute the <code>Build API Server Image</code> run configuration.</p> <p>Ensure you've built the API server JAR as outlined above.</p> <p>To build the API server image:</p> <pre><code>docker build \\\n  -t ghcr.io/dependencytrack/hyades-apiserver:local \\\n  -f ./apiserver/src/main/docker/Dockerfile \\\n  ./apiserver\n</code></pre>"},{"location":"development/building/#hyades-frontend","title":"<code>hyades-frontend</code>","text":"<p>This segment provides build instructions for the <code>DependencyTrack/hyades-frontend</code> repository.</p>"},{"location":"development/building/#distribution","title":"Distribution","text":"<p>To build the frontend using webpack:</p> <pre><code>npm run build\n</code></pre> <p>The build artifacts are placed in <code>./dist</code>. The contents of <code>dist</code> can be deployed to any webserver capable of serving static files.</p>"},{"location":"development/building/#container_1","title":"Container","text":"<p>Ensure you've built the frontend as outlined above.</p> <p>To build the frontend image:</p> <pre><code>docker build -f docker/Dockerfile.alpine -t ghcr.io/dependencytrack/hyades-frontend:local .\n</code></pre>"},{"location":"development/database-migrations/","title":"Database Migrations","text":""},{"location":"development/database-migrations/#introduction","title":"Introduction","text":"<p>In contrast to Dependency-Track v4 and earlier, Dependency-Track v5 manages database migrations with Liquibase. The database schema is still owned by the API server though. It will execute migrations upon startup, unless explicitly disabled via <code>database.run.migrations</code>.</p> <p>Liquibase operates with the concept of changelogs. For the sake of better visibility, Dependency-Track uses separate changelogs for each release version. Individual changelogs are referenced by <code>changelog-main.xml</code>.</p> <p>Stored procedures and custom SQL functions are treated differently: They are re-created whenever their content changes. Their sources are located in the <code>procedures</code> directory.</p>"},{"location":"development/database-migrations/#adding-migrations","title":"Adding Migrations","text":"<ol> <li>If it doesn't exist already, create a <code>changelog-vX.Y.Z.xml</code> file<ul> <li><code>X</code>, <code>Y</code>, and <code>Z</code> must correspond to the current release version</li> </ul> </li> <li>Ensure the <code>changelog-vX.Y.Z.xml</code> file is referenced via <code>include</code> in <code>changelog-main.xml</code></li> <li>Add your changeset to <code>changelog-vX.Y.Z.xml</code></li> </ol> <p>When adding a new <code>changeset</code>, consider the following guidelines:</p> <ul> <li>The changeset ID must follow the <code>vX.Y.Z-&lt;NUM&gt;</code> format, where:<ul> <li><code>X</code>, <code>Y</code>, and <code>Z</code> match the changelog's version</li> <li><code>NUM</code> is an incrementing number, starting at <code>1</code> for the first <code>changeset</code> of the release</li> </ul> </li> <li>The <code>author</code> must correspond to your GitHub username</li> <li>Prefer built-in change types<ul> <li>Use the <code>sql</code> change type if no fitting built-in exists</li> <li>Use a custom change in edge cases, when additional computation is required</li> </ul> </li> <li>When using custom changes:<ul> <li>Use the <code>org.dependencytrack.persistence.migration.change</code> package</li> <li>Changes must not depend on domain logic</li> </ul> </li> <li>You must not modify <code>changeset</code>s that were already committed to <code>main</code></li> </ul>"},{"location":"development/database-migrations/#making-schema-changes-available-to-hyades-services","title":"Making Schema Changes Available to Hyades Services","text":"<p>Because the schema is owned by the API server, and the API server is also responsible for executing migrations, other services that access the database must replicate the current schema, in order to run tests against it.</p> <p>Currently, this is achieved by:</p> <ol> <li>Having Liquibase generate the schema SQL based on the changelog</li> <li>Adding the <code>schema.sql</code> file as resource to the <code>commons-persistence</code> module</li> <li>Having all services that require database access depend on <code>commons-persistence</code></li> <li>Configuring Quarkus Dev Services to initialize new database containers with <code>schema.sql</code><ul> <li>Using <code>quarkus.datasource.devservices.init-script-path</code></li> </ul> </li> </ol> <p>The schema can be generated using the <code>dbschema-generate.sh</code> script in the <code>hyades-apiserver</code> repository:</p> <pre><code>./dev/scripts/dbschema-generate.sh\n</code></pre> <p>Note</p> <ul> <li>You may need to build the API server project once before running the script</li> <li>Because Liquibase requires database to run against, the script will launch a temporary PostgreSQL container</li> </ul> <p>The output is written to <code>target/liquibase/migrate.sql</code>.</p>"},{"location":"development/documentation/","title":"Documentation","text":""},{"location":"development/documentation/#introduction","title":"Introduction","text":"<p>User-facing documentation is implemented with MkDocs and Material for MkDocs. The sources are located in <code>docs</code>. Changes to the documentation are automatically deployed to GitHub pages, using the <code>deploy-docs.yml</code> GitHub Actions workflow. Once deployed, the documentation is available at https://dependencytrack.github.io/hyades/snapshot.</p>"},{"location":"development/documentation/#versioning","title":"Versioning","text":"<p>Documentation is published for each version of the project, including unstable <code>SNAPSHOT</code> versions. This allows users to browse the docs most relevant to their Dependency-Track deployment.</p> Version selection on the documentation site <p>Documentation for unstable versions is aliased as <code>snapshot</code>, whereas for stable builds it is aliased as <code>latest</code>. They are accessible via <code>/snapshot</code> and <code>/latest</code> respectively.</p> <p>Tip</p> <p>When sharing links to the docs with others, prefer using specific versions instead of <code>latest</code> or <code>snapshot</code>. For example https://dependencytrack.github.io/hyades/0.4.0. This ensures that your links will not break as documentation evolves.</p> <p>The versioning logic is handled by mike as part of the <code>deploy-docs.yml</code> workflow.</p>"},{"location":"development/documentation/#local-development","title":"Local Development","text":"<p>For local building and rendering of the docs, use the  <code>docs-dev.sh</code> script:</p> <pre><code>./scripts/docs-dev.sh\n</code></pre> <p>It will launch a development server that listens on http://localhost:8000 and reloads whenever changes are made to the documentation sources. The script requires the <code>docker</code> command to be available.</p>"},{"location":"development/documentation/#configuration-documentation","title":"Configuration Documentation","text":"<p>To make it easier for users to discover available configuration options (i.e. environment variables), we generate human-readable documentation for it. You can see the result of this here.</p>"},{"location":"development/documentation/#applicationproperties-annotations","title":"<code>application.properties</code> Annotations","text":"<p>We leverage comments on property definitions to gather metadata. Other than a property's description, the following annotations are supported to provide further information:</p> Annotation Description <code>@category</code> Allows for categorization / grouping of related properties <code>@default</code> To be used for cases where the default value is implicit, for example when it is inherited from the framework or other properties <code>@example</code> To give an idea of what a valid value may look like, when it's not possible to provide a sensible default value <code>@hidden</code> Marks a property as to-be-excluded from the generated docs <code>@required</code> Marks a property as required <code>@type</code> Defines the type of the property <p>For example, a properly annotated property might look like this:</p> <pre><code># Defines the path to the secret key to be used for data encryption and decryption.\n# The key will be generated upon first startup if it does not exist.\n#\n# @category: General\n# @default:  ${alpine.data.directory}/keys/secret.key\n# @type:     string\nalpine.secret.key.path=\n</code></pre> <p>It is also possible to index properties that are commented out, for example:</p> <pre><code># Foo bar baz.\n#\n# @category: General\n# @example:  Some example value\n# @type:     string\n# foo.bar.baz=\n</code></pre> <p>This can be useful when it's not possible to provide a sensible default, and providing the property without a value would break something. Generally though, you should always prefer setting a sensible default.</p> <p>If a property depends on another property, or relates to it, mention it in the description. A deep-link will automatically be generated for it. For example:</p> Example of a generated deep-link for <code>alpine.cors.enabled</code>"},{"location":"development/documentation/#generation","title":"Generation","text":"<p>Configuration documentation is generated from <code>application.properties</code> files. We use the <code>GenerateConfigDocs</code> JBang script for this:</p> <pre><code>Usage: GenerateConfigDocs [--include-hidden] [-o=OUTPUT_PATH] -t=TEMPLATE_FILE\n                          PROPERTIES_FILE\n      PROPERTIES_FILE        The properties file to generate documentation for\n      --include-hidden       Include hidden properties in the output\n  -o, --output=OUTPUT_PATH   Path to write the output to, will write to STDOUT\n                               if not provided\n  -t, --template=TEMPLATE_FILE\n                             The Pebble template file to use for generation\n</code></pre> <p>Tip</p> <p>Usually you do not need to run the script yourself. We have a GitHub Actions workflow (<code>update-config-docs.yml</code>)  that does that automatically whenever a modification to <code>application.properties</code> files is pushed to the <code>main</code> branch. It also works across repositories, i.e. it will be triggered for changes in the <code>hyades-apiserver</code> repository as well.</p> <p>To generate documentation for the API server, you would run:</p> <pre><code>jbang scripts/GenerateConfigDocs.java \\\n    -t ./scripts/config-docs.md.peb \\\n    -o ./docs/reference/configuration/api-server.md \\\n    ../hyades-apiserver/src/main/resources/application.properties\n</code></pre> <p>Output is generated based on a customizable Pebble template  (currently <code>config-docs.md.peb</code>).</p>"},{"location":"development/overview/","title":"Overview","text":"<p>Want to hack on Hyades, the upcoming Dependency-Track v5? Awesome, here's what you need to know to get started!</p> <p>Important</p> <p>Please be sure to read <code>CONTRIBUTING.md</code> and <code>CODE_OF_CONDUCT.md</code> as well.</p>"},{"location":"development/overview/#repositories","title":"Repositories","text":"<p>The project consists of the following repositories:</p> Repository Description DependencyTrack/hyades Main repository. Includes Hyades services, end-to-end tests, documentation, and deployment manifests. GitHub issues and discussions are managed here. DependencyTrack/hyades-apiserver Fork of <code>DependencyTrack/dependency-track</code>.  GitHub issues and discussions are disabled. DependencyTrack/hyades-frontend Fork of <code>DependencyTrack/frontend</code>.  GitHub issues and discussions are disabled. <p>Note</p> <p>The <code>hyades</code> and <code>hyades-apiserver</code> repositories are split for historical reasons. We are planning to merge them, which should result in less overhead and more opportunities for code sharing.</p> <p>To clone them all:</p> <pre><code>git clone https://github.com/DependencyTrack/hyades.git\ngit clone https://github.com/DependencyTrack/hyades-apiserver.git\ngit clone https://github.com/DependencyTrack/hyades-frontend.git\n</code></pre>"},{"location":"development/overview/#prerequisites","title":"Prerequisites","text":"<p>There are a few things you'll need on your journey:</p> <ul> <li>Java Development Kit<sup>1</sup> &gt;=21 (Temurin distribution recommended)</li> <li>Maven<sup>1</sup> &gt;=3.9 (comes bundled with IntelliJ and Eclipse)</li> <li>NodeJS<sup>2</sup> &gt;=20</li> <li>A Java and JavaScript capable editor or IDE of your preference (we recommend IntelliJ<sup>3</sup>)</li> <li>Docker or Podman</li> <li>Docker Compose or Podman Compose</li> </ul> Tip <p><sup>1</sup> We recommend sdkman to install Java and Maven. When working in a corporate environment, you should obviously prefer the packages provided by your organization.</p> <p><sup>2</sup> If you need to juggle multiple NodeJS versions on your system, consider using nvm to make this more bearable.</p> <p><sup>3</sup> We provide common run configurations for IntelliJ in the <code>.idea/runConfigurations</code> directories of each repository for convenience. IntelliJ will automatically pick those up when you open this repository.</p>"},{"location":"development/overview/#core-technologies","title":"Core Technologies","text":"<p>Knowing about the core technologies may help you with understanding the code base.</p>"},{"location":"development/overview/#infrastructure","title":"Infrastructure","text":"Technology Purpose PostgreSQL Database Apache Kafka Messaging / Streaming"},{"location":"development/overview/#api-server","title":"API Server","text":"Technology Purpose JAX-RS REST API specification Jersey JAX-RS implementation Java Data Objects (JDO) Persistence specification DataNucleus JDO implementation JDBI Lightweight database operations Liquibase Database migrations Confluent Parallel Consumer Kafka message processing Jetty Servlet Container Alpine Framework / Scaffolding Maven Package manager / Build tool Java Programming language"},{"location":"development/overview/#hyades-services","title":"Hyades Services","text":"Technology Purpose Kafka Streams Stream processing Quarkus Framework / Scaffolding Maven Package manager / Build tool Java Programming language"},{"location":"development/overview/#frontend","title":"Frontend","text":"Technology Purpose webpack Asset bundler Vue.js Framework NPM Package manager / Build tool JavaScript Programming language"},{"location":"development/testing/","title":"Testing","text":""},{"location":"development/testing/#introduction","title":"Introduction","text":"<p>We generally aim for a test coverage of ~80%. This is also true for new code introduced through pull requests. We value integration tests more than unit tests, and try to avoid using mocks as much as possible. If reaching the 80% test coverage requires us to write tests that don't really test anything meaningful, or require loads of mocking, we rather take lower coverage than writing those tests.</p> <p>We use Testcontainers, Wiremock, and GreenMail to test how the system interacts with the outside world.</p>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":""},{"location":"development/testing/#hyades","title":"<code>hyades</code>","text":"<p>TBD</p>"},{"location":"development/testing/#hyades-apiserver","title":"<code>hyades-apiserver</code>","text":"<p>Warning</p> <p>To reduce execution time of the test suite in CI, the PostgreSQL Testcontainer is reused. While tables are truncated after each test, sequences (e.g. for <code>ID</code> columns) won't be reset. As a consequence, you should not assert on IDs of database records.</p>"},{"location":"development/testing/#hyades-frontend","title":"<code>hyades-frontend</code>","text":"<p>There are currently no unit tests for the frontend.</p>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":""},{"location":"development/testing/#hyades_1","title":"<code>hyades</code>","text":"<p>Integration tests in the <code>hyades</code> repository are implemented as @QuarkusIntegrationTest.  As such, they are executed against an actual build artifact (JAR, container, or native executable).</p> <p>Class names of integration tests are suffixed with <code>IT</code> instead of <code>Test</code>.</p>"},{"location":"development/testing/#execution","title":"Execution","text":"<p>Integration tests can be launched individually through your IDE, or all at once using Maven:</p> <pre><code>mvn -pl '!e2e' clean verify failsafe:integration-test -DskipITs=false\n</code></pre> <p>To limit the test run to specific modules, use <code>-pl &lt;module&gt;</code>, for example:</p> <pre><code>mvn -pl vulnerability-analyzer clean verify failsafe:integration-test -DskipITs=false\n</code></pre>"},{"location":"development/testing/#execution-in-ci","title":"Execution in CI","text":"<p>In CI, integration tests are executed:</p> <ul> <li>Against all JARs, as part of the <code>CI / Test</code> workflow</li> <li>Against native executables, as part of the <code>CI / Test Native Image</code> workflow(s)</li> </ul> <p>Both workflows run for pushes and pull requests to the <code>main</code> branch.</p>"},{"location":"development/testing/#hyades-apiserver_1","title":"<code>hyades-apiserver</code>","text":"<p>The API server repository does not currently differentiate between unit- and integration-tests.</p>"},{"location":"development/testing/#hyades-frontend_1","title":"<code>hyades-frontend</code>","text":"<p>There are currently no integration tests for the frontend.</p>"},{"location":"development/testing/#end-to-end-tests","title":"End-to-End Tests","text":"<p>End-to-End tests spin up containers for all services of the system. The test environment is torn down and rebuilt for every test case. Containers are started and managed using Testcontainers.</p> <p>The tests are located in the <code>e2e</code> module of the <code>hyades</code> repository. Container images used are defined in the <code>AbstractE2ET</code> class.</p> <p>Image versions can be overwritten using the following environment variables:</p> <ul> <li><code>APISERVER_VERSION</code></li> <li><code>HYADES_VERSION</code></li> </ul>"},{"location":"development/testing/#execution_1","title":"Execution","text":"<p>Tests can be launched individually through your IDE, or all at once using Maven:</p> <pre><code>mvn -pl e2e clean verify -Pe2e-all\n</code></pre> <p>To test against local changes:</p> <ol> <li>Build container images for the modified services</li> <li>Update the image tags in <code>AbstractE2ET</code> accordingly</li> <li>Run e2e tests as detailed above</li> </ol>"},{"location":"development/testing/#execution-in-ci_1","title":"Execution in CI","text":"<p>In CI, end-to-end tests are executed for every push to the <code>main</code> branch, as well as every night at 12AM.</p> <p>They can additionally be run manually, via the GitHub Actions UI. Both the API server and Hyades version can be customized before execution.</p>"},{"location":"development/testing/#e2e-testing-with-playwright-bdd","title":"E2E Testing with Playwright (BDD)","text":"<p>Apart from the current E2E-Testing approach, there will be another approach managed from now on that uses Playwright BDD (Behaviour-Driven-Development). The new approach provides a more simplified understanding on what is executed in each test. More on that can be found inside Playwright README</p> <p>V1 will be managed inside the Hyades repository.  The next version will be decoupled into its own repository, abstracting it from Hyades entirely and also allowing for DTrackV4 (aka the Monolith) to be tested.</p>"},{"location":"development/testing/#manual-tests","title":"Manual Tests","text":""},{"location":"development/testing/#docker-compose","title":"Docker Compose","text":"<p>The easiest way to test the entire system is by using Docker Compose.</p> <p>A <code>docker-compose.yml</code> file is provided in the <code>DependencyTrack/hyades</code> repository.</p> <p>Without any profile specified, <code>docker compose up -d</code> will launch:</p> <ul> <li>PostgreSQL</li> <li>Kafka (currently Redpanda)</li> <li>Kafka UI (currently Redpanda Console)</li> </ul> <p>To launch Dependency-Track services, use the <code>demo</code> profile:</p> <pre><code>docker compose up -d --profile demo\n</code></pre> <p>To test different versions of the services, simply modify the <code>image</code> property of the respective Compose <code>service</code>.</p>"},{"location":"development/testing/#api-server","title":"API Server","text":"<p>When testing changes that are limited to the API server, such as updates to the REST API, it's possible to launch the API server in dev services mode:</p> <pre><code>mvn -pl apiserver -Pdev-services jetty:run\n</code></pre> <p>The container images used may be configured via:</p> <ul> <li><code>dev.services.image.frontend</code></li> <li><code>dev.services.image.kafka</code></li> <li><code>dev.services.image.postgresql</code></li> </ul>"},{"location":"getting-started/changes-over-v4/","title":"Changes over v4","text":""},{"location":"getting-started/changes-over-v4/#new-features","title":"New Features","text":"<ul> <li>New powerful CEL-based policy engine, providing more flexibility while being more efficient than the engine shipped with v4. </li> <li>Ability to automatically audit vulnerabilities across the entire portfolio using CEL expressions. </li> <li>Hash-based integrity analysis for components. </li> <li>The API server now supports high availability (HA) deployments in active-active configuration.</li> <li>Zero downtime deployments when running API server in HA configuration.</li> <li>Greatly reduced resource footprint of the API server.</li> <li>The status of asynchronous tasks (e.g. vulnerability analysis) is now   tracked in a persistent manner,   improving observability.</li> </ul>"},{"location":"getting-started/changes-over-v4/#architecture-operations","title":"Architecture / Operations","text":"<ul> <li>PostgreSQL is the only supported database.<ul> <li>Support for H2, MySQL, and Microsoft SQL Server is dropped.</li> </ul> </li> <li>To facilitate communication between services, a Kafka-compatible broker is required.</li> <li>Publishing of notifications, fetching component metadata from repositories, and vulnerability analysis is performed by services separately from the API server.<ul> <li>The services can be scaled up and down as needed.</li> <li>Some services (i.e. <code>notification-publisher</code>) can be omitted entirely from a deployment,   if publishing of notification via e.g. Webhook is not needed.</li> </ul> </li> <li>All services except the API server can optionally be deployed as native executables (thanks to GraalVM), offering a lower resource footprint than their JVM-based counterparts.</li> <li>Database migrations are performed through a more reliable, changelog-based approach.</li> </ul>"},{"location":"getting-started/changes-over-v4/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>All deprecated endpoints mentioned below were removed:<ul> <li>POST <code>/api/v1/policy/{policyUuid}/tag/{tagName}</code></li> <li>DELETE <code>/api/v1/policy/{policyUuid}/tag/{tagName}</code></li> <li>GET <code>/api/v1/tag/{policyUuid}</code></li> <li>GET <code>/api/v1/bom/token/{uuid}</code></li> </ul> </li> </ul>"},{"location":"getting-started/changes-over-v4/#notifications","title":"Notifications","text":"<ul> <li><code>subject</code> objects passed to notification templates are now objects generated from Protobuf definitions.<ul> <li>The respective schema is defined in notification.proto.</li> <li>List fields now have a <code>List</code> suffix (i.e. <code>vulnerabilities</code> -&gt; <code>vulnerabilitiesList</code>).</li> </ul> </li> <li>Level values are now prefixed with <code>LEVEL_</code><ul> <li>Before: <code>INFORMATIONAL</code></li> <li>Now: <code>LEVEL_INFORMATIONAL</code></li> </ul> </li> <li>Scope values are now prefixed with <code>SCOPE_</code><ul> <li>Before: <code>SYSTEM</code></li> <li>Now: <code>SCOPE_SYSTEM</code></li> </ul> </li> <li>Group values are now prefixed with <code>GROUP_</code><ul> <li>Before: <code>NEW_VULNERABILITY</code></li> <li>Now: <code>GROUP_NEW_VULNERABILITY</code></li> </ul> </li> <li>The <code>timestamp</code> value passed to notification templates is now consistently formatted with three fractional digits.<ul> <li>Before, any of:<ul> <li><code>1970-01-01T00:11:06Z</code></li> <li><code>1970-01-01T00:11:06.000Z</code></li> <li><code>1970-01-01T00:11:06.000000Z</code></li> <li><code>1970-01-01T00:11:06.000000000Z</code></li> </ul> </li> <li>Now: <code>1970-01-01T00:11:06.000Z</code></li> </ul> </li> </ul>"},{"location":"getting-started/changes-over-v4/#search","title":"Search","text":"<ul> <li>The API server no longer maintains Lucene indexes.<ul> <li>The local <code>~/.dependency-track/index</code> directory is no longer required.</li> </ul> </li> <li>All REST endpoints under <code>/api/v1/search</code> were removed.</li> <li>Fuzzy matching for the internal analyzer is no longer supported.</li> </ul>"},{"location":"getting-started/changes-over-v4/#findings","title":"Findings","text":"<ul> <li>The Findings response object's <code>vulnerability</code> will no longer contain two fields below, <code>cwes</code> will hold the respective ids. <ul> <li><code>cweId</code></li> <li><code>cweName</code></li> </ul> </li> <li>In the SARIF file (schema defined in sarif.peb), <code>cweId</code> will be replaced by list of cwe ids in <code>cwes</code>. And name of the SARIF rule will be vulnerability's <code>vulnId</code> instead of <code>cweName</code>.</li> <li>The <code>/api/v1/finding/project/{uuid}</code> REST API endpoint now supports pagination   apiserver/#1111. The page size defaults to <code>100</code>.     Clients currently expecting all items to be returned at once must be updated to deal with pagination.</li> </ul>"},{"location":"getting-started/migrating-from-v4/","title":"Migrating from v4","text":""},{"location":"getting-started/migrating-from-v4/#introduction","title":"Introduction","text":"<p>If you're currently running a Dependency-Track v4 deployment, don't worry!</p> <p>We're aiming to provide tooling and guides on how to migrate to v5 once it reaches general availability. The goal is to offer tools that perform the migration automatically, with little to no manual effort (except the provisioning of infrastructure of course).</p> <p>Tip</p> <p>Follow https://github.com/DependencyTrack/hyades/issues/881 for updates on this topic.</p>"},{"location":"getting-started/migrating-from-v4/#running-v4-and-v5-in-parallel","title":"Running v4 and v5 in parallel","text":"<p>Given an existing production deployment of v4, it can be helpful to run a v5 test deployment in parallel, to compare behavior and testing new features on real data.</p> <p>This can be achieved by leveraging notifications, in particular <code>BOM_PROCESSED</code> notifications. They are emitted by Dependency-Track after a BOM's contents are synchronized with the database.</p> <p>The subject of <code>BOM_PROCESSED</code> notifications contains the original BOM that way uploaded, encoded in Base64. It also contains the name and version of the project it was uploaded to. This information is sufficient to  construct a BOM upload request, that can be submitted to another Dependency-Track instance.</p> <p>All that's needed is an application that can:</p> <ul> <li>Receive Webhooks, and parse the JSON payload within them</li> <li>Perform a mapping from notification subject, to BOM upload request</li> <li>Forward the BOM upload request to another Dependency-Track instance</li> </ul> <p>This can, of course, be scripted. However, we recommend using Bento, which reduces it all to a single config file.</p> <p>Tip</p> <p>You can use the same approach outlined here to construct a pre-prod / staging environment.</p> <p>Conceptually, this is what the setup will accomplish:</p> <pre><code>sequenceDiagram\n    Client-&gt;&gt;DT v4: Upload BOM&lt;br/&gt;PUT /api/v1/bom\n    DT v4-&gt;&gt;DT v4: Validate and&lt;br/&gt;Process\n    DT v4-&gt;&gt;Bento: Notification&lt;br/&gt;BOM_PROCESSED\n    Bento-&gt;&gt;Bento: Map to BOM&lt;br/&gt;upload request\n    Bento-&gt;&gt;DT v5: Upload BOM&lt;br/&gt;PUT /api/v1/bom\n    DT v5-&gt;&gt;DT v5: Validate and&lt;br/&gt;Process</code></pre>"},{"location":"getting-started/migrating-from-v4/#creating-api-key","title":"Creating API Key","text":"<p>In order to upload BOMs to the Dependency-Track v5 system, an API key with <code>BOM_UPLOAD</code> and <code>PROJECT_CREATION_UPLOAD</code> permissions is required. Log into your Dependency-Track v5 instance, navigate to Administration -&gt; Access Management -&gt; Teams, and create a new team with accompanying API key:</p> <p></p>"},{"location":"getting-started/migrating-from-v4/#deploy-bento","title":"Deploy Bento","text":"<p>Bento works with the concept of pipelines, which are configured via YAML. The following pipeline will achieve the desired outcome:</p> <pre><code>---\ninput:\n  http_server:\n    path: /notification/bom-processed\n    allowed_verbs:\n    - POST\n    timeout: 5s\n    sync_response:\n      status: \"202\"\n\npipeline:\n  processors:\n  - mapping: |\n      root.projectName = this.notification.subject.project.name\n      root.projectVersion = this.notification.subject.project.version\n      root.projectTags = this.notification.subject.project.tags.split(\",\").catch([]).map_each(tag -&gt; {\"name\": tag})\n      root.bom = this.notification.subject.bom.content\n      root.autoCreate = true\n\noutput:\n  http_client:\n    url: \"${DTV5_API_URL}/api/v1/bom\"\n    verb: PUT\n    headers:\n      Content-Type: application/json\n      X-Api-Key: \"${DTV5_API_KEY}\"\n    max_in_flight: 10\n    # tls:\n    #  skip_cert_verify: true\n    #  ^-- Uncomment this if you're using self-signed certificates.\n</code></pre> <p>Refer to the respective pipeline component's documentation for more details:</p> <ul> <li><code>http_server</code> input</li> <li><code>mapping</code> processor</li> <li><code>http_client</code> output</li> </ul> <p>Run Bento as container:</p> <pre><code>docker run -d --name bento \\\n    -p \"4195:4195\" \\\n    -v \"$(pwd)/config.yaml:/bento.yaml\" \\\n    -e 'DTV5_API_URL=https://dtv5.example.com' \\\n    -e 'DTV5_API_KEY=odt_****************' \\\n    ghcr.io/warpstreamlabs/bento\n</code></pre>"},{"location":"getting-started/migrating-from-v4/#configure-notification","title":"Configure Notification","text":"<p>Log into your Dependency-Track v4 instance, navigate to Administration -&gt; Notifications -&gt; Alerts, and create a new alert with the following settings:</p> <ul> <li>Scope: Portfolio</li> <li>Notification level: Informational</li> <li>Publisher: Outbound Webhook</li> </ul> <p></p> <p>Once created, enable <code>BOM_PROCESSED</code> under Groups, and configure the URL of your Bento endpoint as Destination:</p> <p></p>"},{"location":"getting-started/migrating-from-v4/#testing","title":"Testing","text":"<ul> <li>Upload a BOM to a project in your Dependency-Track v4 instance.</li> <li>Head over to your Dependency-Track v5 instance and wait for the upload to replicate.</li> </ul> <p>If all goes well, you're done! Happy testing!</p> <p>Tip</p> <p>If the BOM upload does not replicate:</p> <ol> <li>Check the logs of your Dependency-Track v4 deployment for any errors during notification publishing.</li> <li>Check the logs of Bento for any errors or warnings.</li> <li>Check the logs of your Dependency-Track v5 deployment for any errors during BOM processing.</li> <li>Ensure that the API key you created has the correct permissions.</li> <li>Ensure that Bento is reachable from your Dependency-Track v4 deployment.</li> <li>Ensure that your Dependency-Track v5 deployment is reachable from Bento.</li> </ol>"},{"location":"getting-started/upgrading/","title":"Upgrading","text":""},{"location":"getting-started/upgrading/#upgrading-to-060","title":"Upgrading to 0.6.0","text":"<ul> <li>The <code>kafka.topic.prefix</code> configuration was renamed to <code>dt.kafka.topic.prefix</code> to prevent collisions with native Kafka properties (hyades/#1392).</li> <li> <p>Configuration names for task cron expressions and lock durations have changed (apiserver/#840). They now follow a consistent <code>task.&lt;task-name&gt;.&lt;config&gt;</code> scheme. Lock durations are now specified in ISO 8601 format instead of milliseconds. Refer to the task scheduling configuration reference for details. Example of name change:</p> Before After <code>task.cron.metrics.portfolio</code> <code>task.portfolio.metrics.update.cron</code> <code>task.metrics.portfolio.lockAtMostForInMillis</code> <code>task.portfolio.metrics.update.lock.max.duration</code> <code>task.metrics.portfolio.lockAtLeastForInMillis</code> <code>task.portfolio.metrics.update.lock.min.duration</code> </li> <li> <p>The <code>/api/v1/vulnerability/source/{source}/vuln/{vuln}/projects</code> REST API endpoint now supports pagination (apiserver/#888). Like all other paginated endpoints, the page size defaults to <code>100</code>. Clients currently expecting all items to be returned at once must be updated to deal with pagination.</p> </li> <li> <p>The <code>alpine.</code> prefix was removed from Kafka processor properties of the API server (apiserver/#904). Refer to the kafka configuration reference for details. Example of name change:</p> Before After <code>alpine.kafka.processor.vuln.scan.result.processing.order</code> <code>kafka.processor.vuln.scan.result.processing.order</code> </li> <li> <p>The endpoints deprecated in v4.x mentioned below were removed (apiserver/#910):</p> Removed endpoint Replacement <code>POST /api/v1/policy/{policyUuid}/tag/{tagName}</code> <code>POST /api/v1/tag/{name}/policy</code> <code>DELETE /api/v1/policy/{policyUuid}/tag/{tagName}</code> <code>DELETE /api/v1/tag/{name}/policy</code> <code>GET /api/v1/tag/{policyUuid}</code> <code>GET /api/v1/tag/policy/{uuid}</code> <code>GET /api/v1/bom/token/{uuid}</code> <code>GET /api/v1/event/token/{uuid}</code> </li> <li> <p>The minimum supported PostgreSQL version has been raised from 11 to 13 (hyades/#1724).   Lower versions may still work, but are no longer tested against.</p> </li> <li> <p>User records in the database are consolidated from the separate <code>LDAPUSER</code>, <code>MANAGEDUSER</code>, and <code>OIDCUSER</code> tables, into a single <code>USER</code> table (apiserver/#1169). The new <code>USER</code> table enforces uniqueness of usernames. To prevent data loss, <code>LDAPUSER</code> and <code>OIDCUSER</code> records with conflicting usernames will have their username values suffixed with <code>-CONFLICT-LDAP</code> and <code>-CONFLICT-OIDC</code> respectively. Affected users will not be able to authenticate. Administrators are expected to resolve this by removing users or renaming them as desired. Note that this is an edge case and should not affect the vast majority of deployments.</p> </li> <li> <p>The metrics tables <code>DEPENDENCYMETRICS</code>, <code>PORTFOLIOMETRICS</code>, and <code>PROJECTMETRICS</code> are partitioned by date (apiserver/#1141). The migration procedure involves copying existing  metrics data, thus requiring up to double the amount of storage for the duration of the migration. To reduce the amount of data being copied, consider temporarily reducing the metrics retention timespan in the administration panel under Configuration \u2192 Maintenance. Only historic data falling within the configured retention duration will be migrated.</p> </li> </ul>"},{"location":"operations/database/","title":"Database","text":"<p>Dependency-Track requires a PostgreSQL, or PostgreSQL-compatible database to operate.</p> <p>The lowest supported version is 13. You are encouraged to use the newest available version.</p> <p>Depending on available resources, individual preferences, or organizational policies, you will have to choose between a managed, or self-hosted solution.</p>"},{"location":"operations/database/#extensions","title":"Extensions","text":"<p>The following PostgreSQL extensions are required by Dependency-Track. When choosing a hosting solution, verify that the extensions listed here are supported.</p> <ul> <li><code>pg_trgm</code>: Support for similarity of text using trigram matching</li> </ul> <p>Note</p> <p>Dependency-Track will execute the necessary <code>CREATE EXTENSION IF NOT EXISTS</code> statements during schema migration. Enabling extensions manually is not necessary.</p> <p>Generally, we limit usage of extensions to those that:</p> <ol> <li>Ship with PostgreSQL out-of-the-box</li> <li>Are trusted by default</li> </ol> <p>This ensures compatibility with most managed solutions, and reduces setup effort for self-hosted deployments.</p>"},{"location":"operations/database/#managed-solutions","title":"Managed Solutions","text":"<p>The official PostgreSQL website hosts a list of well-known commercial hosting providers.</p> <p>Popular choices include:</p> <ul> <li>Amazon RDS for PostgreSQL</li> <li>Aiven for PostgreSQL</li> <li>Azure Database for PostgreSQL</li> <li>Google Cloud SQL for PostgreSQL</li> </ul> <p>We are not actively testing against cloud offerings. But as a rule of thumb, solutions offering \"vanilla\" PostgreSQL,  or extensions of it (for example Neon or Timescale), will most definitely work with Dependency-Track.</p> <p>The same is not necessarily true for platforms based on heavily modified PostgreSQL, or even entire re-implementations such as CockroachDB or YugabyteDB. Such solutions make certain trade-offs to achieve higher levels of scalability, which might impact functionality that Dependency-Track relies on. If you'd like to see support for those, please let us know!</p>"},{"location":"operations/database/#self-hosting","title":"Self-Hosting","text":""},{"location":"operations/database/#bare-metal-docker","title":"Bare Metal / Docker","text":"<p>For Docker deployments, use the official <code>postgres</code> image.</p> <p>Warning</p> <p>Do not use the <code>latest</code> tag! You may end up doing a major version upgrade without knowing it, ultimately breaking your database! Pin the tag to at least the major version (e.g. <code>16</code>), or better yet the minor version (e.g. <code>16.2</code>). Refer to Upgrades to upgrade instructions.</p> <p>For bare metal deloyments, it's usually best to install PostgreSQL from your distribution's package repository. See for example:</p> <ul> <li>PostgreSQL instructions for Debian</li> <li>Install and configure PostgreSQL on Ubuntu</li> <li>Using PostgreSQL with Red Hat Enterprise Linux</li> </ul> <p>To get the most out of your Dependency-Track installation, we recommend to run PostgreSQL on a separate machine than the application containers. You want PostgreSQL to be able to leverage the entire machine's resources, without being impacted by other applications.</p> <p>For smaller and non-critical deployments, it is totally fine to run everything on a single machine.</p>"},{"location":"operations/database/#basic-configuration","title":"Basic Configuration","text":"<p>You should be aware that the default PostgreSQL configuration is extremely conservative. It is intended to make PostgreSQL usable on minimal hardware, which is great for testing, but can seriously cripple performance in production environments. Not adjusting it to your specific setup will most certainly leave performance on the table.</p> <p>If you're lucky enough to have access to professional database administrators, ask them for help. They will know your organisation's best practices and can guide you in adjusting it for Dependency-Track.</p> <p>If you're not as lucky, we can wholeheartedly recommend PGTune. Given a bit of basic info about your system, it will provide a sensible baseline configuration. For the DB Type option, select <code>Online transaction processing system</code>.</p> <p></p> <p>The <code>postgresql.conf</code> is usually located at <code>/var/lib/postgresql/data/postgresql.conf</code>. Most of these settings require a restart of the application.</p> <p>In a Docker Compose setup, you can alternatively apply the desired configuration via command line flags. For example:</p> <pre><code>services:\n  postgres:\n    image: postgres:17\n    command: &gt;-\n        -c 'shared_buffers=2GB'\n        -c 'effective_cache_size=6GB'\n</code></pre>"},{"location":"operations/database/#advanced-configuration","title":"Advanced Configuration","text":"<p>For larger deployments, you may eventually run into situations where database performance degrades with just the basic configuration applied. Oftentimes, tweaking advanced settings can resolve such problems. But knowing which knobs to turn is a challenge in itself.</p> <p>If you happen to be in this situation, make sure you have database monitoring set up. Changing advanced configuration options blindly can potentially cause more damage than it helps.</p> <p>Below, you'll find a few options that, based on our observations with large-scale deployments, make sense to tweak. Note that some settings are applied system-wide, while others are only applied for certain tables.</p> <p>Note</p> <p>Got more tips to configure or tune PostgreSQL, that may be helpful to others? We'd love to include it in the docs, please do raise a PR!</p>"},{"location":"operations/database/#checkpoint_completion_target","title":"checkpoint_completion_target","text":"Default <ul> <li><code>0.5</code> (PostgreSQL &lt;= 13)</li> <li><code>0.9</code> (PostgreSQL &gt;= 14)</li> </ul> Recommendation <code>0.9</code> Tables <code>*</code> References Documentation <p>Spreads the WAL checkpoint creation across a longer period of time, resulting in a more evenly distributed I/O load. A lower value has been observed to cause undesirable spikes in I/O usage on the database server.</p> <pre><code>ALTER SYSTEM SET CHECKPOINT_COMPLETION_TARGET = 0.9;\n</code></pre>"},{"location":"operations/database/#autovacuum_vacuum_scale_factor","title":"autovacuum_vacuum_scale_factor","text":"Default <code>0.2</code> Recommendation <code>0.02</code> Tables <ul> <li><code>COMPONENT</code></li> <li><code>DEPENDENCYMETRICS</code></li> </ul> References Documentation <p>The default causes Autovacuum to start way too late on large tables with lots of churn, yielding long execution times. Reduction in scale factor causes autovacuum to happen more often, making each execution less time-intensive.</p> <p>The <code>COMPONENT</code> and <code>DEPENDENCYMETRICS</code> table are very frequently inserted into, updated, and deleted from. This causes lots of dead tuples that PostgreSQL needs to clean up. Because autovacuum also performs <code>ANALYZE</code>, slow vacuuming can cause the query planner to choose inefficient execution plans.</p> <pre><code>ALTER TABLE \"COMPONENT\" SET (AUTOVACUUM_VACUUM_SCALE_FACTOR = 0.02);\nALTER TABLE \"DEPENDENCYMETRICS\" SET (AUTOVACUUM_VACUUM_SCALE_FACTOR = 0.02);\n</code></pre>"},{"location":"operations/database/#upgrades","title":"Upgrades","text":"<p>Follow the official upgrading guide. Be sure to select the version of the documentation that corresponds to the PostgreSQL version you are running.</p> <p>Warning</p> <p>Pay attention to the fact that major version upgrades usually require a backup-and-restore cycle, due to potentially breaking changes in the underlying data storage format. Minor version upgrades are usually safe to perform in a rolling manor.</p>"},{"location":"operations/database/#kubernetes","title":"Kubernetes","text":"<p>We generally advise against running PostgreSQL on Kubernetes, unless you really know what you're doing. Wielding heavy machinery such as Postgres Operator is not something you should do lightheartedly.</p> <p>If you know what you're doing, you definitely don't need advice from us. Smooth sailing! \u2693\ufe0f</p>"},{"location":"operations/database/#schema-migrations","title":"Schema Migrations","text":"<p>Schema migrations are performed automatically by the API server upon startup. It leverages Liquibase for doing so. There is usually no manual action required when upgrading from an older Dependency-Track version, unless explicitly stated otherwise in the release notes.</p> <p>This behavior can be turned off by setting <code>database.run.migrations</code>  on the API server container to <code>false</code>.</p> <p>It is possible to use different credentials for migrations than for the application itself. This can be achieved with the following options:</p> <ul> <li><code>database.migration.url</code></li> <li><code>database.migration.username</code></li> <li><code>database.migration.password</code></li> </ul> <p>The above with default to the main database credentials if not provided explicitly.</p>"},{"location":"reference/topics/","title":"Topics","text":"Name Partitions Config <code>dtrack.notification.analyzer</code> 3 <code>dtrack.notification.bom</code> 3 <code>dtrack.notification.configuration</code> 3 <code>dtrack.notification.datasource-mirroring</code> 3 <code>dtrack.notification.file-system</code> 3 <code>dtrack.notification.integration</code> 3 <code>dtrack.notification.new-vulnerability</code> 3 <code>dtrack.notification.new-vulnerable-dependency</code> 3 <code>dtrack.notification.policy-violation</code> 3 <code>dtrack.notification.project-audit-change</code> 3 <code>dtrack.notification.project-created</code> 3 <code>dtrack.notification.repository</code> 3 <code>dtrack.notification.vex</code> 3 <code>dtrack.notification.user</code> 3 <code>dtrack.notification.project-vuln-analysis-complete</code> <sup>3</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>dtrack.repo-meta-analysis.component</code><sup>1A</sup> 3 <code>dtrack.repo-meta-analysis.result</code> 3 <code>dtrack.vuln-analysis.component</code><sup>1B</sup> 3 <code>dtrack.vuln-analysis.result</code> 3 <code>dtrack.vuln-analysis.result.processed</code> 3 <code>dtrack.vuln-analysis.scanner.result</code><sup>1B</sup> 3 <code>dtrack.vulnerability</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.epss</code> 3 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.digest</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>dtrack.vulnerability.mirror.command</code><sup>2</sup> 1 <code>dtrack.vulnerability.mirror.state</code><sup>2</sup> 1 <code>cleanup.policy=compact</code> <code>hyades-repository-meta-analyzer-command-by-purl-coordinates-repartition</code><sup>1A</sup> 3 <code>hyades-vulnerability-analyzer-completed-scans-table-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-last-update-store-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-expected-scanner-results-table-changelog</code><sup>1B</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-ossindex-batch-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-ossindex-retry-store-changelog</code><sup>1C</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-scan-task-internal-repartition</code> 3 <code>hyades-vulnerability-analyzer-scan-task-ossindex-repartition</code><sup>1C</sup> 3 <code>hyades-vulnerability-analyzer-scan-task-snyk-repartition</code><sup>1D</sup> 3 <code>hyades-vulnerability-analyzer-snyk-batch-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <code>hyades-vulnerability-analyzer-snyk-retry-store-changelog</code><sup>1D</sup> 3 <code>cleanup.policy=compact</code><code>segment.bytes=67108864</code><code>max.compaction.lag.ms=1</code> <p><sup>1X</sup> The topic is subject to co-partitioning requirements <sup>2</sup> The partition number of this topic should not be changed <sup>3</sup> To use this notification, the PROJECT_VULN_ANALYSIS_COMPLETE group needs to be manually added through api call to the notify on condition The put request to be executed is here</p>"},{"location":"reference/topics/#co-partitioning-requirements","title":"Co-Partitioning Requirements","text":"<p>Some topics must be co-partitioned, meaning they must share the exact same number of partitions. Applications using those topics will not work correctly when this is not the case.</p>"},{"location":"reference/configuration/api-server/","title":"API Server","text":""},{"location":"reference/configuration/api-server/#cors","title":"CORS","text":""},{"location":"reference/configuration/api-server/#alpinecorsallowcredentials","title":"alpine.cors.allow.credentials","text":"<p>Controls the content of the <code>Access-Control-Allow-Credentials</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_CORS_ALLOW_CREDENTIALS</code>"},{"location":"reference/configuration/api-server/#alpinecorsallowheaders","title":"alpine.cors.allow.headers","text":"<p>Controls the content of the <code>Access-Control-Allow-Headers</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count, *</code> ENV <code>ALPINE_CORS_ALLOW_HEADERS</code>"},{"location":"reference/configuration/api-server/#alpinecorsallowmethods","title":"alpine.cors.allow.methods","text":"<p>Controls the content of the <code>Access-Control-Allow-Methods</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>GET POST PUT DELETE OPTIONS</code> ENV <code>ALPINE_CORS_ALLOW_METHODS</code>"},{"location":"reference/configuration/api-server/#alpinecorsalloworigin","title":"alpine.cors.allow.origin","text":"<p>Controls the content of the <code>Access-Control-Allow-Origin</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>*</code> ENV <code>ALPINE_CORS_ALLOW_ORIGIN</code>"},{"location":"reference/configuration/api-server/#alpinecorsenabled","title":"alpine.cors.enabled","text":"<p>Defines whether Cross Origin Resource Sharing  (CORS) headers shall be included in REST API responses.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_CORS_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpinecorsexposeheaders","title":"alpine.cors.expose.headers","text":"<p>Controls the content of the <code>Access-Control-Expose-Headers</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>string</code> Default <code>Origin, Content-Type, Authorization, X-Requested-With, Content-Length, Accept, Origin, X-Api-Key, X-Total-Count</code> ENV <code>ALPINE_CORS_EXPOSE_HEADERS</code>"},{"location":"reference/configuration/api-server/#alpinecorsmaxage","title":"alpine.cors.max.age","text":"<p>Controls the content of the <code>Access-Control-Max-Age</code> response header.    Has no effect when <code>alpine.cors.enabled</code> is <code>false</code>.  </p> Required false Type <code>integer</code> Default <code>3600</code> ENV <code>ALPINE_CORS_MAX_AGE</code>"},{"location":"reference/configuration/api-server/#database","title":"Database","text":""},{"location":"reference/configuration/api-server/#alpinedatabasepassword","title":"alpine.database.password","text":"<p>Specifies the password to use when authenticating to the database.  </p> Required false Type <code>string</code> Default <code>dtrack</code> ENV <code>ALPINE_DATABASE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepasswordfile","title":"alpine.database.password.file","text":"<p>Specifies the file to load the database password from.  If set, takes precedence over <code>alpine.database.password</code>.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>/var/run/secrets/database-password</code> ENV <code>ALPINE_DATABASE_PASSWORD_FILE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolenabled","title":"alpine.database.pool.enabled","text":"<p>Specifies if the database connection pool is enabled.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>ALPINE_DATABASE_POOL_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolidletimeout","title":"alpine.database.pool.idle.timeout","text":"<p>This property controls the maximum amount of time that a connection is  allowed to sit idle in the pool.  </p> Required false Type <code>integer</code> Default <code>300000</code> ENV <code>ALPINE_DATABASE_POOL_IDLE_TIMEOUT</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolmaxlifetime","title":"alpine.database.pool.max.lifetime","text":"<p>This property controls the maximum lifetime of a connection in the pool.  An in-use connection will never be retired, only when it is closed will  it then be removed.  </p> Required false Type <code>integer</code> Default <code>600000</code> ENV <code>ALPINE_DATABASE_POOL_MAX_LIFETIME</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolmaxsize","title":"alpine.database.pool.max.size","text":"<p>This property controls the maximum size that the pool is allowed to reach,  including both idle and in-use connections.  </p> Required false Type <code>integer</code> Default <code>20</code> ENV <code>ALPINE_DATABASE_POOL_MAX_SIZE</code>"},{"location":"reference/configuration/api-server/#alpinedatabasepoolminidle","title":"alpine.database.pool.min.idle","text":"<p>This property controls the minimum number of idle connections in the pool.  This value should be equal to or less than <code>alpine.database.pool.max.size</code>.  Warning: If the value is less than <code>alpine.database.pool.max.size</code>,  <code>alpine.database.pool.idle.timeout</code> will have no effect.  </p> Required false Type <code>integer</code> Default <code>10</code> ENV <code>ALPINE_DATABASE_POOL_MIN_IDLE</code>"},{"location":"reference/configuration/api-server/#alpinedatabaseurl","title":"alpine.database.url","text":"<p>Specifies the JDBC URL to use when connecting to the database.  For best performance, set the <code>reWriteBatchedInserts</code> query parameter to <code>true</code>.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>jdbc:postgresql://localhost:5432/dtrack?reWriteBatchedInserts=true</code> ENV <code>ALPINE_DATABASE_URL</code>"},{"location":"reference/configuration/api-server/#alpinedatabaseusername","title":"alpine.database.username","text":"<p>Specifies the username to use when authenticating to the database.  </p> Required false Type <code>string</code> Default <code>dtrack</code> ENV <code>ALPINE_DATABASE_USERNAME</code>"},{"location":"reference/configuration/api-server/#databasemigrationpassword","title":"database.migration.password","text":"<p>Defines the database password for executing migrations.  If not set, the value of <code>alpine.database.password</code> will be used.  </p> Required false Type <code>string</code> Default <code>${alpine.database.password}</code> ENV <code>DATABASE_MIGRATION_PASSWORD</code>"},{"location":"reference/configuration/api-server/#databasemigrationurl","title":"database.migration.url","text":"<p>Defines the database JDBC URL to use when executing migrations.  If not set, the value of <code>alpine.database.url</code> will be used.  Should generally not be set, unless TLS authentication is used,  and custom connection variables are required.  </p> Required false Type <code>string</code> Default <code>${alpine.database.url}</code> ENV <code>DATABASE_MIGRATION_URL</code>"},{"location":"reference/configuration/api-server/#databasemigrationusername","title":"database.migration.username","text":"<p>Defines the database user for executing migrations.  If not set, the value of <code>alpine.database.username</code> will be used.  </p> Required false Type <code>string</code> Default <code>${alpine.database.username}</code> ENV <code>DATABASE_MIGRATION_USERNAME</code>"},{"location":"reference/configuration/api-server/#databaserunmigrations","title":"database.run.migrations","text":"<p>Defines whether database migrations should be executed on startup.    From v5.6.0 onwards, migrations are considered part of the initialization tasks.  Setting <code>init.tasks.enabled</code> to <code>false</code> will disable migrations,  even if <code>database.run.migrations</code> is enabled.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>DATABASE_RUN_MIGRATIONS</code>"},{"location":"reference/configuration/api-server/#databaserunmigrationsonly","title":"database.run.migrations.only","text":"<p>Defines whether the application should exit upon successful execution of database migrations.  Enabling this option makes the application suitable for running as k8s init container.  Has no effect unless <code>database.run.migrations</code> is <code>true</code>.    From v5.6.0 onwards, usage of <code>init.and.exit</code> should be preferred.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>DATABASE_RUN_MIGRATIONS_ONLY</code>"},{"location":"reference/configuration/api-server/#development","title":"Development","text":""},{"location":"reference/configuration/api-server/#devservicesenabled","title":"dev.services.enabled","text":"<p>Whether dev services shall be enabled.    When enabled, Dependency-Track will automatically launch containers for:  <ul> <li>Frontend</li> <li>Kafka</li> <li>PostgreSQL</li> </ul>  at startup, and configures itself to use them. They are disposed when  Dependency-Track stops. The containers are exposed on randomized ports,  which will be logged during startup.    Trying to enable dev services in a production build will prevent  the application from starting.    Note that the containers launched by the API server can not currently  be discovered and re-used by other Hyades services. This is a future  enhancement tracked in https://github.com/DependencyTrack/hyades/issues/1188.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>DEV_SERVICES_ENABLED</code>"},{"location":"reference/configuration/api-server/#devservicesimagefrontend","title":"dev.services.image.frontend","text":"<p>The image to use for the frontend dev services container.  </p> Required false Type <code>string</code> Default <code>ghcr.io/dependencytrack/hyades-frontend:snapshot</code> ENV <code>DEV_SERVICES_IMAGE_FRONTEND</code>"},{"location":"reference/configuration/api-server/#devservicesimagekafka","title":"dev.services.image.kafka","text":"<p>The image to use for the Kafka dev services container.  </p> Required false Type <code>string</code> Default <code>apache/kafka-native:3.9.0</code> ENV <code>DEV_SERVICES_IMAGE_KAFKA</code>"},{"location":"reference/configuration/api-server/#devservicesimagepostgres","title":"dev.services.image.postgres","text":"<p>The image to use for the PostgreSQL dev services container.  </p> Required false Type <code>string</code> Default <code>postgres:13-alpine</code> ENV <code>DEV_SERVICES_IMAGE_POSTGRES</code>"},{"location":"reference/configuration/api-server/#general","title":"General","text":""},{"location":"reference/configuration/api-server/#alpineapikeyprefix","title":"alpine.api.key.prefix","text":"<p>Defines the prefix to be used for API keys. A maximum prefix length of 251  characters is supported. The prefix may also be left empty.  </p> Required false Type <code>string</code> Default <code>odt_</code> ENV <code>ALPINE_API_KEY_PREFIX</code>"},{"location":"reference/configuration/api-server/#alpineauthjwtttlseconds","title":"alpine.auth.jwt.ttl.seconds","text":"<p>Defines the number of seconds for which JWTs issued by Dependency-Track will be valid for.  </p> Required false Type <code>integer</code> Default <code>604800</code> ENV <code>ALPINE_AUTH_JWT_TTL_SECONDS</code>"},{"location":"reference/configuration/api-server/#alpinebcryptrounds","title":"alpine.bcrypt.rounds","text":"<p>Specifies the number of bcrypt rounds to use when hashing a user's password.  The higher the number the more secure the password, at the expense of  hardware resources and additional time to generate the hash.  </p> Required true Type <code>integer</code> Default <code>14</code> ENV <code>ALPINE_BCRYPT_ROUNDS</code>"},{"location":"reference/configuration/api-server/#alpinedatadirectory","title":"alpine.data.directory","text":"<p>Defines the path to the data directory. This directory will hold logs,  keys, and any database or index files along with application-specific  files or directories.  </p> Required true Type <code>string</code> Default <code>~/.dependency-track</code> ENV <code>ALPINE_DATA_DIRECTORY</code>"},{"location":"reference/configuration/api-server/#alpineprivatekeypath","title":"alpine.private.key.path","text":"<p>Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.  The keys will be generated upon first startup if they do not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/private.key</code> Example <code>/var/run/secrets/private.key</code> ENV <code>ALPINE_PRIVATE_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#alpinepublickeypath","title":"alpine.public.key.path","text":"<p>Defines the paths to the public-private key pair to be used for signing and verifying digital signatures.  The keys will be generated upon first startup if they do not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/public.key</code> Example <code>/var/run/secrets/public.key</code> ENV <code>ALPINE_PUBLIC_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#alpinesecretkeypath","title":"alpine.secret.key.path","text":"<p>Defines the path to the secret key to be used for data encryption and decryption.  The key will be generated upon first startup if it does not exist.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/keys/secret.key</code> ENV <code>ALPINE_SECRET_KEY_PATH</code>"},{"location":"reference/configuration/api-server/#initandexit","title":"init.and.exit","text":"<p>Whether to only execute initialization tasks and exit.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INIT_AND_EXIT</code>"},{"location":"reference/configuration/api-server/#inittasksenabled","title":"init.tasks.enabled","text":"<p>Whether to execute initialization tasks on startup.  Initialization tasks include:  <ul> <li>Execution of database migrations</li> <li>Populating the database with default objects (permissions, users, licenses, etc.)</li> </ul> </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>INIT_TASKS_ENABLED</code>"},{"location":"reference/configuration/api-server/#integritycheckenabled","title":"integrity.check.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INTEGRITY_CHECK_ENABLED</code>"},{"location":"reference/configuration/api-server/#integrityinitializerenabled","title":"integrity.initializer.enabled","text":"<p>Specifies whether the Integrity Initializer shall be enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>INTEGRITY_INITIALIZER_ENABLED</code>"},{"location":"reference/configuration/api-server/#tmpdelaybomprocessednotification","title":"tmp.delay.bom.processed.notification","text":"<p>Delays the BOM_PROCESSED notification until the vulnerability analysis associated with a given BOM upload  is completed. The intention being that it is then \"safe\" to query the API for any identified vulnerabilities.  This is specifically for cases where polling the /api/v1/bom/token/ endpoint is not feasible.  THIS IS A TEMPORARY FUNCTIONALITY AND MAY BE REMOVED IN FUTURE RELEASES WITHOUT FURTHER NOTICE.   Required false Type <code>boolean</code> Default <code>false</code> ENV <code>TMP_DELAY_BOM_PROCESSED_NOTIFICATION</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicyanalysisenabled","title":"vulnerability.policy.analysis.enabled","text":"<p>Defines whether vulnerability policy analysis is enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>VULNERABILITY_POLICY_ANALYSIS_ENABLED</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleauthpassword","title":"vulnerability.policy.bundle.auth.password","text":"<p>For nginx server, if username and bearer token both are provided, basic auth will be used,  else the auth header will be added based on the not null values  Defines the password to be used for basic authentication against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_AUTH_PASSWORD</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleauthusername","title":"vulnerability.policy.bundle.auth.username","text":"<p>Defines the username to be used for basic authentication against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_AUTH_USERNAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundlebearertoken","title":"vulnerability.policy.bundle.bearer.token","text":"<p>Defines the token to be used as bearerAuth against the service hosting the policy bundle.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_BEARER_TOKEN</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundlesourcetype","title":"vulnerability.policy.bundle.source.type","text":"<p>Defines the type of source from which policy bundles are being fetched from.  Required when <code>vulnerability.policy.bundle.url</code> is set.  </p> Required false Type <code>enum</code> Valid Values <code>[nginx, s3]</code> Default <code>NGINX</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_SOURCE_TYPE</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicybundleurl","title":"vulnerability.policy.bundle.url","text":"<p>Defines where to fetch the policy bundle from.For S3, just the base url needs to be provided with port  For nginx, the whole url with bundle name needs to be given  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>http://example.com:80/bundles/bundle.zip</code> ENV <code>VULNERABILITY_POLICY_BUNDLE_URL</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3accesskey","title":"vulnerability.policy.s3.access.key","text":"<p>S3 related details. Access key, secret key, bucket name and bundle names are mandatory if S3 is chosen. Region is optional  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_ACCESS_KEY</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3bucketname","title":"vulnerability.policy.s3.bucket.name","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_BUCKET_NAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3bundlename","title":"vulnerability.policy.s3.bundle.name","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_BUNDLE_NAME</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3region","title":"vulnerability.policy.s3.region","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_REGION</code>"},{"location":"reference/configuration/api-server/#vulnerabilitypolicys3secretkey","title":"vulnerability.policy.s3.secret.key","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>VULNERABILITY_POLICY_S3_SECRET_KEY</code>"},{"location":"reference/configuration/api-server/#http","title":"HTTP","text":""},{"location":"reference/configuration/api-server/#alpinehttpproxyaddress","title":"alpine.http.proxy.address","text":"<p>HTTP proxy address. If set, then <code>alpine.http.proxy.port</code> must be set too.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>proxy.example.com</code> ENV <code>ALPINE_HTTP_PROXY_ADDRESS</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxypassword","title":"alpine.http.proxy.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_HTTP_PROXY_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxypasswordfile","title":"alpine.http.proxy.password.file","text":"<p>Specifies the file to load the HTTP proxy password from.  If set, takes precedence over <code>alpine.http.proxy.password</code>.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>/var/run/secrets/http-proxy-password</code> ENV <code>ALPINE_HTTP_PROXY_PASSWORD_FILE</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxyport","title":"alpine.http.proxy.port","text":"Required false Type <code>integer</code> Default <code>null</code> Example <code>8888</code> ENV <code>ALPINE_HTTP_PROXY_PORT</code>"},{"location":"reference/configuration/api-server/#alpinehttpproxyusername","title":"alpine.http.proxy.username","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_HTTP_PROXY_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutconnection","title":"alpine.http.timeout.connection","text":"<p>Defines the connection timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>30</code> ENV <code>ALPINE_HTTP_TIMEOUT_CONNECTION</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutpool","title":"alpine.http.timeout.pool","text":"<p>Defines the request timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>60</code> ENV <code>ALPINE_HTTP_TIMEOUT_POOL</code>"},{"location":"reference/configuration/api-server/#alpinehttptimeoutsocket","title":"alpine.http.timeout.socket","text":"<p>Defines the socket / read timeout in seconds for outbound HTTP connections.  </p> Required false Type <code>integer</code> Default <code>30</code> ENV <code>ALPINE_HTTP_TIMEOUT_SOCKET</code>"},{"location":"reference/configuration/api-server/#alpinenoproxy","title":"alpine.no.proxy","text":"Required false Type <code>string</code> Default <code>null</code> Example <code>localhost,127.0.0.1</code> ENV <code>ALPINE_NO_PROXY</code>"},{"location":"reference/configuration/api-server/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/api-server/#dtkafkatopicprefix","title":"dt.kafka.topic.prefix","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>DT_KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/api-server/#kafkaautooffsetreset","title":"kafka.auto.offset.reset","text":"Required false Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"Required true Type <code>string</code> Default <code>null</code> Example <code>localhost:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/api-server/#kafkakeystorepassword","title":"kafka.keystore.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_KEYSTORE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#kafkakeystorepath","title":"kafka.keystore.path","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_KEYSTORE_PATH</code>"},{"location":"reference/configuration/api-server/#kafkamtlsenabled","title":"kafka.mtls.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>KAFKA_MTLS_ENABLED</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorconsumerautooffsetreset","title":"kafka.processor.epss.mirror.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorconsumergroupid","title":"kafka.processor.epss.mirror.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrormaxbatchsize","title":"kafka.processor.epss.mirror.max.batch.size","text":"Required true Type <code>integer</code> Default <code>500</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_MAX_BATCH_SIZE</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrormaxconcurrency","title":"kafka.processor.epss.mirror.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorprocessingorder","title":"kafka.processor.epss.mirror.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorretryinitialdelayms","title":"kafka.processor.epss.mirror.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorretrymaxdelayms","title":"kafka.processor.epss.mirror.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorretrymultiplier","title":"kafka.processor.epss.mirror.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorepssmirrorretryrandomizationfactor","title":"kafka.processor.epss.mirror.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_EPSS_MIRROR_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultconsumerautooffsetreset","title":"kafka.processor.repo.meta.analysis.result.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultconsumergroupid","title":"kafka.processor.repo.meta.analysis.result.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultmaxconcurrency","title":"kafka.processor.repo.meta.analysis.result.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultprocessingorder","title":"kafka.processor.repo.meta.analysis.result.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultretryinitialdelayms","title":"kafka.processor.repo.meta.analysis.result.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultretrymaxdelayms","title":"kafka.processor.repo.meta.analysis.result.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultretrymultiplier","title":"kafka.processor.repo.meta.analysis.result.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorrepometaanalysisresultretryrandomizationfactor","title":"kafka.processor.repo.meta.analysis.result.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_REPO_META_ANALYSIS_RESULT_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorconsumerautooffsetreset","title":"kafka.processor.vuln.mirror.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorconsumergroupid","title":"kafka.processor.vuln.mirror.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrormaxconcurrency","title":"kafka.processor.vuln.mirror.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorprocessingorder","title":"kafka.processor.vuln.mirror.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>partition</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorretryinitialdelayms","title":"kafka.processor.vuln.mirror.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorretrymaxdelayms","title":"kafka.processor.vuln.mirror.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorretrymultiplier","title":"kafka.processor.vuln.mirror.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnmirrorretryrandomizationfactor","title":"kafka.processor.vuln.mirror.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_VULN_MIRROR_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultconsumerautooffsetreset","title":"kafka.processor.vuln.scan.result.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultconsumergroupid","title":"kafka.processor.vuln.scan.result.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultmaxconcurrency","title":"kafka.processor.vuln.scan.result.max.concurrency","text":"Required true Type <code>integer</code> Default <code>-1</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedconsumerautooffsetreset","title":"kafka.processor.vuln.scan.result.processed.consumer.auto.offset.reset","text":"Required true Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedconsumerfetchminbytes","title":"kafka.processor.vuln.scan.result.processed.consumer.fetch.min.bytes","text":"Required true Type <code>integer</code> Default <code>524288</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_FETCH_MIN_BYTES</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedconsumergroupid","title":"kafka.processor.vuln.scan.result.processed.consumer.group.id","text":"Required true Type <code>string</code> Default <code>dtrack-apiserver-processor</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_GROUP_ID</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedconsumermaxpollrecords","title":"kafka.processor.vuln.scan.result.processed.consumer.max.poll.records","text":"Required true Type <code>integer</code> Default <code>10000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_CONSUMER_MAX_POLL_RECORDS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedmaxbatchsize","title":"kafka.processor.vuln.scan.result.processed.max.batch.size","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_MAX_BATCH_SIZE</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedmaxconcurrency","title":"kafka.processor.vuln.scan.result.processed.max.concurrency","text":"Required true Type <code>integer</code> Default <code>1</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_MAX_CONCURRENCY</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedprocessingorder","title":"kafka.processor.vuln.scan.result.processed.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>unordered</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedretryinitialdelayms","title":"kafka.processor.vuln.scan.result.processed.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>3000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedretrymaxdelayms","title":"kafka.processor.vuln.scan.result.processed.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedretrymultiplier","title":"kafka.processor.vuln.scan.result.processed.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessedretryrandomizationfactor","title":"kafka.processor.vuln.scan.result.processed.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSED_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultprocessingorder","title":"kafka.processor.vuln.scan.result.processing.order","text":"Required true Type <code>enum</code> Valid Values <code>[key, partition, unordered]</code> Default <code>key</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_PROCESSING_ORDER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultretryinitialdelayms","title":"kafka.processor.vuln.scan.result.retry.initial.delay.ms","text":"Required true Type <code>integer</code> Default <code>1000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_INITIAL_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultretrymaxdelayms","title":"kafka.processor.vuln.scan.result.retry.max.delay.ms","text":"Required true Type <code>integer</code> Default <code>180000</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_MAX_DELAY_MS</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultretrymultiplier","title":"kafka.processor.vuln.scan.result.retry.multiplier","text":"Required true Type <code>integer</code> Default <code>2</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#kafkaprocessorvulnscanresultretryrandomizationfactor","title":"kafka.processor.vuln.scan.result.retry.randomization.factor","text":"Required true Type <code>double</code> Default <code>0.3</code> ENV <code>KAFKA_PROCESSOR_VULN_SCAN_RESULT_RETRY_RANDOMIZATION_FACTOR</code>"},{"location":"reference/configuration/api-server/#kafkasecurityprotocol","title":"kafka.security.protocol","text":"Required false Type <code>enum</code> Valid Values <code>[PLAINTEXT, SASL_SSL_PLAINTEXT, SASL_PLAINTEXT, SSL]</code> Default <code>null</code> ENV <code>KAFKA_SECURITY_PROTOCOL</code>"},{"location":"reference/configuration/api-server/#kafkatlsenabled","title":"kafka.tls.enabled","text":"Required false Type <code>boolean</code> Default <code>false</code> ENV <code>KAFKA_TLS_ENABLED</code>"},{"location":"reference/configuration/api-server/#kafkatruststorepassword","title":"kafka.truststore.password","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TRUSTSTORE_PASSWORD</code>"},{"location":"reference/configuration/api-server/#kafkatruststorepath","title":"kafka.truststore.path","text":"Required false Type <code>string</code> Default <code>null</code> ENV <code>KAFKA_TRUSTSTORE_PATH</code>"},{"location":"reference/configuration/api-server/#ldap","title":"LDAP","text":""},{"location":"reference/configuration/api-server/#alpineldapattributemail","title":"alpine.ldap.attribute.mail","text":"<p>Specifies the LDAP attribute used to store a users email address  </p> Required false Type <code>string</code> Default <code>mail</code> ENV <code>ALPINE_LDAP_ATTRIBUTE_MAIL</code>"},{"location":"reference/configuration/api-server/#alpineldapattributename","title":"alpine.ldap.attribute.name","text":"<p>Specifies the Attribute that identifies a users ID.    Example (Microsoft Active Directory):  <ul><li><code>userPrincipalName</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>uid</code></li></ul> </p> Required false Type <code>string</code> Default <code>userPrincipalName</code> ENV <code>ALPINE_LDAP_ATTRIBUTE_NAME</code>"},{"location":"reference/configuration/api-server/#alpineldapauthusernameformat","title":"alpine.ldap.auth.username.format","text":"<p>Specifies if the username entered during login needs to be formatted prior  to asserting credentials against the directory. For Active Directory, the  userPrincipal attribute typically ends with the domain, whereas the  samAccountName attribute and other directory server implementations do not.  The %s variable will be substituted with the username asserted during login.    Example (Microsoft Active Directory):  <ul><li><code>%s@example.com</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>%s</code></li></ul> </p> Required false Type <code>string</code> Default <code>null</code> Example <code>%s@example.com</code> ENV <code>ALPINE_LDAP_AUTH_USERNAME_FORMAT</code>"},{"location":"reference/configuration/api-server/#alpineldapbasedn","title":"alpine.ldap.basedn","text":"<p>Specifies the base DN that all queries should search from  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>dc=example,dc=com</code> ENV <code>ALPINE_LDAP_BASEDN</code>"},{"location":"reference/configuration/api-server/#alpineldapbindpassword","title":"alpine.ldap.bind.password","text":"<p>If anonymous access is not permitted, specify a password for the username  used to bind.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_BIND_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpineldapbindusername","title":"alpine.ldap.bind.username","text":"<p>If anonymous access is not permitted, specify a username with limited access  to the directory, just enough to perform searches. This should be the fully  qualified DN of the user.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_BIND_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpineldapenabled","title":"alpine.ldap.enabled","text":"<p>Defines if LDAP will be used for user authentication. If enabled,  <code>alpine.ldap.*</code> properties should be set accordingly.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpineldapgroupsfilter","title":"alpine.ldap.groups.filter","text":"<p>Specifies the LDAP search filter used to retrieve all groups from the directory.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=group)(objectCategory=Group))</code> ENV <code>ALPINE_LDAP_GROUPS_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapgroupssearchfilter","title":"alpine.ldap.groups.search.filter","text":"<p>Specifies the LDAP search filter used to search for groups by their name.  The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(cn={SEARCH_TERM}))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames)(cn={SEARCH_TERM}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=group)(objectCategory=Group)(cn=*{SEARCH_TERM}*))</code> ENV <code>ALPINE_LDAP_GROUPS_SEARCH_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapsecurityauth","title":"alpine.ldap.security.auth","text":"<p>Specifies the LDAP security authentication level to use. Its value is one of  the following strings: \"none\", \"simple\", \"strong\". If this property is empty  or unspecified, the behaviour is determined by the service provider.  </p> Required false Type <code>enum</code> Valid Values <code>[none, simple, strong]</code> Default <code>simple</code> ENV <code>ALPINE_LDAP_SECURITY_AUTH</code>"},{"location":"reference/configuration/api-server/#alpineldapserverurl","title":"alpine.ldap.server.url","text":"<p>Specifies the LDAP server URL.    Examples (Microsoft Active Directory):  <ul> <li><code>ldap://ldap.example.com:3268</code></li> <li><code>ldaps://ldap.example.com:3269</code></li> </ul>  Examples (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul> <li><code>ldap://ldap.example.com:389</code></li> <li><code>ldaps://ldap.example.com:636</code></li> </ul> </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_LDAP_SERVER_URL</code>"},{"location":"reference/configuration/api-server/#alpineldapteamsynchronization","title":"alpine.ldap.team.synchronization","text":"<p>This option will ensure that team memberships for LDAP users are dynamic and  synchronized with membership of LDAP groups. When a team is mapped to an LDAP  group, all local LDAP users will automatically be assigned to the team if  they are a member of the group the team is mapped to. If the user is later  removed from the LDAP group, they will also be removed from the team. This  option provides the ability to dynamically control user permissions via an  external directory.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_TEAM_SYNCHRONIZATION</code>"},{"location":"reference/configuration/api-server/#alpineldapusergroupsfilter","title":"alpine.ldap.user.groups.filter","text":"<p>Specifies the LDAP search filter to use to query a user and retrieve a list  of groups the user is a member of. The <code>{USER_DN}</code> variable will be substituted  with the actual value of the users DN at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(member={USER_DN}))</code></li></ul>  Example (Microsoft Active Directory - with nested group support):  <ul><li><code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=groupOfUniqueNames)(uniqueMember={USER_DN}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(member:1.2.840.113556.1.4.1941:={USER_DN})</code> ENV <code>ALPINE_LDAP_USER_GROUPS_FILTER</code>"},{"location":"reference/configuration/api-server/#alpineldapuserprovisioning","title":"alpine.ldap.user.provisioning","text":"<p>Specifies if mapped LDAP accounts are automatically created upon successful  authentication. When a user logs in with valid credentials but an account has  not been previously provisioned, an authentication failure will be returned.  This allows admins to control specifically which ldap users can access the  system and which users cannot. When this value is set to true, a local ldap  user will be created and mapped to the ldap account automatically. This  automatic provisioning only affects authentication, not authorization.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_LDAP_USER_PROVISIONING</code>"},{"location":"reference/configuration/api-server/#alpineldapuserssearchfilter","title":"alpine.ldap.users.search.filter","text":"<p>Specifies the LDAP search filter used to search for users by their name.  The <code>{SEARCH_TERM}</code> variable will be substituted at runtime.    Example (Microsoft Active Directory):  <ul><li><code>(&amp;(objectClass=group)(objectCategory=Group)(cn={SEARCH_TERM}))</code></li></ul>  Example (ApacheDS, Fedora 389 Directory, NetIQ/Novell eDirectory, etc):  <ul><li><code>(&amp;(objectClass=inetOrgPerson)(cn={SEARCH_TERM}))</code></li></ul> </p> Required false Type <code>string</code> Default <code>(&amp;(objectClass=user)(objectCategory=Person)(cn=*{SEARCH_TERM}*))</code> ENV <code>ALPINE_LDAP_USERS_SEARCH_FILTER</code>"},{"location":"reference/configuration/api-server/#observability","title":"Observability","text":""},{"location":"reference/configuration/api-server/#alpinemetricsauthpassword","title":"alpine.metrics.auth.password","text":"<p>Defines the password required to access metrics.  Has no effect when <code>alpine.metrics.auth.username</code> is not set.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_METRICS_AUTH_PASSWORD</code>"},{"location":"reference/configuration/api-server/#alpinemetricsauthusername","title":"alpine.metrics.auth.username","text":"<p>Defines the username required to access metrics.  Has no effect when <code>alpine.metrics.auth.password</code> is not set.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_METRICS_AUTH_USERNAME</code>"},{"location":"reference/configuration/api-server/#alpinemetricsenabled","title":"alpine.metrics.enabled","text":"<p>Defines whether Prometheus metrics will be exposed.  If enabled, metrics will be available via the /metrics endpoint.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_METRICS_ENABLED</code>"},{"location":"reference/configuration/api-server/#openid-connect","title":"OpenID Connect","text":""},{"location":"reference/configuration/api-server/#alpineoidcclientid","title":"alpine.oidc.client.id","text":"<p>Defines the client ID to be used for OpenID Connect.  The client ID should be the same as the one configured for the frontend,  and will only be used to validate ID tokens.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_OIDC_CLIENT_ID</code>"},{"location":"reference/configuration/api-server/#alpineoidcenabled","title":"alpine.oidc.enabled","text":"<p>Defines if OpenID Connect will be used for user authentication.  If enabled, <code>alpine.oidc.*</code> properties should be set accordingly.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_ENABLED</code>"},{"location":"reference/configuration/api-server/#alpineoidcissuer","title":"alpine.oidc.issuer","text":"<p>Defines the issuer URL to be used for OpenID Connect.  This issuer MUST support provider configuration via the <code>/.well-known/openid-configuration</code> endpoint.  See also:  <ul> <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderMetadata</li> <li>https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig</li> </ul> </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>ALPINE_OIDC_ISSUER</code>"},{"location":"reference/configuration/api-server/#alpineoidcteamsynchronization","title":"alpine.oidc.team.synchronization","text":"<p>This option will ensure that team memberships for OpenID Connect users are dynamic and  synchronized with membership of OpenID Connect groups or assigned roles. When a team is  mapped to an OpenID Connect group, all local OpenID Connect users will automatically be  assigned to the team if they are a member of the group the team is mapped to. If the user  is later removed from the OpenID Connect group, they will also be removed from the team. This  option provides the ability to dynamically control user permissions via the identity provider.  Note that team synchronization is only performed during user provisioning and after successful  authentication.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_TEAM_SYNCHRONIZATION</code>"},{"location":"reference/configuration/api-server/#alpineoidcteamsclaim","title":"alpine.oidc.teams.claim","text":"<p>Defines the name of the claim that contains group memberships or role assignments in the provider's userinfo endpoint.  The claim must be an array of strings. Most public identity providers do not support group or role management.  When using a customizable / on-demand hosted identity provider, name, content, and inclusion in the userinfo endpoint  will most likely need to be configured.  </p> Required false Type <code>string</code> Default <code>groups</code> ENV <code>ALPINE_OIDC_TEAMS_CLAIM</code>"},{"location":"reference/configuration/api-server/#alpineoidcuserprovisioning","title":"alpine.oidc.user.provisioning","text":"<p>Specifies if mapped OpenID Connect accounts are automatically created upon successful  authentication. When a user logs in with a valid access token but an account has  not been previously provisioned, an authentication failure will be returned.  This allows admins to control specifically which OpenID Connect users can access the  system and which users cannot. When this value is set to true, a local OpenID Connect  user will be created and mapped to the OpenID Connect account automatically. This  automatic provisioning only affects authentication, not authorization.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>ALPINE_OIDC_USER_PROVISIONING</code>"},{"location":"reference/configuration/api-server/#alpineoidcusernameclaim","title":"alpine.oidc.username.claim","text":"<p>Defines the name of the claim that contains the username in the provider's userinfo endpoint.  Common claims are <code>name</code>, <code>username</code>, <code>preferred_username</code> or <code>nickname</code>.  See also:  <ul> <li>https://openid.net/specs/openid-connect-core-1_0.html#UserInfoResponse</li> </ul> </p> Required false Type <code>string</code> Default <code>name</code> ENV <code>ALPINE_OIDC_USERNAME_CLAIM</code>"},{"location":"reference/configuration/api-server/#storage","title":"Storage","text":""},{"location":"reference/configuration/api-server/#filestoragedefaultextension","title":"file.storage.default.extension","text":"<p>Defines the file storage extension to use.  When not set, an enabled extension will be chosen based on its priority.  It is recommended to explicitly configure an extension for predictable behavior.  </p> Required false Type <code>enum</code> Valid Values <code>[local, memory, s3]</code> Default <code>null</code> ENV <code>FILE_STORAGE_DEFAULT_EXTENSION</code>"},{"location":"reference/configuration/api-server/#filestorageextensionlocalcompressionlevel","title":"file.storage.extension.local.compression.level","text":"<p>Defines the zstd compression level to use.  Has no effect unless <code>file.storage.extension.local.enabled</code> is <code>true</code>.  </p> Required false Type <code>integer</code> Valid Values <code>[-7..22]</code> Default <code>5</code> ENV <code>FILE_STORAGE_EXTENSION_LOCAL_COMPRESSION_LEVEL</code>"},{"location":"reference/configuration/api-server/#filestorageextensionlocalcompressionthresholdbytes","title":"file.storage.extension.local.compression.threshold.bytes","text":"<p>Defines the size threshold for files after which they will be compressed.  Compression is performed using the zstd algorithm.  Has no effect unless <code>file.storage.extension.local.enabled</code> is <code>true</code>.  </p> Required false Type <code>integer</code> Default <code>4096</code> ENV <code>FILE_STORAGE_EXTENSION_LOCAL_COMPRESSION_THRESHOLD_BYTES</code>"},{"location":"reference/configuration/api-server/#filestorageextensionlocaldirectory","title":"file.storage.extension.local.directory","text":"<p>Defines the local directory where files shall be stored.  Has no effect unless <code>file.storage.extension.local.enabled</code> is <code>true</code>.  </p> Required false Type <code>string</code> Default <code>${alpine.data.directory}/storage</code> ENV <code>FILE_STORAGE_EXTENSION_LOCAL_DIRECTORY</code>"},{"location":"reference/configuration/api-server/#filestorageextensionlocalenabled","title":"file.storage.extension.local.enabled","text":"<p>Whether the local file storage extension shall be enabled.  </p> Required false Type <code>boolean</code> Default <code>true</code> ENV <code>FILE_STORAGE_EXTENSION_LOCAL_ENABLED</code>"},{"location":"reference/configuration/api-server/#filestorageextensionmemoryenabled","title":"file.storage.extension.memory.enabled","text":"<p>Whether the in-memory file storage extension shall be enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>FILE_STORAGE_EXTENSION_MEMORY_ENABLED</code>"},{"location":"reference/configuration/api-server/#filestorageextensions3accesskey","title":"file.storage.extension.s3.access.key","text":"<p>Defines the S3 access key / username.  Has no effect unless <code>file.storage.extension.s3.enabled</code> is <code>true</code>.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>FILE_STORAGE_EXTENSION_S3_ACCESS_KEY</code>"},{"location":"reference/configuration/api-server/#filestorageextensions3bucket","title":"file.storage.extension.s3.bucket","text":"<p>Defines the name of the S3 bucket.  The existence of the bucket will be verified during startup,  even when S3 is not configured as default extension.  Has no effect unless <code>file.storage.extension.s3.enabled</code> is <code>true</code>.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>FILE_STORAGE_EXTENSION_S3_BUCKET</code>"},{"location":"reference/configuration/api-server/#filestorageextensions3compressionlevel","title":"file.storage.extension.s3.compression.level","text":"<p>Defines the zstd compression level to use.  Has no effect unless <code>file.storage.extension.s3.enabled</code> is <code>true</code>.  </p> Required false Type <code>integer</code> Valid Values <code>[-7..22]</code> Default <code>5</code> ENV <code>FILE_STORAGE_EXTENSION_S3_COMPRESSION_LEVEL</code>"},{"location":"reference/configuration/api-server/#filestorageextensions3compressionthresholdbytes","title":"file.storage.extension.s3.compression.threshold.bytes","text":"<p>Defines the size threshold for files after which they will be compressed.  Compression is performed using the zstd algorithm.  Has no effect unless <code>file.storage.extension.s3.enabled</code> is <code>true</code>.  </p> Required false Type <code>integer</code> Default <code>4096</code> ENV <code>FILE_STORAGE_EXTENSION_S3_COMPRESSION_THRESHOLD_BYTES</code>"},{"location":"reference/configuration/api-server/#filestorageextensions3enabled","title":"file.storage.extension.s3.enabled","text":"<p>Whether the s3 file storage extension shall be enabled.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>FILE_STORAGE_EXTENSION_S3_ENABLED</code>"},{"location":"reference/configuration/api-server/#filestorageextensions3endpoint","title":"file.storage.extension.s3.endpoint","text":"<p>Defines the S3 endpoint URL.  Has no effect unless <code>file.storage.extension.s3.enabled</code> is <code>true</code>.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>FILE_STORAGE_EXTENSION_S3_ENDPOINT</code>"},{"location":"reference/configuration/api-server/#filestorageextensions3region","title":"file.storage.extension.s3.region","text":"<p>Defines the region of the S3 bucket.  Has no effect unless <code>file.storage.extension.s3.enabled</code> is <code>true</code>.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>FILE_STORAGE_EXTENSION_S3_REGION</code>"},{"location":"reference/configuration/api-server/#filestorageextensions3secretkey","title":"file.storage.extension.s3.secret.key","text":"<p>Defines the S3 secret key / password.  Has no effect unless <code>file.storage.extension.s3.enabled</code> is <code>true</code>.  </p> Required false Type <code>string</code> Default <code>null</code> ENV <code>FILE_STORAGE_EXTENSION_S3_SECRET_KEY</code>"},{"location":"reference/configuration/api-server/#task-execution","title":"Task Execution","text":""},{"location":"reference/configuration/api-server/#alpineworkerthreadmultiplier","title":"alpine.worker.thread.multiplier","text":"<p>Defines a multiplier that is used to calculate the number of threads used  by the event subsystem. This property is only used when <code>alpine.worker.threads</code>  is set to 0. A machine with 4 cores and a multiplier of 4, will use (at most)  16 worker threads.  </p> Required true Type <code>integer</code> Default <code>4</code> ENV <code>ALPINE_WORKER_THREAD_MULTIPLIER</code>"},{"location":"reference/configuration/api-server/#alpineworkerthreads","title":"alpine.worker.threads","text":"<p>Defines the number of worker threads that the event subsystem will consume.  Events occur asynchronously and are processed by the Event subsystem. This  value should be large enough to handle most production situations without  introducing much delay, yet small enough not to pose additional load on an  already resource-constrained server.  A value of 0 will instruct Alpine to allocate 1 thread per CPU core. This  can further be tweaked using the <code>alpine.worker.thread.multiplier</code> property.  </p> Required true Type <code>integer</code> Default <code>0</code> ENV <code>ALPINE_WORKER_THREADS</code>"},{"location":"reference/configuration/api-server/#task-scheduling","title":"Task Scheduling","text":""},{"location":"reference/configuration/api-server/#taskcomponentmetadatamaintenancecron","title":"task.component.metadata.maintenance.cron","text":"<p>Cron expression of the component metadata maintenance task.    The task deletes orphaned records from the <code>INTEGRITY_META_COMPONENT</code> and  <code>REPOSITORY_META_COMPONENT</code> tables.  </p> Required true Type <code>cron</code> Default <code>0 */12 * * *</code> ENV <code>TASK_COMPONENT_METADATA_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskcomponentmetadatamaintenancelockmaxduration","title":"task.component.metadata.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the component metadata maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_COMPONENT_METADATA_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskcomponentmetadatamaintenancelockminduration","title":"task.component.metadata.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the component metadata maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_COMPONENT_METADATA_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskdefectdojouploadcron","title":"task.defect.dojo.upload.cron","text":"<p>Cron expression of the DefectDojo upload task.  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_DEFECT_DOJO_UPLOAD_CRON</code>"},{"location":"reference/configuration/api-server/#taskepssmirrorcron","title":"task.epss.mirror.cron","text":"<p>Cron expression of the EPSS mirroring task.  </p> Required true Type <code>cron</code> Default <code>0 1 * * *</code> ENV <code>TASK_EPSS_MIRROR_CRON</code>"},{"location":"reference/configuration/api-server/#taskfortifysscuploadcron","title":"task.fortify.ssc.upload.cron","text":"<p>Cron expression of the Fortify SSC upload task.  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_FORTIFY_SSC_UPLOAD_CRON</code>"},{"location":"reference/configuration/api-server/#taskgithubadvisorymirrorcron","title":"task.git.hub.advisory.mirror.cron","text":"<p>Cron expression of the vulnerability GitHub Advisories mirroring task.  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_GIT_HUB_ADVISORY_MIRROR_CRON</code>"},{"location":"reference/configuration/api-server/#taskintegritymetainitializercron","title":"task.integrity.meta.initializer.cron","text":"<p>Cron expression of the integrity metadata initializer task.  </p> Required true Type <code>cron</code> Default <code>0 */12 * * *</code> ENV <code>TASK_INTEGRITY_META_INITIALIZER_CRON</code>"},{"location":"reference/configuration/api-server/#taskintegritymetainitializerlockmaxduration","title":"task.integrity.meta.initializer.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the integrity metadata initializer task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_INTEGRITY_META_INITIALIZER_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskintegritymetainitializerlockminduration","title":"task.integrity.meta.initializer.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the integrity metadata initializer task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_INTEGRITY_META_INITIALIZER_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskinternalcomponentidentificationcron","title":"task.internal.component.identification.cron","text":"<p>Cron expression of the internal component identification task.  </p> Required true Type <code>cron</code> Default <code>25 */6 * * *</code> ENV <code>TASK_INTERNAL_COMPONENT_IDENTIFICATION_CRON</code>"},{"location":"reference/configuration/api-server/#taskinternalcomponentidentificationlockmaxduration","title":"task.internal.component.identification.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the internal component identification task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_INTERNAL_COMPONENT_IDENTIFICATION_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskinternalcomponentidentificationlockminduration","title":"task.internal.component.identification.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the internal component identification task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_INTERNAL_COMPONENT_IDENTIFICATION_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskkennasecurityuploadcron","title":"task.kenna.security.upload.cron","text":"<p>Cron expression of the Kenna Security upload task.  </p> Required true Type <code>cron</code> Default <code>0 2 * * *</code> ENV <code>TASK_KENNA_SECURITY_UPLOAD_CRON</code>"},{"location":"reference/configuration/api-server/#taskldapsynccron","title":"task.ldap.sync.cron","text":"<p>Cron expression of the LDAP synchronization task.  </p> Required true Type <code>cron</code> Default <code>0 */6 * * *</code> ENV <code>TASK_LDAP_SYNC_CRON</code>"},{"location":"reference/configuration/api-server/#taskldapsynclockmaxduration","title":"task.ldap.sync.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the LDAP synchronization task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_LDAP_SYNC_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskldapsynclockminduration","title":"task.ldap.sync.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the LDAP synchronization task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_LDAP_SYNC_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskmetricsmaintenancecron","title":"task.metrics.maintenance.cron","text":"<p>Cron expression of the metrics maintenance task.    The task creates new partitions for the day for the following tables  And deletes records older than the configured metrics retention duration from the following tables:  <ul> <li><code>DEPENDENCYMETRICS</code></li> <li><code>PROJECTMETRICS</code></li> <li><code>PORTFOLIOMETRICS</code></li> </ul> </p> Required true Type <code>cron</code> Default <code>1 * * * *</code> ENV <code>TASK_METRICS_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskmetricsmaintenancelockmaxduration","title":"task.metrics.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the metrics maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_METRICS_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskmetricsmaintenancelockminduration","title":"task.metrics.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the metrics maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_METRICS_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#tasknistmirrorcron","title":"task.nist.mirror.cron","text":"<p>Cron expression of the NIST / NVD mirroring task.  </p> Required true Type <code>cron</code> Default <code>0 4 * * *</code> ENV <code>TASK_NIST_MIRROR_CRON</code>"},{"location":"reference/configuration/api-server/#taskosvmirrorcron","title":"task.osv.mirror.cron","text":"<p>Cron expression of the OSV mirroring task.  </p> Required true Type <code>cron</code> Default <code>0 3 * * *</code> ENV <code>TASK_OSV_MIRROR_CRON</code>"},{"location":"reference/configuration/api-server/#taskportfoliometricsupdatecron","title":"task.portfolio.metrics.update.cron","text":"<p>Cron expression of the portfolio metrics update task.  </p> Required true Type <code>cron</code> Default <code>10 * * * *</code> ENV <code>TASK_PORTFOLIO_METRICS_UPDATE_CRON</code>"},{"location":"reference/configuration/api-server/#taskportfoliometricsupdatelockmaxduration","title":"task.portfolio.metrics.update.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the portfolio metrics update task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_PORTFOLIO_METRICS_UPDATE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskportfoliometricsupdatelockminduration","title":"task.portfolio.metrics.update.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the portfolio metrics update task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_PORTFOLIO_METRICS_UPDATE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskprojectmaintenancecron","title":"task.project.maintenance.cron","text":"<p>Cron expression of the project maintenance task.    The task deletes inactive projects based on retention policy.  </p> Required true Type <code>cron</code> Default <code>0 */4 * * *</code> ENV <code>TASK_PROJECT_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskprojectmaintenancelockmaxduration","title":"task.project.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the project maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_PROJECT_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskprojectmaintenancelockminduration","title":"task.project.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the project maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_PROJECT_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskrepositorymetaanalysiscron","title":"task.repository.meta.analysis.cron","text":"<p>Cron expression of the portfolio repository metadata analysis task.  </p> Required true Type <code>cron</code> Default <code>0 1 * * *</code> ENV <code>TASK_REPOSITORY_META_ANALYSIS_CRON</code>"},{"location":"reference/configuration/api-server/#taskrepositorymetaanalysislockmaxduration","title":"task.repository.meta.analysis.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the portfolio repository metadata analysis task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_REPOSITORY_META_ANALYSIS_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskrepositorymetaanalysislockminduration","title":"task.repository.meta.analysis.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the portfolio repository metadata analysis task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_REPOSITORY_META_ANALYSIS_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskschedulerinitialdelay","title":"task.scheduler.initial.delay","text":"<p>Scheduling tasks after 3 minutes (3601000) of starting application  </p> Required true Type <code>integer</code> Default <code>180000</code> ENV <code>TASK_SCHEDULER_INITIAL_DELAY</code>"},{"location":"reference/configuration/api-server/#taskschedulerpollinginterval","title":"task.scheduler.polling.interval","text":"<p>Cron expressions for tasks have the precision of minutes so polling every minute  </p> Required true Type <code>integer</code> Default <code>60000</code> ENV <code>TASK_SCHEDULER_POLLING_INTERVAL</code>"},{"location":"reference/configuration/api-server/#tasktagmaintenancecron","title":"task.tag.maintenance.cron","text":"<p>Cron expression of the tag maintenance task.    The task deletes orphaned tags that are not used anymore.  </p> Required true Type <code>cron</code> Default <code>0 */12 * * *</code> ENV <code>TASK_TAG_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#tasktagmaintenancelockmaxduration","title":"task.tag.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the tag maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_TAG_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#tasktagmaintenancelockminduration","title":"task.tag.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the tag maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_TAG_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityanalysiscron","title":"task.vulnerability.analysis.cron","text":"<p>Cron expression of the portfolio vulnerability analysis task.  </p> Required true Type <code>cron</code> Default <code>0 6 * * *</code> ENV <code>TASK_VULNERABILITY_ANALYSIS_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityanalysislockmaxduration","title":"task.vulnerability.analysis.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the portfolio vulnerability analysis task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_VULNERABILITY_ANALYSIS_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityanalysislockminduration","title":"task.vulnerability.analysis.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the portfolio vulnerability analysis task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_VULNERABILITY_ANALYSIS_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitydatabasemaintenancecron","title":"task.vulnerability.database.maintenance.cron","text":"<p>Cron expression of the vulnerability database maintenance task.    The task deletes orphaned records from the <code>VULNERABLESOFTWARE</code> table.  </p> Required true Type <code>cron</code> Default <code>0 0 * * *</code> ENV <code>TASK_VULNERABILITY_DATABASE_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitydatabasemaintenancelockmaxduration","title":"task.vulnerability.database.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_VULNERABILITY_DATABASE_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitydatabasemaintenancelockminduration","title":"task.vulnerability.database.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_VULNERABILITY_DATABASE_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitymetricsupdatecron","title":"task.vulnerability.metrics.update.cron","text":"<p>Cron expression of the vulnerability metrics update task.  </p> Required true Type <code>cron</code> Default <code>40 * * * *</code> ENV <code>TASK_VULNERABILITY_METRICS_UPDATE_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitymetricsupdatelockmaxduration","title":"task.vulnerability.metrics.update.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the vulnerability metrics update task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_VULNERABILITY_METRICS_UPDATE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitymetricsupdatelockminduration","title":"task.vulnerability.metrics.update.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the vulnerability metrics update task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT90S</code> ENV <code>TASK_VULNERABILITY_METRICS_UPDATE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitypolicyfetchcron","title":"task.vulnerability.policy.fetch.cron","text":"<p>Cron expression of the vulnerability policy bundle fetch task.  </p> Required true Type <code>cron</code> Default <code>*/5 * * * *</code> ENV <code>TASK_VULNERABILITY_POLICY_FETCH_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitypolicyfetchlockmaxduration","title":"task.vulnerability.policy.fetch.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the vulnerability policy bundle fetch task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT5M</code> ENV <code>TASK_VULNERABILITY_POLICY_FETCH_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilitypolicyfetchlockminduration","title":"task.vulnerability.policy.fetch.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the vulnerability policy bundle fetch task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT5S</code> ENV <code>TASK_VULNERABILITY_POLICY_FETCH_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityscanmaintenancecron","title":"task.vulnerability.scan.maintenance.cron","text":"<p>Cron expression of the vulnerability scan maintenance task.    The task deletes records older than the configured retention duration from the <code>VULNERABILITYSCAN</code> table.  </p> Required true Type <code>cron</code> Default <code>0 * * * *</code> ENV <code>TASK_VULNERABILITY_SCAN_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityscanmaintenancelockmaxduration","title":"task.vulnerability.scan.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT15M</code> ENV <code>TASK_VULNERABILITY_SCAN_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskvulnerabilityscanmaintenancelockminduration","title":"task.vulnerability.scan.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the vulnerability database maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_VULNERABILITY_SCAN_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/api-server/#taskworkflowmaintenancecron","title":"task.workflow.maintenance.cron","text":"<p>Cron expression of the workflow maintenance task.    The task:  <ul> <li>Transitions workflow steps from <code>PENDING</code> to <code>TIMED_OUT</code> state</li> <li>Transitions workflow steps from <code>TIMED_OUT</code> to <code>FAILED</code> state</li> <li>Transitions children of <code>FAILED</code> steps to <code>CANCELLED</code> state</li> <li>Deletes finished workflows according to the configured retention duration</li> </ul> </p> Required true Type <code>cron</code> Default <code>*/15 * * * *</code> ENV <code>TASK_WORKFLOW_MAINTENANCE_CRON</code>"},{"location":"reference/configuration/api-server/#taskworkflowmaintenancelockmaxduration","title":"task.workflow.maintenance.lock.max.duration","text":"<p>Maximum duration in ISO 8601 format for which the workflow maintenance task will hold a lock.    The duration should be long enough to cover the task's execution duration.  </p> Required true Type <code>duration</code> Default <code>PT5M</code> ENV <code>TASK_WORKFLOW_MAINTENANCE_LOCK_MAX_DURATION</code>"},{"location":"reference/configuration/api-server/#taskworkflowmaintenancelockminduration","title":"task.workflow.maintenance.lock.min.duration","text":"<p>Minimum duration in ISO 8601 format for which the workflow maintenance task will hold a lock.    The duration should be long enough to cover eventual clock skew across API server instances.  </p> Required true Type <code>duration</code> Default <code>PT1M</code> ENV <code>TASK_WORKFLOW_MAINTENANCE_LOCK_MIN_DURATION</code>"},{"location":"reference/configuration/mirror-service/","title":"Mirror Service","text":""},{"location":"reference/configuration/mirror-service/#datasource","title":"Datasource","text":""},{"location":"reference/configuration/mirror-service/#mirrordatasourceosvalias-sync-enabled","title":"mirror.datasource.osv.alias-sync-enabled","text":"<p>Defines whether vulnerability aliases should be parsed from OSV.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code>"},{"location":"reference/configuration/mirror-service/#mirrordatasourceosvbase-url","title":"mirror.datasource.osv.base-url","text":"<p>Defines the URL of the OSV storage bucket.  </p> Required false Type <code>string</code> Default <code>https://osv-vulnerabilities.storage.googleapis.com</code> ENV <code>MIRROR_DATASOURCE_OSV_BASE_URL</code>"},{"location":"reference/configuration/mirror-service/#http","title":"HTTP","text":""},{"location":"reference/configuration/mirror-service/#quarkushttpport","title":"quarkus.http.port","text":"<p>HTTP port to listen on. Application metrics will be available via this port.  </p> Required false Type <code>integer</code> Default <code>8093</code> ENV <code>QUARKUS_HTTP_PORT</code>"},{"location":"reference/configuration/mirror-service/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/mirror-service/#dtkafkatopicprefix","title":"dt.kafka.topic.prefix","text":"<p>Defines an optional prefix to assume for all Kafka topics the application  consumes from, or produces to. The prefix will also be prepended to the  application's consumer group ID.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>acme-</code> ENV <code>DT_KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamscommitintervalms","title":"kafka-streams.commit.interval.ms","text":"<p>Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.  The Kafka default of <code>30s</code> has been modified to <code>5s</code>.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms for details.  </p> Required false Type <code>integer</code> Default <code>5000</code> ENV <code>KAFKA_STREAMS_COMMIT_INTERVAL_MS</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsdeserializationcount","title":"kafka-streams.exception.thresholds.deserialization.count","text":"<p>Defines the threshold for records failing to be deserialized within <code>kafka-streams.exception.thresholds.deserialization.interval</code>.  Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsdeserializationinterval","title":"kafka-streams.exception.thresholds.deserialization.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.deserialization.count</code> records are  allowed to fail deserialization. Deserialization failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsprocessingcount","title":"kafka-streams.exception.thresholds.processing.count","text":"<p>Defines the threshold for records failing to be processed within <code>kafka-streams.exception.thresholds.processing.interval</code>.  Processing failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>50</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsprocessinginterval","title":"kafka-streams.exception.thresholds.processing.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.processing.count</code> records are  allowed to fail processing. Processing failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsproductioncount","title":"kafka-streams.exception.thresholds.production.count","text":"<p>Defines the threshold for records failing to be produced within <code>kafka-streams.exception.thresholds.production.interval</code>.  Production failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsexceptionthresholdsproductioninterval","title":"kafka-streams.exception.thresholds.production.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.production.count</code> records are  allowed to fail producing. Production failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code>"},{"location":"reference/configuration/mirror-service/#kafka-streamsnumstreamthreads","title":"kafka-streams.num.stream.threads","text":"<p>The number of threads to allocate for stream processing tasks.  Note that Specifying a number higher than the number of input partitions provides no additional benefit,  as excess threads will simply run idle.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads for details.  </p> Required true Type <code>integer</code> Default <code>3</code> ENV <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code>"},{"location":"reference/configuration/mirror-service/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"<p>Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for details.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>broker-01.acme.com:9092,broker-02.acme.com:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/mirror-service/#kafkamaxrequestsize","title":"kafka.max.request.size","text":"<p>Defines the maximum size of a Kafka producer request in bytes.    Some messages like Bill of Vulnerabilities can be bigger than the default 1MiB.  Since the size check is performed before records are compressed, this value may need to be increased  even though the compressed value is much smaller. The Kafka default of 1MiB has been raised to 2MiB.    Refer to https://kafka.apache.org/documentation/#producerconfigs_max.request.size for details.  </p> Required true Type <code>integer</code> Default <code>2097152</code> ENV <code>KAFKA_MAX_REQUEST_SIZE</code>"},{"location":"reference/configuration/mirror-service/#quarkuskafka-streamsapplication-id","title":"quarkus.kafka-streams.application-id","text":"<p>Defines the ID to uniquely identify this application in the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_application.id for details.  </p> Required false Type <code>string</code> Default <code>${dt.kafka.topic.prefix}hyades-mirror-service</code> ENV <code>QUARKUS_KAFKA_STREAMS_APPLICATION_ID</code>"},{"location":"reference/configuration/mirror-service/#observability","title":"Observability","text":""},{"location":"reference/configuration/mirror-service/#quarkuslogconsolejson","title":"quarkus.log.console.json","text":"<p>Defines whether logs should be written in JSON format.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>QUARKUS_LOG_CONSOLE_JSON</code>"},{"location":"reference/configuration/overview/","title":"Configuration","text":"<p>All available configuration options used by all applications are listed in their respective <code>application.properties</code>. Options can be provided via environment variables as well, refer to the Quarkus docs for details.</p> <p>Not all options are supposed to be tweaked by users though. This document contains an overview of all options that are expected to be changed by users.</p>"},{"location":"reference/configuration/overview/#kafka-topic-configuration","title":"Kafka Topic Configuration","text":"<p>Kafka topics (including internal topics) can be configured with custom prefix. In order to provide custom prefix, below environment variable can be used.</p> Environment Variable Description Default Required <code>DT_KAFKA_TOPIC_PREFIX</code> Prefix for topic names - \u274c"},{"location":"reference/configuration/overview/#notification-publisher","title":"Notification Publisher","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>PARALLEL_CONSUMER_MAX_CONCURRENCY</code> Number of threads to process notifications with <code>6</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_INITIAL_DELAY</code> Initial delay before retrying notification delivery <code>3S</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_MULTIPLIER</code> Multiplier for retry delays <code>0.3</code> \u2705 <code>PARALLEL_CONSUMER_RETRY_RANDOMIZATION_FACTOR</code> Randomization factory for jitter in retry delays <code>0.3</code> \u274c <code>PARALLEL_CONSUMER_RETRY_MAX_DURATION</code> Maximum duration of delays between retry attempts <code>2M</code> \u2705 <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#repository-meta-analyzer","title":"Repository Meta Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#vulnerability-analyzer","title":"Vulnerability Analyzer","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>STATE_STORE_TYPE</code> Whether to use in-memory or persistent (RocksDB) Kafka Streams state stores <code>in_memory</code> \u2705 <code>STATE_STORE_ROCKS_DB_COMPACTION_STYLE</code> Compaction style to use for RocksDB state stores - \u274c <code>STATE_STORE_ROCKS_DB_COMPRESSION_TYPE</code> Compression type to use for RocksDB state stores - \u274c <code>QUARKUS_DATASOURCE_DB_KIND</code> The database type <code>postgresql</code> \u2705 <code>QUARKUS_DATASOURCE_JDBC_URL</code> The database JDBC URL - \u2705 <code>QUARKUS_DATASOURCE_USERNAME</code> The database username - \u2705 <code>QUARKUS_DATASOURCE_PASSWORD</code> The database password - \u2705 <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <code>SCANNER_INTERNAL_ENABLED</code> Enable the internal vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_ENABLED</code> Enable the OSS Index vulnerability scanner <code>true</code> \u274c <code>SCANNER_OSSINDEX_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by OssIndex analyzer. This depends on standard and custom supported package urls <code>cargo,composer,gem,hex,maven,npm,nuget,pypi,rpm,conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by ossindex. These are not part of the standard package urls list <code>conan,conda,swift,cocoapods,cran</code> \u274c <code>SCANNER_OSSINDEX_API_USERNAME</code> OSS Index API username - \u274c <code>SCANNER_OSSINDEX_API_TOKEN</code> OSS Index API token - \u274c <code>SCANNER_OSSINDEX_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> \u274c <code>SCANNER_OSSINDEX_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSS Index <code>false</code> \u274c <code>SCANNER_SNYK_ENABLED</code> Enable the Snyk vulnerability scanner <code>false</code> \u274c <code>SCANNER_SNYK_API_ORG_ID</code> Snyk organization ID - When Snyk is enabled <code>SCANNER_SNYK_API_TOKENS</code> Comma-separated list of Snyk API tokens - When Snyk is enabled <code>SCANNER_SNYK_API_VERSION</code> Version of the Snyk API to use <code>2022-12-15</code> When Snyk is enabled <code>SCANNER_SNYK_SEVERITY_SOURCE_PRIORITY</code> Priority of preferred source for vulnerability severities <code>nvd,snyk,redhat,suse</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_INTERVAL</code> Max time to wait before submitting incomplete batches <code>5S</code> When Snyk is enabled <code>SCANNER_SNYK_BATCH_SIZE</code> Max size of batch at which it will be submitted <code>100</code> When Snyk is enabled <code>SCANNER_SNYK_ALIAS_SYNC_ENABLED</code> Enable alias syncing for Snyk <code>false</code> \u274c <code>SCANNER_SNYK_INPUT_PACKAGE_URLS</code> Package urls the user wants to be covered by Snyk analyzer. This depends on supported standard and custom package urls <code>cargo,cocoapods,composer,gem,generic,hex,maven,npm,nuget,pypi,swift,golang</code> \u274c <code>SCANNER_SNYK_CUSTOM_PACKAGE_URLS_SUPPORTED</code> Custom package urls supported by Snyk. These are not part of the standard package urls list <code>cocoapods,apk,swift</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/overview/#mirror-service","title":"Mirror Service","text":"Environment Variable Description Default Required <code>KAFKA_BOOTSTRAP_SERVERS</code> Comma-separated list of Kafka servers <code>localhost:9092</code> \u2705 <code>KAFKA_SSL_ENABLED</code> SSL enabled for using kafka broker <code>false</code> \u274c <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code> Number of Kafka Streams threads <code>3</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code> Threshold number of acceptable deserialization errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code> Interval for threshold of acceptable deserialization errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code> Threshold number of acceptable processing errors <code>50</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code> Interval for threshold of acceptable processing errors <code>PT30M</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code> Threshold number of acceptable production errors <code>5</code> \u274c <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code> Interval for threshold of acceptable production errors <code>PT30M</code> \u274c <code>MIRROR_DATASOURCE_GITHUB_ALIAS_SYNC_ENABLED</code> Enable alias syncing for GitHub Advisories <code>false</code> \u274c <code>MIRROR_DATASOURCE_OSV_ALIAS_SYNC_ENABLED</code> Enable alias syncing for OSV <code>false</code> \u274c <code>QUARKUS_LOG_CONSOLE_JSON</code> Enable logging in JSON format <code>false</code> \u274c <p>Note Refer</p> <p>to <code>application.properties</code> for a complete overview of available config options.</p>"},{"location":"reference/configuration/repo-meta-analyzer/","title":"Repository Metadata Analyzer","text":""},{"location":"reference/configuration/repo-meta-analyzer/#cache","title":"Cache","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscachecaffeinemetaanalyzerexpire-after-write","title":"quarkus.cache.caffeine.\"metaAnalyzer\".expire-after-write","text":"<p>Defines the time-to-live of cache entries.  </p> Required true Type <code>duration</code> Default <code>PT2H</code> ENV <code>QUARKUS_CACHE_CAFFEINE__METAANALYZER__EXPIRE_AFTER_WRITE</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscachecaffeinemetaanalyzerinitial-capacity","title":"quarkus.cache.caffeine.\"metaAnalyzer\".initial-capacity","text":"<p>Defines the initial capacity of the cache.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>QUARKUS_CACHE_CAFFEINE__METAANALYZER__INITIAL_CAPACITY</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuscacheenabled","title":"quarkus.cache.enabled","text":"<p>Defines whether caching of analysis results shall be enabled.  </p> Required true Type <code>boolean</code> Default <code>true</code> ENV <code>QUARKUS_CACHE_ENABLED</code>"},{"location":"reference/configuration/repo-meta-analyzer/#database","title":"Database","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourcejdbcurl","title":"quarkus.datasource.jdbc.url","text":"<p>Specifies the JDBC URL to use when connecting to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_JDBC_URL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourcepassword","title":"quarkus.datasource.password","text":"<p>Specifies the password to use when authenticating to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_PASSWORD</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkusdatasourceusername","title":"quarkus.datasource.username","text":"<p>Specifies the username to use when authenticating to the database.  </p> Required true Type <code>string</code> Default <code>null</code> ENV <code>QUARKUS_DATASOURCE_USERNAME</code>"},{"location":"reference/configuration/repo-meta-analyzer/#general","title":"General","text":""},{"location":"reference/configuration/repo-meta-analyzer/#secretkeypath","title":"secret.key.path","text":"<p>Defines the path to the secret key to be used for data encryption and decryption.  </p> Required false Type <code>string</code> Default <code>~/.dependency-track/keys/secret.key</code> ENV <code>SECRET_KEY_PATH</code>"},{"location":"reference/configuration/repo-meta-analyzer/#http","title":"HTTP","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkushttpport","title":"quarkus.http.port","text":"<p>HTTP port to listen on. Application metrics will be available via this port.  </p> Required false Type <code>integer</code> Default <code>8091</code> ENV <code>QUARKUS_HTTP_PORT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka","title":"Kafka","text":""},{"location":"reference/configuration/repo-meta-analyzer/#dtkafkatopicprefix","title":"dt.kafka.topic.prefix","text":"<p>Defines an optional prefix to assume for all Kafka topics the application  consumes from, or produces to. The prefix will also be prepended to the  application's consumer group ID.  </p> Required false Type <code>string</code> Default <code>null</code> Example <code>acme-</code> ENV <code>DT_KAFKA_TOPIC_PREFIX</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsautooffsetreset","title":"kafka-streams.auto.offset.reset","text":"<p>Refer to https://kafka.apache.org/documentation/#consumerconfigs_auto.offset.reset for details.  </p> Required false Type <code>enum</code> Valid Values <code>[earliest, latest, none]</code> Default <code>earliest</code> ENV <code>KAFKA_STREAMS_AUTO_OFFSET_RESET</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamscommitintervalms","title":"kafka-streams.commit.interval.ms","text":"<p>Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.  The Kafka default of <code>30s</code> has been modified to <code>5s</code>.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms for details.  </p> Required false Type <code>integer</code> Default <code>5000</code> ENV <code>KAFKA_STREAMS_COMMIT_INTERVAL_MS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsdeserializationcount","title":"kafka-streams.exception.thresholds.deserialization.count","text":"<p>Defines the threshold for records failing to be deserialized within <code>kafka-streams.exception.thresholds.deserialization.interval</code>.  Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsdeserializationinterval","title":"kafka-streams.exception.thresholds.deserialization.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.deserialization.count</code> records are  allowed to fail deserialization. Deserialization failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_DESERIALIZATION_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsprocessingcount","title":"kafka-streams.exception.thresholds.processing.count","text":"<p>Defines the threshold for records failing to be processed within <code>kafka-streams.exception.thresholds.processing.interval</code>.  Processing failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>50</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsprocessinginterval","title":"kafka-streams.exception.thresholds.processing.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.processing.count</code> records are  allowed to fail processing. Processing failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PROCESSING_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsproductioncount","title":"kafka-streams.exception.thresholds.production.count","text":"<p>Defines the threshold for records failing to be produced within <code>kafka-streams.exception.thresholds.production.interval</code>.  Production failures within the threshold will be logged, failures exceeding the threshold cause the application  to stop processing further records, and shutting down.  </p> Required true Type <code>integer</code> Default <code>5</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_COUNT</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsexceptionthresholdsproductioninterval","title":"kafka-streams.exception.thresholds.production.interval","text":"<p>Defines the interval within which up to <code>kafka-streams.exception.thresholds.production.count</code> records are  allowed to fail producing. Production failures within the threshold will be logged,  failures exceeding the threshold cause the application to stop processing further records, and shutting down.  </p> Required true Type <code>duration</code> Default <code>PT30M</code> ENV <code>KAFKA_STREAMS_EXCEPTION_THRESHOLDS_PRODUCTION_INTERVAL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsmetricsrecordinglevel","title":"kafka-streams.metrics.recording.level","text":"<p>Refer to https://kafka.apache.org/documentation/#adminclientconfigs_metrics.recording.level for details.  </p> Required false Type <code>enum</code> Valid Values <code>[INFO, DEBUG, TRACE]</code> Default <code>DEBUG</code> ENV <code>KAFKA_STREAMS_METRICS_RECORDING_LEVEL</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafka-streamsnumstreamthreads","title":"kafka-streams.num.stream.threads","text":"<p>Refer to https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads for details.  </p> Required true Type <code>integer</code> Default <code>3</code> ENV <code>KAFKA_STREAMS_NUM_STREAM_THREADS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#kafkabootstrapservers","title":"kafka.bootstrap.servers","text":"<p>Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for details.  </p> Required true Type <code>string</code> Default <code>null</code> Example <code>broker-01.acme.com:9092,broker-02.acme.com:9092</code> ENV <code>KAFKA_BOOTSTRAP_SERVERS</code>"},{"location":"reference/configuration/repo-meta-analyzer/#quarkuskafka-streamsapplication-id","title":"quarkus.kafka-streams.application-id","text":"<p>Defines the ID to uniquely identify this application in the Kafka cluster.    Refer to https://kafka.apache.org/documentation/#streamsconfigs_application.id for details.  </p> Required false Type <code>string</code> Default <code>${dt.kafka.topic.prefix}hyades-repository-meta-analyzer</code> ENV <code>QUARKUS_KAFKA_STREAMS_APPLICATION_ID</code>"},{"location":"reference/configuration/repo-meta-analyzer/#observability","title":"Observability","text":""},{"location":"reference/configuration/repo-meta-analyzer/#quarkuslogconsolejson","title":"quarkus.log.console.json","text":"<p>Defines whether logs should be written in JSON format.  </p> Required false Type <code>boolean</code> Default <code>false</code> ENV <code>QUARKUS_LOG_CONSOLE_JSON</code>"},{"location":"reference/schemas/notification/","title":"Notification","text":""},{"location":"reference/schemas/notification/#notification","title":"Notification","text":"Field Type Description <code>level</code> <code>Level</code> <code>scope</code> <code>Scope</code> <code>group</code> <code>Group</code> <code>title</code> <code>string</code> <code>content</code> <code>string</code> <code>timestamp</code> <code>google.protobuf.Timestamp</code> <code>subject</code> <code>google.protobuf.Any</code>"},{"location":"reference/schemas/notification/#subjects","title":"Subjects","text":""},{"location":"reference/schemas/notification/#bomconsumedorprocessedsubject","title":"BomConsumedOrProcessedSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>bom</code> <code>Bom</code> <code>token</code> <code>string</code>"},{"location":"reference/schemas/notification/#bomprocessingfailedsubject","title":"BomProcessingFailedSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>bom</code> <code>Bom</code> <code>cause</code> <code>string</code> <code>token</code> <code>string</code>"},{"location":"reference/schemas/notification/#bomvalidationfailedsubject","title":"BomValidationFailedSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>bom</code> <code>Bom</code> <code>errors</code> <code>string[]</code>"},{"location":"reference/schemas/notification/#componentvulnanalysiscompletesubject","title":"ComponentVulnAnalysisCompleteSubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>vulnerabilities</code> <code>Vulnerability[]</code>"},{"location":"reference/schemas/notification/#newvulnerabilitysubject","title":"NewVulnerabilitySubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>vulnerability</code> <code>Vulnerability</code> <code>affected_projects_reference</code> <code>BackReference</code> <code>vulnerability_analysis_level</code> <code>string</code> <code>affected_projects</code> <code>Project[]</code> List of projects affected by the vulnerability. DEPRECATED: This list only holds one item, and it is identical to the one in the project field. The field is kept for backward compatibility of JSON notifications, but consumers should not expect multiple projects here. Transmitting all affected projects in one notification is not feasible for large portfolios, see https://github.com/DependencyTrack/hyades/issues/467 for details."},{"location":"reference/schemas/notification/#newvulnerabledependencysubject","title":"NewVulnerableDependencySubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>vulnerabilities</code> <code>Vulnerability[]</code>"},{"location":"reference/schemas/notification/#policyviolationanalysisdecisionchangesubject","title":"PolicyViolationAnalysisDecisionChangeSubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>policy_violation</code> <code>PolicyViolation</code> <code>analysis</code> <code>PolicyViolationAnalysis</code>"},{"location":"reference/schemas/notification/#policyviolationsubject","title":"PolicyViolationSubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>policy_violation</code> <code>PolicyViolation</code>"},{"location":"reference/schemas/notification/#projectvulnanalysiscompletesubject","title":"ProjectVulnAnalysisCompleteSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>findings</code> <code>ComponentVulnAnalysisCompleteSubject[]</code> <code>status</code> <code>ProjectVulnAnalysisStatus</code> <code>token</code> <code>string</code>"},{"location":"reference/schemas/notification/#usersubject","title":"UserSubject","text":"Field Type Description <code>username</code> <code>string</code> <code>email</code> <code>string</code>"},{"location":"reference/schemas/notification/#vexconsumedorprocessedsubject","title":"VexConsumedOrProcessedSubject","text":"Field Type Description <code>project</code> <code>Project</code> <code>vex</code> <code>bytes</code> <code>format</code> <code>string</code> <code>spec_version</code> <code>string</code>"},{"location":"reference/schemas/notification/#vulnerabilityanalysisdecisionchangesubject","title":"VulnerabilityAnalysisDecisionChangeSubject","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>vulnerability</code> <code>Vulnerability</code> <code>analysis</code> <code>VulnerabilityAnalysis</code>"},{"location":"reference/schemas/notification/#messages","title":"Messages","text":""},{"location":"reference/schemas/notification/#backreference","title":"BackReference","text":"Field Type Description <code>api_uri</code> <code>string</code> URI to the API endpoint from which additional information can be fetched. <code>frontend_uri</code> <code>string</code> URI to the frontend where additional information can be seen."},{"location":"reference/schemas/notification/#bom","title":"Bom","text":"Field Type Description <code>content</code> <code>string</code> <code>format</code> <code>string</code> <code>spec_version</code> <code>string</code>"},{"location":"reference/schemas/notification/#component","title":"Component","text":"Field Type Description <code>uuid</code> <code>string</code> <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>version</code> <code>string</code> <code>purl</code> <code>string</code> <code>md5</code> <code>string</code> <code>sha1</code> <code>string</code> <code>sha256</code> <code>string</code> <code>sha512</code> <code>string</code>"},{"location":"reference/schemas/notification/#policy","title":"Policy","text":"Field Type Description <code>uuid</code> <code>string</code> <code>name</code> <code>string</code> <code>violation_state</code> <code>string</code>"},{"location":"reference/schemas/notification/#policycondition","title":"PolicyCondition","text":"Field Type Description <code>uuid</code> <code>string</code> <code>subject</code> <code>string</code> <code>operator</code> <code>string</code> <code>value</code> <code>string</code> <code>policy</code> <code>Policy</code>"},{"location":"reference/schemas/notification/#policyviolation","title":"PolicyViolation","text":"Field Type Description <code>uuid</code> <code>string</code> <code>type</code> <code>string</code> <code>timestamp</code> <code>google.protobuf.Timestamp</code> <code>condition</code> <code>PolicyCondition</code>"},{"location":"reference/schemas/notification/#policyviolationanalysis","title":"PolicyViolationAnalysis","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>policy_violation</code> <code>PolicyViolation</code> <code>state</code> <code>string</code> <code>suppressed</code> <code>bool</code>"},{"location":"reference/schemas/notification/#project","title":"Project","text":"Field Type Description <code>uuid</code> <code>string</code> <code>name</code> <code>string</code> <code>version</code> <code>string</code> <code>description</code> <code>string</code> <code>purl</code> <code>string</code> <code>tags</code> <code>string[]</code>"},{"location":"reference/schemas/notification/#vulnerability","title":"Vulnerability","text":"Field Type Description <code>uuid</code> <code>string</code> <code>vuln_id</code> <code>string</code> <code>source</code> <code>string</code> <code>aliases</code> <code>Vulnerability.Alias[]</code> <code>title</code> <code>string</code> <code>sub_title</code> <code>string</code> <code>description</code> <code>string</code> <code>recommendation</code> <code>string</code> <code>cvss_v2</code> <code>double</code> <code>cvss_v3</code> <code>double</code> <code>owasp_rr_likelihood</code> <code>double</code> <code>owasp_rr_technical_impact</code> <code>double</code> <code>owasp_rr_business_impact</code> <code>double</code> <code>severity</code> <code>string</code> <code>cwes</code> <code>Vulnerability.Cwe[]</code> <code>cvss_v2_vector</code> <code>string</code> <code>cvss_v3_vector</code> <code>string</code> <code>owasp_rr_vector</code> <code>string</code>"},{"location":"reference/schemas/notification/#vulnerabilityalias","title":"Vulnerability.Alias","text":"Field Type Description <code>id</code> <code>string</code> <code>source</code> <code>string</code>"},{"location":"reference/schemas/notification/#vulnerabilitycwe","title":"Vulnerability.Cwe","text":"Field Type Description <code>cwe_id</code> <code>int32</code> <code>name</code> <code>string</code>"},{"location":"reference/schemas/notification/#vulnerabilityanalysis","title":"VulnerabilityAnalysis","text":"Field Type Description <code>component</code> <code>Component</code> <code>project</code> <code>Project</code> <code>vulnerability</code> <code>Vulnerability</code> <code>state</code> <code>string</code> <code>suppressed</code> <code>bool</code>"},{"location":"reference/schemas/notification/#enums","title":"Enums","text":""},{"location":"reference/schemas/notification/#group","title":"Group","text":"Name Description <code>GROUP_UNSPECIFIED</code> <code>GROUP_CONFIGURATION</code> <code>GROUP_DATASOURCE_MIRRORING</code> <code>GROUP_REPOSITORY</code> <code>GROUP_INTEGRATION</code> <code>GROUP_FILE_SYSTEM</code> <code>GROUP_ANALYZER</code> <code>GROUP_NEW_VULNERABILITY</code> <code>GROUP_NEW_VULNERABLE_DEPENDENCY</code> <code>GROUP_PROJECT_AUDIT_CHANGE</code> <code>GROUP_BOM_CONSUMED</code> <code>GROUP_BOM_PROCESSED</code> <code>GROUP_VEX_CONSUMED</code> <code>GROUP_VEX_PROCESSED</code> <code>GROUP_POLICY_VIOLATION</code> <code>GROUP_PROJECT_CREATED</code> <code>GROUP_BOM_PROCESSING_FAILED</code> <code>GROUP_PROJECT_VULN_ANALYSIS_COMPLETE</code> <code>GROUP_USER_CREATED</code> <code>GROUP_USER_DELETED</code> <code>GROUP_BOM_VALIDATION_FAILED</code>"},{"location":"reference/schemas/notification/#level","title":"Level","text":"Name Description <code>LEVEL_UNSPECIFIED</code> <code>LEVEL_INFORMATIONAL</code> <code>LEVEL_WARNING</code> <code>LEVEL_ERROR</code>"},{"location":"reference/schemas/notification/#projectvulnanalysisstatus","title":"ProjectVulnAnalysisStatus","text":"Name Description <code>PROJECT_VULN_ANALYSIS_STATUS_UNSPECIFIED</code> <code>PROJECT_VULN_ANALYSIS_STATUS_FAILED</code> <code>PROJECT_VULN_ANALYSIS_STATUS_COMPLETED</code>"},{"location":"reference/schemas/notification/#scope","title":"Scope","text":"Name Description <code>SCOPE_UNSPECIFIED</code> <code>SCOPE_PORTFOLIO</code> <code>SCOPE_SYSTEM</code>"},{"location":"reference/schemas/policy/","title":"Policy","text":""},{"location":"reference/schemas/policy/#messages","title":"Messages","text":""},{"location":"reference/schemas/policy/#component","title":"Component","text":"Field Type Description <code>uuid</code> <code>string</code> UUID of the component. <code>group</code> <code>string</code> Group / namespace of the component. <code>name</code> <code>string</code> Name of the component. <code>version</code> <code>string</code> Version of the component. <code>classifier</code> <code>string</code> Classifier / type of the component. May be any of: - APPLICATION - CONTAINER - DEVICE - FILE - FIRMWARE - FRAMEWORK - LIBRARY - OPERATING_SYSTEM <code>cpe</code> <code>string</code> CPE of the component. https://csrc.nist.gov/projects/security-content-automation-protocol/specifications/cpe <code>purl</code> <code>string</code> Package URL of the component. https://github.com/package-url/purl-spec <code>swid_tag_id</code> <code>string</code> SWID tag ID of the component. https://csrc.nist.gov/projects/Software-Identification-SWID <code>is_internal</code> <code>bool</code> Whether the component is internal to the organization. <code>md5</code> <code>string</code> <code>sha1</code> <code>string</code> <code>sha256</code> <code>string</code> <code>sha384</code> <code>string</code> <code>sha512</code> <code>string</code> <code>sha3_256</code> <code>string</code> <code>sha3_384</code> <code>string</code> <code>sha3_512</code> <code>string</code> <code>blake2b_256</code> <code>string</code> <code>blake2b_384</code> <code>string</code> <code>blake2b_512</code> <code>string</code> <code>blake3</code> <code>string</code> <code>license_name</code> <code>string</code> <code>license_expression</code> <code>string</code> <code>resolved_license</code> <code>License</code> <code>published_at</code> <code>google.protobuf.Timestamp</code> When the component current version last modified. <code>latest_version</code> <code>string</code>"},{"location":"reference/schemas/policy/#license","title":"License","text":"Field Type Description <code>uuid</code> <code>string</code> <code>id</code> <code>string</code> <code>name</code> <code>string</code> <code>groups</code> <code>License.Group[]</code> <code>is_osi_approved</code> <code>bool</code> <code>is_fsf_libre</code> <code>bool</code> <code>is_deprecated_id</code> <code>bool</code> <code>is_custom</code> <code>bool</code>"},{"location":"reference/schemas/policy/#licensegroup","title":"License.Group","text":"Field Type Description <code>uuid</code> <code>string</code> <code>name</code> <code>string</code>"},{"location":"reference/schemas/policy/#project","title":"Project","text":"Field Type Description <code>uuid</code> <code>string</code> <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>version</code> <code>string</code> <code>classifier</code> <code>string</code> <code>is_active</code> <code>bool</code> <code>tags</code> <code>string[]</code> <code>properties</code> <code>Project.Property[]</code> <code>cpe</code> <code>string</code> <code>purl</code> <code>string</code> <code>swid_tag_id</code> <code>string</code> <code>last_bom_import</code> <code>google.protobuf.Timestamp</code> <code>metadata</code> <code>Project.Metadata</code>"},{"location":"reference/schemas/policy/#projectmetadata","title":"Project.Metadata","text":"Field Type Description <code>tools</code> <code>Tools</code> <code>bom_generated</code> <code>google.protobuf.Timestamp</code>"},{"location":"reference/schemas/policy/#projectproperty","title":"Project.Property","text":"Field Type Description <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>value</code> <code>string</code> <code>type</code> <code>string</code>"},{"location":"reference/schemas/policy/#tools","title":"Tools","text":"Field Type Description <code>components</code> <code>Component[]</code> Components used as tools."},{"location":"reference/schemas/policy/#versiondistance","title":"VersionDistance","text":"Field Type Description <code>epoch</code> <code>string</code> <code>major</code> <code>string</code> <code>minor</code> <code>string</code> <code>patch</code> <code>string</code>"},{"location":"reference/schemas/policy/#vulnerability","title":"Vulnerability","text":"Field Type Description <code>uuid</code> <code>string</code> <code>id</code> <code>string</code> <code>source</code> <code>string</code> <code>aliases</code> <code>Vulnerability.Alias[]</code> <code>cwes</code> <code>int32[]</code> <code>created</code> <code>google.protobuf.Timestamp</code> <code>published</code> <code>google.protobuf.Timestamp</code> <code>updated</code> <code>google.protobuf.Timestamp</code> <code>severity</code> <code>string</code> <code>cvssv2_base_score</code> <code>double</code> <code>cvssv2_impact_subscore</code> <code>double</code> <code>cvssv2_exploitability_subscore</code> <code>double</code> <code>cvssv2_vector</code> <code>string</code> <code>cvssv3_base_score</code> <code>double</code> <code>cvssv3_impact_subscore</code> <code>double</code> <code>cvssv3_exploitability_subscore</code> <code>double</code> <code>cvssv3_vector</code> <code>string</code> <code>owasp_rr_likelihood_score</code> <code>double</code> <code>owasp_rr_technical_impact_score</code> <code>double</code> <code>owasp_rr_business_impact_score</code> <code>double</code> <code>owasp_rr_vector</code> <code>string</code> <code>epss_score</code> <code>double</code> <code>epss_percentile</code> <code>double</code>"},{"location":"reference/schemas/policy/#vulnerabilityalias","title":"Vulnerability.Alias","text":"Field Type Description <code>id</code> <code>string</code> <code>source</code> <code>string</code>"},{"location":"usage/policy-compliance/expressions/","title":"Expressions","text":""},{"location":"usage/policy-compliance/expressions/#introduction","title":"Introduction","text":"<p>Dependency-Track allows policy conditions to be defined using the Common Expression Language (CEL), enabling more flexibility, and more control compared to predefined conditions.</p> <p>To use CEL, simply select the subject <code>Expression</code> when adding a new condition. A code editor will appear in which expressions can be provided.</p> <p></p> <p>In addition to the expression itself, it's necessary to specify a violation type, which may be any of <code>License</code>, <code>Operational</code>, or <code>Security</code>. The violation type aids in communicating what kind of risk is introduced by the condition being matched.</p>"},{"location":"usage/policy-compliance/expressions/#syntax","title":"Syntax","text":"<p>The CEL syntax is similar to other C-style languages like Java and JavaScript. However, CEL is not Turing-complete. As such, it does not support constructs like <code>if</code> statements or loops (i.e. <code>for</code>, <code>while</code>).</p> <p>As a compensation for missing loops, CEL offers macros like <code>all</code>, <code>exists</code>, <code>exists_one</code>, <code>map</code>, and <code>filter</code>. Refer to the macros documentation for more details, or have a look at the examples to see how they may be utilized in practice.</p> <p>CEL syntax is described thoroughly in the official language definition.</p>"},{"location":"usage/policy-compliance/expressions/#evaluation-context","title":"Evaluation Context","text":"<p>Conditions are scoped to individual components. Each condition is evaluated for every single component in a project.</p> <p>The context in which expressions are evaluated in contains the following variables:</p> Variable Type Description <code>component</code> <code>Component</code> The component being evaluated <code>project</code> <code>Project</code> The project the component is part of <code>vulns</code> <code>list(Vulnerability)</code> Vulnerabilities the component is affected by"},{"location":"usage/policy-compliance/expressions/#best-practices","title":"Best Practices","text":"<ol> <li>Keep expressions simple and concise. The more complex an expression becomes, the harder it gets to determine why it did or did not match. Use policy operators (<code>Any</code>, <code>All</code>) to chain multiple expressions if practical.</li> <li>Call functions last. Custom functions involve additional computation that is more expensive than simple field accesses. Performing any checks on fields first, and calling functions last, oftentimes allows evaluation to short-circuit.</li> <li>Remove conditions that are no longer needed. Dependency-Track analyzes the configured expressions to determine what data it has to load from the database in order to evaluate them. The more fields are being accessed, the more data has to be loaded. Removal of outdated conditions thus has a direct positive performance impact.</li> </ol>"},{"location":"usage/policy-compliance/expressions/#examples","title":"Examples","text":""},{"location":"usage/policy-compliance/expressions/#component-age","title":"Component age","text":"<p>Besides out-of-date versions, component age is another indicator of potential risk. Components may be on the latest available version, but still be 20 years old. </p> <p>Component age can be evaluated using the <code>compare_age</code> function. The first function argument  is a numeric comparator (<code>&lt;</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&gt;=</code>), and the second is a duration in ISO8601 notation.</p> <p>The following expression matches Components that are two years old, or even older:</p> <pre><code>component.compare_age(\"&gt;=\", \"P2Y\")\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#component-blacklist","title":"Component blacklist","text":"<p>The following expression matches on the Component's Package URL, using a regular expression in RE2 syntax. Additionally, it checks whether the Component's version falls into a given vers range, consisting of multiple constraints.</p> <pre><code>component.purl.matches(\"^pkg:maven/com.acme/acme-lib\\\\b.*\")\n  &amp;&amp; component.matches_range(\"vers:maven/&gt;0|&lt;1|!=0.2.4\")\n</code></pre> <p>The expression will match:</p> <ul> <li><code>pkg:maven/com.acme/acme-lib@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.9.9</code></li> </ul> <p>but not:</p> <ul> <li><code>pkg:maven/com.acme/acme-library@0.1.0</code></li> <li><code>pkg:maven/com.acme/acme-lib@0.2.4</code></li> </ul> <p><code>matches_range</code> currently supports the following versioning schemes:</p> Versioning Scheme Ecosystem <code>deb</code> Debian / Ubuntu <code>generic</code> Generic / Any <code>golang</code> Go <code>maven</code> Java / Maven <code>npm</code> JavaScript / NodeJS <code>rpm</code> CentOS / Fedora / Red Hat / SUSE <p>Note</p> <p>If the ecosystem of the component(s) to match against is known upfront, it's good practice to use the according versioning scheme in <code>matches_range</code>. This helps with accuracy, as versioning schemes have different nuances across ecosystems, which makes comparisons error-prone.</p>"},{"location":"usage/policy-compliance/expressions/#dependency-graph-traversal","title":"Dependency graph traversal","text":"<p>The following expression matches Components that are a (possibly transitive) dependency of a Component with name <code>foo</code>, but only if a Component with name <code>bar</code> is also present in the Project.</p> <pre><code>component.is_dependency_of(v1.Component{name: \"foo\"})\n  &amp;&amp; project.depends_on(v1.Component{name: \"bar\"})\n</code></pre> <p><code>is_dependency_of</code> and <code>depends_on</code> lookups currently support the following Component fields:</p> <ul> <li><code>uuid</code></li> <li><code>group</code></li> <li><code>name</code></li> <li><code>version</code></li> <li><code>classifier</code></li> <li><code>cpe</code></li> <li><code>purl</code></li> <li><code>swid_tag_id</code></li> <li><code>internal</code></li> </ul> <p>Initially, only exact matches on those fields are supported. In the future, more sophisticated matching options will be added.</p> <p>Note</p> <p>When constructing objects like Component on-the-fly, it is necessary to use their version namespace, i.e. <code>v1</code>. This is required in order to perform type checking, as well as ensuring backward compatibility.</p>"},{"location":"usage/policy-compliance/expressions/#license-blacklist","title":"License blacklist","text":"<p>The following expression matches Components that are not internal to the organization, and have either:</p> <ul> <li>No resolved License at all</li> <li>A resolved License that is not part of the <code>Permissive</code> license group</li> </ul> <pre><code>!component.is_internal &amp;&amp; (\n  !has(component.resolved_license)\n    || component.resolved_license.groups.exisits(licenseGroup, \n         licenseGroup.name == \"Permissive\")\n)\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerability-blacklist","title":"Vulnerability blacklist","text":"<p>The following expression matches Components in Projects tagged as <code>3rd-party</code>, with at least one Vulnerability being any of the given blacklisted IDs.</p> <pre><code>\"3rd-party\" in project.tags\n  &amp;&amp; vulns.exists(vuln, vuln.id in [\n       \"CVE-2017-5638\",  // struts RCE\n       \"CVE-2021-44228\", // log4shell\n       \"CVE-2022-22965\", // spring4shell\n     ])\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#vulnerabilities-with-high-severity-in-public-facing-projects","title":"Vulnerabilities with high severity in public facing projects","text":"<p>The following expression matches Components in Projects tagged as <code>public-facing</code>, with at least one <code>HIGH</code> or <code>CRITICAL</code> Vulnerability, where the CVSSv3 attack vector is <code>Network</code>.</p> <pre><code>\"public-facing\" in project.tags\n  &amp;&amp; vulns.exists(vuln,\n    vuln.severity in [\"HIGH\", \"CRITICAL\"]\n      &amp;&amp; vuln.cvssv3_vector.matches(\".*/AV:N/.*\")\n  )\n</code></pre>"},{"location":"usage/policy-compliance/expressions/#reference","title":"Reference","text":""},{"location":"usage/policy-compliance/expressions/#types","title":"Types","text":""},{"location":"usage/policy-compliance/expressions/#component","title":"<code>Component</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>is_internal</code> <code>bool</code> Is internal? <code>md5</code> <code>string</code> MD5 hash <code>sha1</code> <code>string</code> SHA1 hash <code>sha256</code> <code>string</code> SHA256 hash <code>sha384</code> <code>string</code> SHA384 hash <code>sha512</code> <code>string</code> SHA512 hash <code>sha3_256</code> <code>string</code> SHA3-256 hash <code>sha3_384</code> <code>string</code> SHA3-384 hash <code>sha3_512</code> <code>string</code> SHA3-512 hash <code>blake2b_256</code> <code>string</code> BLAKE2b-256 hash <code>blake2b_384</code> <code>string</code> BLAKE2b-384 hash <code>blake2b_512</code> <code>string</code> BLAKE2b-512 hash <code>blake3</code> <code>string</code> BLAKE3 hash <code>license_name</code> <code>string</code> License name (if unresolved) <code>license_expression</code> <code>string</code> SPDX license expression <code>resolved_license</code> <code>License</code> Resolved license <code>published_at</code> <code>google.protobuf.Timestamp</code> When the component was published <code>latest_version</code> <code>string</code> Latest known version"},{"location":"usage/policy-compliance/expressions/#license","title":"<code>License</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> SPDX license ID <code>name</code> <code>string</code> License name <code>groups</code> <code>list(License.Group)</code> Groups this license is included in <code>is_osi_approved</code> <code>bool</code> Is OSI-approved? <code>is_fsf_libre</code> <code>bool</code> Is included in FSF license list? <code>is_deprecated_id</code> <code>bool</code> Uses a deprecated SPDX license ID? <code>is_custom</code> <code>bool</code> Is custom / not included in SPDX license list?"},{"location":"usage/policy-compliance/expressions/#licensegroup","title":"<code>License.Group</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>name</code> <code>string</code> Group name"},{"location":"usage/policy-compliance/expressions/#project","title":"<code>Project</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>group</code> <code>string</code> Group / namespace <code>name</code> <code>string</code> Name <code>version</code> <code>string</code> Version <code>classifier</code> <code>string</code> Classifier / type <code>is_active</code> <code>bool</code> Is active? <code>tags</code> <code>list(string)</code> Tags <code>properties</code> <code>list(Project.Property)</code> Properties <code>cpe</code> <code>string</code> CPE <code>purl</code> <code>string</code> Package URL <code>swid_tag_id</code> <code>string</code> SWID Tag ID <code>last_bom_import</code> <code>google.protobuf.Timestamp</code>"},{"location":"usage/policy-compliance/expressions/#projectproperty","title":"<code>Project.Property</code>","text":"Field Type Description <code>group</code> <code>string</code> <code>name</code> <code>string</code> <code>value</code> <code>string</code> <code>type</code> <code>string</code>"},{"location":"usage/policy-compliance/expressions/#vulnerability","title":"<code>Vulnerability</code>","text":"Field Type Description <code>uuid</code> <code>string</code> Internal UUID <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>CVE-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>NVD</code>) <code>aliases</code> <code>list(Vulnerability.Alias)</code> Known aliases <code>cwes</code> <code>list(int)</code> CWE IDs <code>created</code> <code>google.protobuf.Timestamp</code> When the vulnerability was created <code>published</code> <code>google.protobuf.Timestamp</code> When the vulnerability was published <code>updated</code> <code>google.protobuf.Timestamp</code> Then the vulnerability was updated <code>severity</code> <code>string</code> <code>cvssv2_base_score</code> <code>double</code> CVSSv2 base score <code>cvssv2_impact_subscore</code> <code>double</code> CVSSv2 impact sub score <code>cvssv2_exploitability_subscore</code> <code>double</code> CVSSv2 exploitability sub score <code>cvssv2_vector</code> <code>string</code> CVSSv2 vector <code>cvssv3_base_score</code> <code>double</code> CVSSv3 base score <code>cvssv3_impact_subscore</code> <code>double</code> CVSSv3 impact sub score <code>cvssv3_exploitability_subscore</code> <code>double</code> CVSSv3 exploitability sub score <code>cvssv3_vector</code> <code>string</code> CVSSv3 vector <code>owasp_rr_likelihood_score</code> <code>double</code> OWASP Risk Rating likelihood score <code>owasp_rr_technical_impact_score</code> <code>double</code> OWASP Risk Rating technical impact score <code>owasp_rr_business_impact_score</code> <code>double</code> OWASP Risk Rating business impact score <code>owasp_rr_vector</code> <code>string</code> OWASP Risk Rating vector <code>epss_score</code> <code>double</code> EPSS score <code>epss_percentile</code> <code>double</code> EPSS percentile"},{"location":"usage/policy-compliance/expressions/#vulnerabilityalias","title":"<code>Vulnerability.Alias</code>","text":"Field Type Description <code>id</code> <code>string</code> ID of the vulnerability (e.g. <code>GHSA-123</code>) <code>source</code> <code>string</code> Authoritative source (e.g. <code>GITHUB</code>)"},{"location":"usage/policy-compliance/expressions/#function-definitions","title":"Function Definitions","text":"<p>In addition to the standard definitions of the CEL specification, Dependency-Track offers additional functions to unlock even more use cases:</p> Symbol Type Description <code>depends_on</code> <code>(Project, Component)</code> -&gt; <code>bool</code> Check if <code>Project</code> depends on <code>Component</code> <code>compare_age</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s age matches a given duration <code>is_dependency_of</code> <code>(Component, Component)</code> -&gt; <code>bool</code> Check if a <code>Component</code> is a dependency of another <code>Component</code> <code>matches_range</code> <code>(Project, string)</code> -&gt; <code>bool</code><code>(Component, string)</code> -&gt; <code>bool</code> Check if a <code>Project</code> or <code>Component</code> matches a vers range <code>matches_version_distance</code> <code>(Component, string, string)</code> -&gt; <code>bool</code> Check if a <code>Component</code>'s version matches a given distance"},{"location":"usage/policy-compliance/overview/","title":"Overview","text":"<p>Organizations can create policies and measure policy violations across the portfolio, and against individual projects and components. Policies are configurable and can be enforced for the portfolio, or can be limited to specific projects. Policies are evaluated when an SBOM is uploaded.</p> <p>There are three types of policy violations:</p> <ul> <li>License</li> <li>Security</li> <li>Operational</li> </ul>"},{"location":"usage/policy-compliance/overview/#license-violation","title":"License Violation","text":"<p>Policy conditions can specify zero or more SPDX license IDs as well as license groups. Dependency-Track comes with pre-configured groups of related licenses (e.g. Copyleft) that provide a starting point for organizations to create custom license policies.</p>"},{"location":"usage/policy-compliance/overview/#security-violation","title":"Security Violation","text":"<p>Policy conditions can specify the severity of vulnerabilities. A vulnerability affecting a component can result in a policy violation if the policy condition matches the severity of the vulnerability. Vulnerabilities that are suppressed will not result in a policy violation.</p>"},{"location":"usage/policy-compliance/overview/#operational-violation","title":"Operational Violation","text":"<p>Policy conditions can specify zero or more:</p> <ul> <li>Coordinates (group, name, version)</li> <li>Package URL</li> <li>CPE</li> <li>SWID Tag ID</li> <li>Hash (MD5, SHA, SHA3, Blake2b, Blake3)</li> </ul> <p>This allows organizations to create lists of allowable and/or prohibited components. Future versions of Dependency-Track will incorporate additional operational parameters into the policy framework.</p>"}]}