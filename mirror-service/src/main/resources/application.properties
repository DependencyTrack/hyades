# @category: General
# @hidden
quarkus.application.name=hyades-mirror-service

# Caching of config properties is not necessary due to the
# low frequency at which mirroring tasks are being executed.
#
# @category: General
# @hidden
quarkus.config.source.dtrack.database.cache.enabled=false

# @category: General
# @hidden
quarkus.native.additional-build-args=\
  --initialize-at-run-time=org.apache.hc.client5.http.impl.classic.ContentCompressionExec,\
  --initialize-at-run-time=org.apache.hc.client5.http.entity.BrotliInputStreamFactory,\
  --initialize-at-run-time=org.apache.hc.client5.http.impl.auth.NTLMEngineImpl,\
  -H:IncludeResources=securityAdvisories.mustache,\
  -H:IncludeResources=securityAdvisoryVulnerabilities.mustache,\
  -H:IncludeResources=securityAdvisoryCwes.mustache,\
  -H:IncludeResources=version.properties

# HTTP port to listen on. Application metrics will be available via this port.
#
# @category: HTTP
# @type:     integer
quarkus.http.port=8093

# Defines whether logs should be written in JSON format.
#
# @category: Observability
# @type:     boolean
quarkus.log.console.json=false

# @category: Observability
# @hidden
quarkus.log.category."org.apache.kafka".level=WARN

# Comma-separated list of brokers to use for establishing the initial connection to the Kafka cluster.
# <br/><br/>
# Refer to <https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers> for details.
#
# @category: Kafka
# @example:  broker-01.acme.com:9092,broker-02.acme.com:9092
# @type:     string
# @required
# kafka.bootstrap.servers=

# @category: Kafka
# @hidden
%dev.kafka.bootstrap.servers=localhost:9092

# @category: Kafka
# @hidden
quarkus.kafka.snappy.enabled=true

# @category: Kafka
# @hidden
kafka.compression.type=snappy

# Defines the maximum size of a Kafka producer request in bytes.
# <br/><br/>
# Some messages like Bill of Vulnerabilities can be bigger than the default 1MiB.
# Since the size check is performed before records are compressed, this value may need to be increased
# even though the compressed value is much smaller. The Kafka default of 1MiB has been raised to 2MiB.
# <br/><br/>
# Refer to <https://kafka.apache.org/documentation/#producerconfigs_max.request.size> for details.
#
# @category: Kafka
# @type:     integer
# @required
kafka.max.request.size=2097152

# Quarkus' ClassLoader black magic doesn't play well with
# native libraries like the one required by Snappy.
# It's causing failures when multiple tests with different
# TestProfile are executed in the same test run.
# TODO: Remove after upgrade to Quarkus 3.11.x (https://github.com/quarkusio/quarkus/issues/39767)
#
# @category: Kafka
# @hidden
%test.quarkus.kafka.snappy.enabled=false

# @category: Kafka
# @hidden
%test.kafka.compression.type=none

# Defines an optional prefix to assume for all Kafka topics the application
# consumes from, or produces to. The prefix will also be prepended to the
# application's consumer group ID.
#
# @category: Kafka
# @example:  acme-
# @type:     string
dt.kafka.topic.prefix=

# Defines the ID to uniquely identify this application in the Kafka cluster.
# <br/><br/>
# Refer to <https://kafka.apache.org/documentation/#streamsconfigs_application.id> for details.
#
# @category: Kafka
# @type:     string
quarkus.kafka-streams.application-id=${dt.kafka.topic.prefix}hyades-mirror-service

# @category: Kafka
# @hidden
quarkus.kafka-streams.topics=\
  ${dt.kafka.topic.prefix}dtrack.vulnerability,\
  ${dt.kafka.topic.prefix}dtrack.vulnerability.digest,\
  ${dt.kafka.topic.prefix}dtrack.vulnerability.mirror.command,\
  ${dt.kafka.topic.prefix}dtrack.vulnerability.mirror.state,\
  ${dt.kafka.topic.prefix}dtrack.epss

# @category: Kafka
# @hidden
%test.quarkus.kafka-streams.topics-timeout=0

# @category: Kafka
# @hidden
%dev.quarkus.kafka.devservices.enabled=false

# The number of threads to allocate for stream processing tasks.
# Note that Specifying a number higher than the number of input partitions provides no additional benefit,
# as excess threads will simply run idle.
# <br/><br/>
# Refer to <https://kafka.apache.org/documentation/#streamsconfigs_num.stream.threads> for details.
#
# @category: Kafka
# @type:     integer
# @required
kafka-streams.num.stream.threads=3

# Defines the interval in milliseconds at which consumer offsets are committed to the Kafka brokers.
# The Kafka default of `30s` has been modified to `5s`.
# <br/><br/>
# Refer to <https://kafka.apache.org/documentation/#streamsconfigs_commit.interval.ms> for details.
#
# @category: Kafka
# @type:     integer
kafka-streams.commit.interval.ms=5000

# Using the default value of 30s in order to make the property configurable via environment variables.
# Without this, Quarkus will interpret "KAFKA_STREAMS" as "kafka.streams", which fails its internal property
# prefix check, which is expecting a "kafka-streams" prefix.
# Overriding this property is required in cases where the Delete topic permission can not be granted to
# Kafka clients (e.g. in multi-tenant Kafka clusters).
#
# @category: Kafka
# @type:     integer
# @hidden
kafka-streams.repartition.purge.interval.ms=30000

# @category: Kafka
# @hidden
kafka-streams.default.deserialization.exception.handler=org.dependencytrack.kstreams.exception.DeserializationExceptionHandler

# @category: Kafka
# @hidden
kafka-streams.default.production.exception.handler=org.dependencytrack.kstreams.exception.ProductionExceptionHandler

# Defines the threshold for records failing to be deserialized within kafka-streams.exception.thresholds.deserialization.interval.
# Deserialization failures within the threshold will be logged, failures exceeding the threshold cause the application
# to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     integer
# @required
kafka-streams.exception.thresholds.deserialization.count=5

# Defines the interval within which up to kafka-streams.exception.thresholds.deserialization.count records are
# allowed to fail deserialization. Deserialization failures within the threshold will be logged,
# failures exceeding the threshold cause the application to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     duration
# @required
kafka-streams.exception.thresholds.deserialization.interval=PT30M

# Defines the threshold for records failing to be processed within kafka-streams.exception.thresholds.processing.interval.
# Processing failures within the threshold will be logged, failures exceeding the threshold cause the application
# to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     integer
# @required
kafka-streams.exception.thresholds.processing.count=50

# Defines the interval within which up to kafka-streams.exception.thresholds.processing.count records are
# allowed to fail processing. Processing failures within the threshold will be logged,
# failures exceeding the threshold cause the application to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     duration
# @required
kafka-streams.exception.thresholds.processing.interval=PT30M

# Defines the threshold for records failing to be produced within kafka-streams.exception.thresholds.production.interval.
# Production failures within the threshold will be logged, failures exceeding the threshold cause the application
# to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     integer
# @required
kafka-streams.exception.thresholds.production.count=5

# Defines the interval within which up to kafka-streams.exception.thresholds.production.count records are
# allowed to fail producing. Production failures within the threshold will be logged,
# failures exceeding the threshold cause the application to stop processing further records, and shutting down.
#
# @category: Kafka
# @type:     duration
# @required
kafka-streams.exception.thresholds.production.interval=PT30M

# @category: Kafka
# @hidden
quarkus.kafka.devservices.image-name=docker.redpanda.com/redpandadata/redpanda:v24.2.17

# @category: Kafka
# @hidden
quarkus.kafka.devservices.topic-partitions."dtrack.vulnerability.mirror.command"=1

# @category: Kafka
# @hidden
quarkus.kafka.devservices.topic-partitions."dtrack.vulnerability.mirror.state"=1

# @category: Kafka
# @hidden
quarkus.kafka.devservices.topic-partitions."dtrack.vulnerability.digest"=1

# @category: Kafka
# @hidden
quarkus.kafka.devservices.topic-partitions."dtrack.vulnerability"=1

# @category: Kafka
# @hidden
quarkus.kafka.devservices.topic-partitions."dtrack.notification.datasource-mirroring"=1

# @category: Kafka
# @hidden
quarkus.kafka.devservices.topic-partitions."dtrack.epss"=1

# @category: Database
# @hidden
%dev.quarkus.datasource.username=dtrack

# @category: Database
# @hidden
%dev.quarkus.datasource.password=dtrack

# @category: Database
# @hidden
%dev.quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/dtrack

# @category: Database
# @hidden
quarkus.datasource.devservices.image-name=postgres:13-alpine

# @category: Database
# @hidden
quarkus.datasource.devservices.init-script-path=schema.sql

# @category: Database
# @hidden
quarkus.hibernate-orm.quote-identifiers.strategy=all

# @category: Database
# @hidden
quarkus.hibernate-orm.database.generation=validate

# @category: General
# @hidden
%test.secret.key.path=src/test/resources/secret.key

# Defines whether vulnerability aliases should be parsed from OSV.
#
# @category: Datasource
# @type:     boolean
mirror.datasource.osv.alias-sync-enabled=false

# Defines the URL of the OSV storage bucket.
#
# @category: Datasource
# @default:  https://osv-vulnerabilities.storage.googleapis.com
# @type:     string
# mirror.datasource.osv.base-url=

# @category: General
# @hidden
quarkus.container-image.registry=ghcr.io

# @category: General
# @hidden
quarkus.container-image.group=dependencytrack