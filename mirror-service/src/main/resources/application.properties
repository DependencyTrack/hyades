## Quarkus
#
quarkus.application.name=hyades-mirror-service
quarkus.http.port=8093

## Logging
#
quarkus.log.console.json=false
quarkus.log.category."org.apache.kafka".level=WARN

## Native Image
#
quarkus.native.additional-build-args=\
  --initialize-at-run-time=org.apache.hc.client5.http.impl.auth.NTLMEngineImpl,\
  -H:IncludeResources=securityAdvisories.mustache,\
  -H:IncludeResources=securityAdvisoryVulnerabilities.mustache,\
  -H:IncludeResources=securityAdvisoryCwes.mustache

## Kafka
#
%dev.kafka.bootstrap.servers=localhost:9092
quarkus.kafka.snappy.enabled=true
kafka.compression.type=snappy
# Some messages like vulnerabilities can be bigger than default 1MB.
# Since the size check is performed before the record is compressed, we get an exception even though compression is enabled
# Changing producer config will allow upto 2MiB (1024*1024*2) to be sent on kafka topic if they can be compressed into default size
kafka.max.request.size=2097152
# Quarkus' ClassLoader black magic doesn't play well with
# native libraries like the one required by Snappy.
# It's causing failures when multiple tests with different
# TestProfile are executed in the same test run.
%test.quarkus.kafka.snappy.enabled=false
%test.kafka.compression.type=none

## Kafka Streams
#
kafka.topic.prefix=
quarkus.kafka-streams.application-id=${kafka.topic.prefix}hyades-mirror-service
quarkus.kafka-streams.application-server=localhost:8093
quarkus.kafka-streams.topics=\
  ${kafka.topic.prefix}dtrack.vulnerability,\
  ${kafka.topic.prefix}dtrack.vulnerability.digest,\
  ${kafka.topic.prefix}dtrack.vulnerability.mirror.command,\
  ${kafka.topic.prefix}dtrack.vulnerability.mirror.state
%dev.quarkus.kafka.devservices.enabled=false
kafka-streams.num.stream.threads=3
kafka-streams.commit.interval.ms=5000
# Using the default value of 30s in order to make the property configurable via environment variables.
# Without this, Quarkus will interpret "KAFKA_STREAMS" as "kafka.streams", which fails its internal property
# prefix check, which is expecting a "kafka-streams" prefix.
# Overriding this property is required in cases where the Delete topic permission can not be granted to
# Kafka clients (e.g. in multi-tenant Kafka clusters).
kafka-streams.repartition.purge.interval.ms=30000

## Kafka Streams Exception Handling
#
kafka-streams.default.deserialization.exception.handler=org.dependencytrack.kstreams.exception.DeserializationExceptionHandler
kafka-streams.default.production.exception.handler=org.dependencytrack.kstreams.exception.ProductionExceptionHandler
kafka-streams.exception.thresholds.deserialization.count=5
kafka-streams.exception.thresholds.deserialization.interval=PT30M
kafka-streams.exception.thresholds.processing.count=50
kafka-streams.exception.thresholds.processing.interval=PT30M
kafka-streams.exception.thresholds.production.count=5
kafka-streams.exception.thresholds.production.interval=PT30M

## Dev Services for Kafka
#
quarkus.kafka.devservices.image-name=docker.redpanda.com/vectorized/redpanda:v23.3.6
quarkus.kafka.devservices.topic-partitions."dtrack.vulnerability.mirror.command"=1
quarkus.kafka.devservices.topic-partitions."dtrack.vulnerability.mirror.state"=1
quarkus.kafka.devservices.topic-partitions."dtrack.vulnerability.digest"=1
quarkus.kafka.devservices.topic-partitions."dtrack.vulnerability"=1
quarkus.kafka.devservices.topic-partitions."dtrack.notification.datasource-mirroring"=1

mirror.datasource.nvd.api-key=
mirror.datasource.nvd.num-threads=4
mirror.datasource.github.api-key=
mirror.datasource.github.base-url=
mirror.datasource.github.alias-sync-enabled=false
mirror.datasource.osv.alias-sync-enabled=false
mirror.datasource.osv.base-url=https://osv-vulnerabilities.storage.googleapis.com

## Container Image
#
quarkus.container-image.registry=ghcr.io
quarkus.container-image.group=dependencytrack
